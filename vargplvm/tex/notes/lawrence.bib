
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  File: Lawrence.bib
%
%  Bibliography file
%
%  Neil Lawrence, 8 August, 1999.
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@string{phdthesis = {PhD Theses}}

@string{article = {Journal Papers}}
@string{book = {Books}}
@string{techreport = {Technical Reports}}
@string{unpublished = {Submitted Papers}}
@string{inproceedings = {Refereed Conference Papers}}
@string{proceedings = {Proceedings}}
@string{incollection = {In Collected Volumes}}
@string{collection = {Volumes of Collected Papers}}
@string{misc = {Miscellaneous}}
@string{patent = {Patents}}
@string{talk = {Talks}}
@string{poster = {Posters}}
@string{mainheading = {Machine Learning Publications}}
@String{bioinf = {Bioinformatics}}
@String{bmcbioinf = {BMC Bioinformatics}}

@string{RMP =      {Reviews of Modern Physics}}
@string{ieeecomp = {IEEE Computer Society Press}}
@string{pCVPR =    {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}}
@string{jasa = {Journal of the American Statistical Association}}
@string{icml =     {Proceedings of the International Conference in 
                   Machine Learning}}
@string{uai =      {Uncertainty in Artificial Intelligence}}
@string{icann =    {International Conference on Artificial Neural Networks}}
@String{jmbcell = {Mol. Biol. Cell.}}
@String{pnasusa = {Proc. Natl. Acad. Sci. USA}}
@String{jair = {Journal of Artificial Intelligence Research}}
@String{jmlr = {Journal of Machine Learning Research}}
@String{lncs = {Lecture Notes in Computer Science}}
@string{nips =     {Advances in Neural Information Processing Systems}}
@string{NC =       {Neural Computation}}
@string{ML =       {Machine Learning}}
@string{NN =       {Neural Networks}}
@string{NW =       {Network: Computation in Neural Systems}}
@string{IJNS =     {International Journal of Neural Systems}}
@string{PRa =      {Physical Review A}}
@string{PRL =      {Physical Review Letters}}
@string{EPL =      {Europhysics Letters}}
@string{icassp =   {International Conference on Acoustics, Speech and Signal Processing}}
@string{IEEE =     {IEEE Transactions on Neural Networks}}
@string{TIT =      {IEEE Transactions on Information Theory}}
@string{TKDE =     {IEEE Transactions on Knowledge and Data Engineering}}
@string{AMS =      {Annals of Mathematical Statistics}}
@string{PAMI =     {IEEE Transactions on Pattern Analysis and 
                   Machine Intelligence}}
@string{DOKLADY =  {Doklady Akademiia Nauk SSSR}}
@string{network =  {Network: Computation in Neural Systems}}
@string{ijcnn =    {Proceedings of the International Joint Conference on
                   Neural Networks}}

@string{addison =  {Addison-Wesley}}
@string{mcgraw =   {McGraw-Hill}}
@string{nholland = {North Holland}}
@string{ams = {AMS}}
@string{springer = {Springer-Verlag}}
@string{harvard =      {Harvard University Press}}
@string{mit =      {MIT Press}}
@string{cup =      {Cambridge University Press}}
@string{mk =       {Morgan Kauffman}}
@string{wiley =    {John Wiley and Sons}}
@string{JRSSb =    {Journal of the Royal Statistical Society, B}}
@string{JMB =    {Journal of Molecular Biology}}

@string{myftp =    {http://www.thelawrences.net/neil/}}
@string{shefftp =    {ftp://ftp.dcs.shef.ac.uk/home/neil/}}
@string{manftp =    {ftp://ftp.cs.man.ac.uk/pub/ai/neill/}}
@string{sheftech =    {The University of Sheffield, Department of Computer Science}}
@string{joabpubs = {http://www.dcs.shef.ac.uk/~joab/Publications/}}
@string{softwarehttp = {http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/}}

@string{gplvmTitle1 = {Probabilistic Non-linear Component Analysis through {G}aussian Process Latent Variable Models}}
@string{gplvmAbstract1 = {It is known that Principal Component Analysis has an 
                 underlying probabilistic representation based on a
                 latent variable model. Principal component analysis
                 (PCA) is recovered when the latent variables are
                 integrated out and the parameters of the model are
                 optimised by maximum likelihood. It is less well
                 known that the dual approach of integrating out the
                 parameters and optimising with respect to the latent
                 variables also leads to PCA. The marginalised
                 likelihood in this case takes the form of Gaussian
                 process mappings, with linear Covariance functions,
                 from a latent space to an observed space, which we
                 refer to as a Gaussian Process Latent Variable Model
                 (GPLVM). This dual probabilistic PCA is still a
                 linear latent variable model, but by looking beyond
                 the inner product kernel as a covariance function we
                 can develop a non-linear probabilistic PCA.

                 In the talk we will introduce the GPLVM and
                 illustrate its application on a range of high
                 dimensional data sets including motion capture data,
                 hand written digits, a medical diagnosis data set and
                 images.}}

@string{gplvmTitle2 = {High Dimensional Probabilistic Modelling through Manifolds}}
@string{gplvmAbstract2 = {Density modelling in high dimensions is a 
                 very difficult problem. Traditional approaches, such
                 as mixtures of Gaussians, typically fail to capture
                 the structure of data sets in high dimensional
                 spaces. In this talk we will argue that for many data
                 sets of interest, the data can be represented as a
                 lower dimensional manifold immersed in the higher
                 dimensional space. We will then present the Gaussian
                 Process Latent Variable Model (GP-LVM), a non-linear
                 probabilistic variant of principal component analysis
                 (PCA) which implicitly assumes that the data lies on
                 a lower dimensional space.

                 We will demonstrate the application of the model to a
                 range of data sets, but with a particular focus on
                 human motion data. We will show some preliminary work
                 on facial animation and make use of a skeletal motion
                 capture data set to illustrate differences between
                 our model and traditional manifold techniques.}}

@string{gplvmTitle3 = {Computer Vision Reading Group: The {G}aussian
                 Process Latent Variable Model}}

@string{gplvmAbstract3 = {The Gaussian process latent variable model
                 (GP-LVM) is a recently proposed probabilistic
                 approach to obtaining a reduced dimension
                 representation of a data set. In this tutorial we
                 motivate and describe the GP-LVM, giving a review of
                 the model itself and some of the concepts behind it.}
                 }

@string{gplvmTitle4 = {Probabilistic Dimensional Reduction with the {G}aussian Process Latent Variable Model}}
@string{gplvmAbstract4 = {Density modelling in high dimensions is a 
                 very difficult problem. Traditional approaches, such
                 as mixtures of Gaussians, typically fail to capture
                 the structure of data sets in high dimensional
                 spaces. In this talk we will argue that for many data
                 sets of interest, the data can be represented as a
                 lower dimensional manifold immersed in the higher
                 dimensional space. We will then present the Gaussian
                 Process Latent Variable Model (GP-LVM), a non-linear
                 probabilistic variant of principal component analysis
                 (PCA) which implicitly assumes that the data lies on
                 a lower dimensional space.

                 Having introduced the GP-LVM we will review
                 extensions to the algorithm, including dynamics,
                 learning of large data sets and back constraints.  We
                 will demonstrate the application of the model and its
                 extensions to a range of data sets, including human
                 motion data, a vowel data set and a robot mapping
                 problem.}  
}
@string{gplvmTitle5 = {Probabilistic Dimensional Reduction with the {G}aussian Process Latent Variable Model}}
@string{gplvmAbstract5 = {Density modelling in high dimensions is a 
                 very difficult problem. Traditional approaches, such
                 as mixtures of Gaussians, typically fail to capture
                 the structure of data sets in high dimensional
                 spaces. In this talk we will argue that for many data
                 sets of interest, the data can be represented as a
                 lower dimensional manifold immersed in the higher
                 dimensional space. We will then present the Gaussian
                 Process Latent Variable Model (GP-LVM), a non-linear
                 probabilistic variant of principal component analysis
                 (PCA) which implicitly assumes that the data lies on
                 a lower dimensional space.

                 
                 Having introduced the GP-LVM we will review
                 extensions to the algorithm. Given time we will review dynamical extensions, Bayesian approaches to dimensionality determination,
                 learning of large data sets. We
                 will demonstrate the application of the model and its
                 extensions to a range of data sets, including human
                 motion data, speech data and video.}  
}

@string{gpTitle1 = {Learning and Inference with {G}aussian Processes}}

@string{gpAbstract1 = {Many application domains of machine learning can be
                 reduced to inference about the values of a
                 function. Gaussian processes are powerful, flexible,
                 probabilistic models that enable us to efficiently
                 perform inference about functions in the presence of
                 uncertainty.

                 In this talk I will introduce Gaussian processes and
                 review a few standard applications of these models. I
                 will then show how Gaussian processes can be used to
                 solve important and diverse real-world problems,
                 including inference of the concentration of
                 transcription factors which regulate gene expression
                 and creating probabilistic models of human motion for
                 animation and tracking.}  
}

@String{pumaTitle1 = {{PUMA}: Propagation of Uncertainty in Microarray Analysis}}
@String{pumaAbstract1 = {}}


@InCollection{Lawrence:gpinference11,
  author = 	 {Neil D. Lawrence and Magnus Rattray and Antti Honkela and Michalis Titsias},
  title = 	 {Gaussian Process Inference for Differential Equation Models of Transcriptional Regulation},
  booktitle = 	 {Handbook of Statistical Systems Biology},
  OPTcrossref =  {},
  OPTkey = 	 {},
  pages =	 {376--394},
  publisher =	 wiley,
  year =	 2011,
  editor =	 {Michael Stumpf and David J. Balding and Mark Girolami},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTtype = 	 {},
  OPTchapter = 	 19,
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  OPTabstract =  {},
  OPTgroup = 	 {}
}

@Article{Lazaro:overlapping11,
  author = 	 {Miguel L\'azaro Gredilla and Steven Van Vaerenberghand Neil D. Lawrence},
  title = 	 {Overlapping Mixtures of Gaussian Processes for the Data Association Problem},
  journal = 	 {Pattern Recognition},
  year = 	 2011,
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi =		 {10.1016/j.patcog.2011.10.004},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {In this work we introduce a mixture of GPs to address the data association problem, i.e., to label a group of observations according to the sources that generated them. Unlike several previously proposed GP mixtures, the novel mixture has the distinct characteristic of using no gating function to determine the association of samples and mixture components. Instead, all the GPs in the mixture are global and samples are clustered following ``trajectories'' across input space. We use a non-standard variational Bayesian algorithm to efficiently recover sample labels and learn the hyperparameters. We show how multi-object tracking problems can be disambiguated and also explore the characteristics of the model in traditional regression settings.},
  OPTgroup = 	 {}
}

@Article{Honkela:tigre11,
  author = 	 {Antti Honkela and Pei Gao and Jonatan Ropponen and Magnus Rattray and Neil D. Lawrence},
  title = 	 {tigre: Transcription factor inference through {Gaussian} process reconstruction of expression for Bioconductor},
  journal = 	 {Bioinformatics},
  year = 	 2011,
  OPTkey = 	 {},
  volume =	 {27},
  linkpdf = 	 {http://bioinformatics.oxfordjournals.org/content/27/7/1026.full.pdf+html},
  pages =	 {1026--1027},
  doi =		 {10.1093/bioinformatics/btr057},
  masid = 	 {39311794},
  abstract = 	 {\textbf{Summary}: tigre is an R/Bioconductor package for 
                 inference of transcription factor activity and
                 ranking candidate target genes from gene expression
                 time series. The underlying methodology is based on
                 Gaussian process inference on a differential equation
                 model that allows the use of short, unevenly sampled,
                 time series. The method has been designed with
                 efficient parallel implementation in mind, and the
                 package supports parallel operation even without
                 additional software.\\\\

                 \textbf{Availability}: The tigre package is included
                 in Bioconductor since release 2.6 for R 2.11. The
                 package and a user's guide are available at
                 http://www.bioconductor.org.\\\\

                 \textbf{Contact}: antti.honkela@hiit.fi;
                 m.rattray@sheffield.ac.uk; n.lawrence@dcs.shef.ac.uk} 
}

@Article{Alvarez:computationally11,
  author = 	 {Mauricio A. \'Alvarez and Neil D. Lawrence},
  title = 	 {Computationally Efficient Convolved Multiple Output {Gaussian} Processes},
  journal = 	 jmlr,
  year = 	 2011,
  OPTkey = 	 {},
  volume =	 12,
  OPTnumber = 	 {},
  pages =	 {1425--1466},
  month =	 {May},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 {http://www.jmlr.org/papers/volume12/alvarez11a/alvarez11a.pdf},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  linksoftware = softwarehttp # {multigp},

  abstract =	 {Recently there has been an increasing interest in regression
                 methods that deal with multiple outputs. This has
                 been motivated partly by frameworks like multitask
                 learning, multisensor networks or structured output
                 data. From a Gaussian processes perspective, the
                 problem reduces to specifying an appropriate
                 covariance function that, whilst being positive
                 semi-definite, captures the dependencies between all
                 the data points and across all the outputs. One
                 approach to account for non-trivial correlations
                 between outputs employs convolution processes. Under
                 a latent function interpretation of the convolution
                 transform we establish dependencies between output
                 variables. The main drawbacks of this approach are
                 the associated computational and storage demands. In
                 this paper we address these issues. We present
                 different efficient approximations for dependent
                 output Gaussian processes constructed through the
                 convolution formalism. We exploit the conditional
                 independencies present naturally in the model. This
                 leads to a form of the covariance similar in spirit
                 to the so called PITC and FITC approximations for a
                 single output. We show experimental results with
                 synthetic and real data, in particular, we show
                 results in school exams score prediction, pollution
                 prediction and gene expression data},
  OPTgroup = 	 {}
}


@Article{Kalaitzis:simple11,
  author = 	 {Alfredo A. Kalaitzis and Neil D. Lawrence},
  title = 	 {A Simple Approach to Ranking Differentially Expressed Gene Expression Time Courses through {Gaussian} Process Regression},
  journal = 	 {BMC Bioinformatics},
  year = 	 2011,
  OPTkey = 	 {},
  volume =	 12,
  number =	 180,
  OPTpages = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi =		 {10.1186/1471-2105-12-180},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {{\bf Background}\\\\
The analysis of gene expression from time series underpins many biological studies. Two basic forms of analysis recur for data of this type: removing inactive (quiet) genes from the study and determining which genes are differentially expressed. Often these analysis stages are applied disregarding the fact that the data is drawn from a time series. In this paper we propose a simple model for accounting for the underlying temporal nature of the data based on a Gaussian process.\\\\

{\bf Results}\\\\
We review Gaussian process (GP) regression for estimating the continuous trajectories underlying in gene expression time-series. We present a simple approach which can be used to filter quiet genes, or for the case of time series in the form of expression ratios, quantify differential expression. We assess via ROC curves the rankings produced by our regression framework and compare them to a recently proposed hierarchical Bayesian model for the analysis of gene expression time-series (BATS). We compare on both simulated and experimental data showing that the proposed approach significantly outperforms the current state of the art.\\\\

{\bf Conclusions}\\\\
Gaussian processes offer an attractive trade-off between efficiency and usability for the analysis of micro-array time series. The Gaussian process framework offers a natural way of handling biological replicates and missing values and provides confidence intervals along the estimated curves of gene expression. Therefore, we believe Gaussian processes should be a standard tool in the analysis of gene expression time series.},
  OPTgroup = 	 {}
}

@Article{Asif:tfinfer10,
  author = 	 {H. M. Shahzad Asif and Matthew D. Rolfe and Jeff Green and Neil D. Lawrence and Magnus Rattray and Guido Sanguinetti},
  title = 	 {TFInfer: a tool for probabilistic inference of transcription factor activities},
  journal = 	 bioinf,
  year = 	 2010,
  OPTkey = 	 {},
  volume =	 {26},
  OPTnumber = 	 {},
  pages =	 {2635--2636},
  linkpdf = 	 {http://bioinformatics.oxfordjournals.org/content/26/20/2635.full.pdf+html},
  abstract =	 {\textbf{Summary}: TFInfer is a novel open access, standalone tool for genome-wide inference of transcription factor activities from gene expression data. Based on an earlier MATLAB version, the software has now been extended in a number of ways. It has been significantly optimised in terms of performance, and it was given novel functionality, by allowing the user to model both time series and data from multiple independent conditions. With a full documentation and intuitive graphical user interface, together with an in-built data base of yeast and Escherichia coli transcription factors, the software does not require any mathematical or computational expertise to be used effectively.\\

\textbf{Availability}: \url{http://homepages.inf.ed.ac.uk/gsanguin/TFInfer.html}

\textbf{Contact}: \url{gsanguin@staffmail.ed.ac.uk}}
}

@InCollection{Titsias:mcmcgp11,
  author =	 {Michalis K. Titsias and Magnus Rattray and Neil D. Lawrence},
  title = 	 {Markov chain {M}onte {C}arlo algorithms for {G}aussian processes},
  chapter = 	 {14},
  year = 	 {2011},
  crossref =	 {Barber:bayestime11},
  OPTkey = 	 {},
  OPTpages = 	 {},
  OPTpublisher = {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTtype = 	 {},
  OPTchapter = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  OPTabstract =  {},
  OPTgroup = 	 {}
}

@Article{Honkela:modelbased10,
  author =	 {Antti Honkela and Charles Girardot and E. Hilary Gustafson and Ya-Hsin Liu and Eileen E. M. Furlong and Neil D. Lawrence and Magnus Rattray},
  title =	 {Model-based Method for Transcription Factor Target
                  Identification with Limited Data},
  journal =	 pnasusa,
  year =	 2010,
  linksoftware = softwarehttp # {disimrank},
  volume =	 {107},
  pages =	 {7793--7798},
  number =	 {17},
  month =	 {Apr},
  doi =		 {10.1073/pnas.0914285107},
  abstract =	 {We present a computational method for identifying potential targets of a transcription factor (TF) using wild-type gene expression time series data. For each putative target gene we fit a simple differential equation model of transcriptional regulation, and the model likelihood serves as a score to rank targets. The expression profile of the TF is modeled as a sample from a Gaussian process prior distribution that is integrated out using a nonparametric Bayesian procedure. This results in a parsimonious model with relatively few parameters that can be applied to short time series datasets without noticeable overfitting. We assess our method using genome-wide chromatin immunoprecipitation (ChIP-chip) and loss-of-function mutant expression data for two TFs, Twist, and Mef2, controlling mesoderm development in Drosophila. Lists of top-ranked genes identified by our method are significantly enriched for genes close to bound regions identified in the ChIP-chip data and for genes that are differentially expressed in loss-of-function mutants. Targets of Twist display diverse expression profiles, and in this case a model-based approach performs significantly better than scoring based on correlation with TF expression. Our approach is found to be comparable or superior to ranking based on mutant differential expression scores. Also, we show how integrating complementary wild-type spatial expression data can further improve target ranking performance.}
}

@InProceedings{Damianou:vgpds11,
  author = 	 {Andreas Damianou and Michalis K. Titsias and Neil D. Lawrence},
  title = 	 {Variational Gaussian Process Dynamical Systems},
  crossref =	 {Bartlett:nips11},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  note =	 {To appear},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 shefftp # {VGPDS_Nips11.pdf},
  label1 = {Supplementary Videos},
  link1 = {http://staffwww.dcs.shef.ac.uk/people/A.Damianou/varFiles/VGPDS/index.html},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  linksoftware = softwarehttp # {vargplvm},
  abstract =	 {High dimensional time series are endemic in applications
                 of machine learning such as robotics (sensor data),
                 computational biology (gene expression data), vision
                 (video sequences) and graphics (motion capture
                 data). Practical nonlinear probabilistic approaches
                 to this data are required. In this paper we introduce
                 the variational Gaussian process dynamical
                 system. Our work builds on recent variational
                 approximations for Gaussian process latent variable
                 models to allow for nonlinear dimensionality
                 reduction simultaneously with learning a dynamical
                 prior in the latent space. The approach also allows
                 for the appropriate dimensionality of the latent
                 space to be automatically determined. We demonstrate
                 the model on a human motion capture data set and a
                 series of high resolution video sequences.},
  OPTgroup = 	 {}
}

@InProceedings{Alvarez:switched10,
  author = 	 {Mauricio A. \'Alvarez and Jan Peters and Bernhard Sch\"olkopf and Neil D. Lawrence},
  title = 	 {Switched Latent Force Models for Movement Segmentation},
  crossref =	 {Lafferty:nips10},
  OPTkey = 	 {},
  pages =	 {55--63},
  pdf =	   	 {http://books.nips.cc/papers/files/nips23/NIPS2010_1222.pdf},
  linkvideo = 	 {http://videolectures.net/nips2010_alvarez_slf/},
  OPTorganization = {},
  OPTpublisher = {},
  abstract =	 {Latent force models encode the interaction between multiple 
                 related dynamical systems in the form of a kernel or
                 covariance function. Each variable to be modeled is
                 represented as the output of a differential equation
                 and each differential equation is driven by a
                 weighted sum of latent functions with uncertainty
                 given by a Gaussian process prior. In this paper we
                 consider employing the latent force model framework
                 for the problem of determining robot motor
                 primitives. To deal with discontinuities in the
                 dynamical systems or the latent driving force we
                 intro- duce an extension of the basic latent force
                 model, that switches between different latent
                 functions and potentially different dynamical
                 systems. This creates a versatile representation
                 for robot movements that can capture discrete changes
                 and non-linearities in the dynamics. We give
                 illustrative examples on both synthetic data and for
                 striking movements recorded using a Barrett WAM robot
                 as haptic in- put device. Our inspiration is robot
                 motor primitives, but we expect our model to have
                 wide application for dynamical systems including
                 models for human motion capture data and systems
                 biology.}
}

@InProceedings{Alvarez:efficient10,
  author =	 {Mauricio A. \'Alvarez and David Luengo and Michalis K. Titsias and Neil D. Lawrence},
  title =	 {Efficient Multioutput {G}aussian Processes through
                  Variational Inducing Kernels},
  crossref =	 {Teh:aistats10},
  pages =	 {25--32},
  year =	 2010,
  OPTeditor =	 {},
  volume =	 9,
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =      {http://jmlr.csail.mit.edu/proceedings/papers/v9/alvarez10a/alvarez10a.pdf},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  linksoftware = softwarehttp # {multigp},
  abstract =	 {Interest in multioutput kernel methods is
                  increasing, whether under the guise of multitask
                  learning, multisensor networks or structured output
                  data. From the Gaussian process perspective a
                  multioutput Mercer kernel is a covariance function
                  over correlated output functions. One way of
                  constructing such kernels is based on convolution
                  processes (CP). A key problem for this approach is
                  efficient inference. \'Alvarez and Lawrence
                  \cite{Alvarez:convolved08} recently presented a
                  sparse approximation for CPs that enabled efficient
                  inference. In this paper, we extend this work in two
                  directions: we introduce the concept of variational
                  inducing functions to handle potential non-smooth
                  functions involved in the kernel CP construction and
                  we consider an alternative approach to approximate
                  inference based on variational methods, extending
                  the work by Titsias \cite{Titsias:variational09} to
                  the multiple output case. We demonstrate our
                  approaches on prediction of school marks, compiler
                  performance and financial time series.},
  OPTgroup =	 {}
}

@InProceedings{Lawrence:spectral11,
  author =	 {Neil D. Lawrence},
  title =	 {Spectral Dimensionality Reduction via Maximum Entropy},
  crossref =	 {Gordon:aistats11},
  pages =	 {},
  year =	 2011,
  OPTeditor =	 {},
  volume =	 15,
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =      {http://jmlr.csail.mit.edu/proceedings/papers/v15/lawrence11a/lawrence11a.pdf},
  OPTlinkps =	 {},
  linkvideo =	 {http://videolectures.net/aistats2011_lawrence_spectral/},
  linksoftware = softwarehttp # {meu},
  abstract =	 {We introduce a new perspective on spectral dimensionality reduction
  which views these methods as Gaussian random fields (GRFs). Our
  unifying perspective is based on the maximum entropy principle which
  is in turn inspired by maximum variance unfolding. The resulting
  probabilistic models are based on GRFs. The resulting model is a
  nonlinear generalization of principal component analysis. We show
  that parameter fitting in the locally linear embedding is
  approximate maximum likelihood in these models. We directly maximize
  the likelihood and show results that are competitive with the
  leading spectral approaches on a robot navigation visualization and
  a human motion capture data set.},
  note =	 {Notable Paper}
}


@InProceedings{Titsias:bayesGPLVM10,
  author =	 {Michalis K. Titsias and Neil D. Lawrence},
  title =	 {Bayesian {G}aussian Process Latent Variable Model},
  crossref =	 {Teh:aistats10},
  pages =	 {844--851},
  year =	 2010,
  OPTeditor =	 {},
  volume =	 9,
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =      {http://jmlr.csail.mit.edu/proceedings/papers/v9/titsias10a/titsias10a.pdf},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  linksoftware = softwarehttp # {vargplvm},
  abstract =	 { We introduce a variational inference framework for
                  training the Gaussian process latent variable model
                  and thus performing Bayesian nonlinear
                  dimensionality reduction. This method allows us to
                  variationally integrate out the input variables of
                  the Gaussian process and compute a lower bound on
                  the exact marginal likelihood of the nonlinear
                  latent variable model. The maximization of the
                  variational lower bound provides a Bayesian training
                  procedure that is robust to overfitting and can
                  automatically select the dimensionality of the
                  nonlinear latent space. We demonstrate our method on
                  real world datasets. The focus in this paper is on
                  dimensionality reduction problems, but the
                  methodology is more general. For example, our
                  algorithm is immediately applicable for training
                  Gaussian process models in the presence of missing
                  or uncertain inputs. },
  OPTgroup =	 {}
}

@Article{Zampini:elementary09,
  author =	 {Valeria Zampini and Stuart Leigh Johnson and
                  Christoph Franz and Neil D. Lawrence and Stefan
                  Muenkner Jutta Engel and Marlies Knipper and Jacopo
                  Magistretti and Sergio Masetto and Walter Marcotti},
  title = 	 {Elementary properties of {CaV}1.3 {Ca}2+ channels expressed in mouse cochlear inner hair cells},
  journal = 	 {The Journal of Physiology},
  year = 	 2010,
  OPTkey = 	 {},
  volume =	 {588},
  OPTnumber = 	 {},
  pages =	 {187--189},
  OPTmonth = 	 {},
  OPTannote = 	 {},
  pmid =	 {19917569},
  doi =		 {10.1113/jphysiol.2009.181917},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {Mammalian cochlear inner hair cells (IHCs) are specialized to process developmental signals during immature stages and sound stimuli in adult animals. These signals are conveyed onto auditory afferent nerve fibres. Neurotransmitter release at IHC ribbon synapses is controlled by L-type CaV1.3 Ca2+ channels, the biophysics of which are still unknown in native mammalian cells. We have investigated the localization and elementary properties of Ca2+ channels in immature mouse IHCs under near-physiological recording conditions. CaV1.3 Ca2+ channels at the cell pre-synaptic site co-localize with about half of the total number of ribbons present in immature IHCs. These channels activated at relatively hyperpolarized membrane potentials (about -70 mV), showed a relatively short first latency and weak inactivation, which would allow IHCs to generate and accurately encode spontaneous Ca2+ action potential activity characteristic of these immature cells. The CaV1.3 Ca2+ channels showed a very low open probability (about 0.15 at -20 mV: near the peak of an action potential). Comparison of elementary and macroscopic Ca2+ currents indicated that very few Ca2+ channels are associated with each docked vesicle at IHC ribbon synapses. Finally, we found that the open probability of Ca2+ channels, but not their opening time, was voltage dependent. This finding provides a possible correlation between presynaptic Ca2+ channel properties and the characteristic frequency/amplitude of EPSCs in auditory afferent fibres.},
  OPTgroup = 	 {}
}
@InProceedings{Darby:backing09,
  author =	 {John Darby and Baihua Li and Nicholas Costen and
                  David J. Fleet and Neil D. Lawrence},
  title =	 {Backing Off: Hierarchical Decomposition of Activity
                  for 3D Novel Pose Recovery},
  OPTcrossref =	 {},
  OPTkey =	 {},
  booktitle =	 {British Machine Vision Conference},
  OPTpages =	 {},
  year =	 2009,
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =	 shefftp # {BMVC2009CR06.pdf},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  linksoftware = {http://www.docm.mmu.ac.uk/STAFF/J.Darby/code.htm},
  abstract =	 {For model-based 3D human pose estimation, even
                  simple models of the human body lead to
                  high-dimensional state spaces. Where the class of
                  activity is known a priori, lowdimensional activity
                  models learned from training data make possible a
                  thorough and efficient search for the best
                  pose. Conversely, searching for solutions in the
                  full state space places no restriction on the class
                  of motion to be recovered, but is both difficult and
                  expensive. This paper explores a potential middle
                  ground between these approaches, using the
                  hierarchical Gaussian process latent variable model
                  to learn activity at different hierarchical scales
                  within the human skeleton. We show that by training
                  on full-body activity data then descending through
                  the hierarchy in stages and exploring subtrees
                  independently of one another, novel poses may be
                  recovered. Experimental results on motion capture
                  data and monocular video sequences demonstrate the
                  utility of the approach, and comparisons are drawn
                  with existing low-dimensional activity models},
  OPTgroup =	 {}
}

@TechReport{Kalaitzis:rca11,
  author = 	 {Alfredo A. Kalaitzis and Neil D. Lawrence},
  title = 	 {Residual Component Analysis},
  institution =  {University of Sheffield},
  year = 	 2011,
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 {http://arxiv.org/pdf/1106.4333v1},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  linksoftware = softwarehttp # {rca},
  OPTlinksoftware = {},
  abstract =	 {Probabilistic principal component analysis (PPCA) seeks a low dimensional representation of a data set in the presence of independent spherical Gaussian noise, Sigma = (sigma^2)*I. The maximum likelihood solution for the model is an eigenvalue problem on the sample covariance matrix. In this paper we consider the situation where the data variance is already partially explained by other factors, e.g. covariates of interest, or temporal correlations leaving some residual variance. We decompose the residual variance into its components through a generalized eigenvalue problem, which we call residual component analysis (RCA). We show that canonical covariates analysis (CCA) is a special case of our algorithm and explore a range of new algorithms that arise from the framework. We illustrate the ideas on a gene expression time series data set and the recovery of human pose from silhouette.},
  OPTgroup = 	 {},
  archivePrefix ={arXiv},
  eprint =	 {1106.4333},
  primaryClass = {stat.ML},
}
@TechReport{Alvarez:llfm11,
  author = 	 {Mauricio A. \'Alvarez and David Luengo and Neil D. Lawrence},
  title = 	 {Linear Latent Force Models Using {G}aussian Processes},
  institution =  {University of Sheffield},
  year = 	 2011,
  OPTkey = 	 {},
  OPTtype = 	 {},
  number = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 {http://arxiv.org/pdf/1107.2699},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  linksoftware = softwarehttp # {multigp},
  OPTlinksoftware = {},
  abstract =	 {Purely data driven approaches for machine learning present 
                 difficulties when data is scarce relative to the
                 complexity of the model or when the model is forced
                 to extrapolate. On the other hand, purely mechanistic
                 approaches need to identify and specify all the
                 interactions in the problem at hand (which may not be
                 feasible) and still leave the issue of how to
                 parameterize the system. In this paper, we present a
                 hybrid approach using Gaussian processes and
                 differential equations to combine data driven
                 modelling with a physical model of the system. We
                 show how different, physically-inspired, kernel
                 functions can be developed through sensible, simple,
                 mechanistic assumptions about the underlying
                 system. The versatility of our approach is
                 illustrated with three case studies from motion
                 capture, computational biology and geostatistics.},
  OPTgroup = 	 {},
  archivePrefix ={arXiv},
  eprint =	 {1107.2699},
  primaryClass = {stat.ML}
}
@TechReport{Alvarez:kernels11,
  author = 	 {Mauricio A. \'Alvarez and Lorenzo Rosasco and Neil D. Lawrence},
  title = 	 {Kernels for Vector-Valued Functions: a Review},
  institution =  {University of Sheffield},
  year = 	 2011,
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 {http://arxiv.org/pdf/1106.6251v1},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  linksoftware = softwarehttp # {multigp},
  OPTlinksoftware = {},
  abstract =	 {Kernel methods are among the most popular techniques in machine learning. From a frequentist/discriminative perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a Bayesian/generative perspective they are the key in the context of Gaussian processes, where the kernel function is also known as the covariance function. Traditionally, kernel methods have been used in supervised learning problem with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partly by frameworks like multitask learning. In this paper, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods.},
  OPTgroup = 	 {},
  archivePrefix ={arXiv},
  eprint =	 {1106.6251},
  primaryClass = {stat.ML},
}


@TechReport{Lawrence:unifying10,
  author = 	 {Neil D. Lawrence},
  title = 	 {A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction},
  institution =  {University of Sheffield},
  year = 	 2010,
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 {http://arxiv.org/pdf/1010.4830v1},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  linksoftware = softwarehttp # {meu},
  OPTlinksoftware = {},
  abstract =	 {We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian random fields (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting probabilistic models are based on GRFs. The resulting model is a nonlinear generalization of principal component analysis. We show that parameter fitting in the locally linear embedding is approximate maximum likelihood in these models. We develop new algorithms that directly maximize the likelihood and show that these new algorithms are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random field via the graphical lasso.},
  OPTgroup = 	 {},
  archivePrefix ={arXiv},
  eprint =	 {1010.4830},
  primaryClass = {cs-AI},
}


@TechReport{Alvarez:vikTech09,
  author =	 {Mauricio A. \'Alvarez and David Luengo and Michalis
                  K. Titsias and Neil D. Lawrence},
  title =	 {Variational Inducing Kernels for Sparse Convolved
                  Multiple Output {G}aussian Processes },
  institution =	 {University of Manchester},
  year =	 {2009},
  OPTkey =	 {},
  OPTtype =	 {},
  OPTnumber =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =	 {http://arxiv.org/pdf/0912.3268v1},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  OPTlinksoftware ={},
  archivePrefix ={arXiv},
  eprint =	 {0912.3268},
  primaryClass = {stat-ml},
  abstract =	 {Interest in multioutput kernel methods is
                  increasing, whether under the guise of multitask
                  learning, multisensor networks or structured output
                  data. From the Gaussian process perspective a
                  multioutput Mercer kernel is a covariance function
                  over correlated output functions. One way of
                  constructing such kernels is based on convolution
                  processes (CP). A key problem for this approach is
                  efficient inference. \'Alvarez and Lawrence (2009)
                  recently presented a sparse approximation for CPs
                  that enabled efficient inference. In this paper, we
                  extend this work in two directions: we introduce the
                  concept of variational inducing functions to handle
                  potential non-smooth functions involved in the
                  kernel CP construction and we consider an
                  alternative approach to approximate inference based
                  on variational methods, extending the work by
                  Titsias (2009) to the multiple output case. We
                  demonstrate our approaches on prediction of school
                  marks, compiler performance and financial time
                  series. },
  OPTgroup =	 {}
}

@TechReport{Alvarez:multiTech09,
  author =	 {Mauricio A. \'Alvarez and Neil D. Lawrence},
  title =	 {Sparse Convolved Multiple Output {G}aussian
                  Processes},
  journal =	 {},
  year =	 {2009},
  OPTkey =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTpages =	 {},
  OPTmonth =	 {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =	 {http://arxiv.org/pdf/0911.5107v1},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  OPTlinksoftware ={},
  archivePrefix ={arXiv},
  eprint =	 {0911.5107v1},
  primaryClass = {stat-ml},
  abstract =	 {Recently there has been an increasing interest in
                  methods that deal with multiple outputs. This has
                  been motivated partly by frameworks like multitask
                  learning, multisensor networks or structured output
                  data. From a Gaussian processes perspective, the
                  problem reduces to specifying an appropriate
                  covariance function that, whilst being positive
                  semi-definite, captures the dependencies between all
                  the data points and across all the outputs. One
                  approach to account for non-trivial correlations
                  between outputs employs convolution processes. Under
                  a latent function interpretation of the convolution
                  transform we establish dependencies between output
                  variables. The main drawbacks of this approach are
                  the associated computational and storage demands. In
                  this paper we address these issues. We present
                  different sparse approximations for dependent output
                  Gaussian processes constructed through the
                  convolution formalism. We exploit the conditional
                  independencies present naturally in the model. This
                  leads to a form of the covariance similar in spirit
                  to the so called PITC and FITC approximations for a
                  single output. We show experimental results with
                  synthetic and real data, in particular, we show
                  results in pollution prediction, school exams score
                  prediction and gene expression data.},
  OPTgroup =	 {}
}

@Article{Pearson:puma09,
  author =	 {Richard D. Pearson and Xuejun Liu and Guido
                  Sanguinetti and Marta Milo and Neil D. Lawrence and
                  Magnus Rattray},
  title =	 {puma: a {B}ioconductor package for Propagating
                  Uncertainty in Microarray Analysis},
  journal =	 {BMC Bioinformatics},
  year =	 2009,
  OPTkey =	 {},
  volume =	 10,
  number =	 211,
  OPTpages =	 {},
  OPTmonth =	 {},
  OPTnote =	 {},
  OPTannote =	 {},
  pmid =	 {19589155},
  doi =		 {10.1186/1471-2105-10-211},
  OPTlinkpdf =	 {},
  abstract =	 {{\bf Background}\\\\ Most analyses of microarray
                  data are based on point estimates of expression
                  levels and ignore the uncertainty of such
                  estimates. By determining uncertainties from
                  Affymetrix GeneChip data and propagating these
                  uncertainties to downstream analyses it has been
                  shown that we can improve results of differential
                  expression detection, principal component analysis
                  and clustering. Previously, implementations of these
                  uncertainty propagation methods have only been
                  available as separate packages, written in different
                  languages. Previous implementations have also
                  suffered from being very costly to compute, and in
                  the case of differential expression detection, have
                  been limited in the experimental designs to which
                  they can be applied.\\\\ {\bf Results}\\\\ puma is a
                  Bioconductor package incorporating a suite of
                  analysis methods for use on Affymetrix GeneChip
                  data. puma extends the differential expression
                  detection methods of previous work from the 2-class
                  case to the multi-factorial case. puma can be used
                  to automatically create design and contrast matrices
                  for typical experimental designs, which can be used
                  both within the package itself but also in other
                  Bioconductor packages. The implementation of
                  differential expression detection methods has been
                  parallelised leading to significant decreases in
                  processing time on a range of computer
                  architectures. puma incorporates the first R
                  implementation of an uncertainty propagation version
                  of principal component analysis, and an
                  implementation of a clustering method based on
                  uncertainty propagation. All of these techniques are
                  brought together in a single, easy-to-use package
                  with clear, task-based documentation.\\\\ {\bf
                  Conclusions}\\\\ For the first time, the puma
                  package makes a suite of uncertainty propagation
                  methods available to a general audience. These
                  methods can be used to improve results from more
                  traditional analyses of microarray data. puma also
                  offers improvements in terms of scope and speed of
                  execution over previously available methods. puma is
                  recommended for anyone working with the Affymetrix
                  GeneChip platform for gene expression analysis and
                  can also be applied more generally.},
  group =	 {puma}
}

@InCollection{Lawrence:licsbintro10,
  author =	 {Neil D. Lawrence},
  title =	 {Introduction to Learning and Inference in
                  Computational Systems Biology},
  chapter =	 1,
  year =	 2010,
  crossref =	 {Lawrence:licsb10},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTtype =	 {},
  OPTaddress =	 {},
  OPTedition =	 {},
  OPTmonth =	 {},
  OPTpages =	 {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  OPTlinkpdf =	 {},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  OPTlinksoftware ={},
  OPTabstract =	 {},
  OPTgroup =	 {}
}

@InCollection{Lawrence:licsbbayes10,
  author =	 {Neil D. Lawrence and Magnus Rattray},
  title =	 {A Brief Introduction to {B}ayesian Inference},
  chapter =	 5,
  year =	 2010,
  crossref =	 {Lawrence:licsb10},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTtype =	 {},
  OPTaddress =	 {},
  OPTedition =	 {},
  OPTmonth =	 {},
  OPTpages =	 {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  OPTlinkpdf =	 {},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  OPTlinksoftware ={},
  OPTabstract =	 {},
  OPTgroup =	 {}
}

@InCollection{Lawrence:licsbgp10,
  author =	 {Neil D. Lawrence and Magnus Rattray and Pei Gao and
                  Michalis K. Titsias},
  title =	 {Gaussian Processes for Missing Species in
                  Biochemical Systems},
  chapter =	 9,
  year =	 2010,
  crossref =	 {Lawrence:licsb10},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTtype =	 {},
  OPTaddress =	 {},
  OPTedition =	 {},
  OPTmonth =	 {},
  OPTpages =	 {},
  OPTnote =	 {},
  OPTannote =	 {},
  pmid =	 {18689843},
  OPTdoi =	 {},
  OPTlinkpdf =	 {},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  OPTlinksoftware ={},
  OPTabstract =	 {},
  OPTgroup =	 {}
}

@InProceedings{Alvarez:lfm09,
  author =	 {Maurico A. \'Alvarez and David Luengo and Neil D. Lawrence},
  title =	 {Latent Force Models},
  crossref =	 {Welling:aistats09},
  pages =	 {9--16},
  year =	 2009,
  OPTeditor =	 {},
  OPTvolume =	 5,
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =      {http://jmlr.csail.mit.edu/proceedings/papers/v5/alvarez09a/alvarez09a.pdf},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  linksoftware = softwarehttp # {multigp},
  abstract =	 {Purely data driven approaches for machine learning
                  present difficulties when data is scarce relative to
                  the complexity of the model or when the model is
                  forced to extrapolate. On the other hand, purely
                  mechanistic approaches need to identify and specify
                  all the interactions in the problem at hand (which
                  may not be feasible) and still leave the issue of
                  how to parameterize the system. In this paper, we
                  present a hybrid approach using Gaussian processes
                  and differential equations to combine data driven
                  modeling with a physical model of the system. We
                  show how different, physically-inspired, kernel
                  functions can be developed through sensible, simple,
                  mechanistic assumptions about the underlying
                  system. The versatility of our approach is
                  illustrated with three case studies from
                  computational biology, motion capture and
                  geostatistics.},
  OPTgroup =	 {}
}

@InProceedings{Lawrence:nlmf09,
  author =	 {Neil D. Lawrence and Raquel Urtasun},
  title =	 {Non-Linear Matrix Factorization with {G}aussian
                  Processes},
  crossref =	 {Bottou:icml09},
  OPTkey =	 {},
  OPTbooktitle = {},
  OPTpages =	 {},
  OPTyear =	 {},
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =	 shefftp # {collab.pdf},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  linksoftware = softwarehttp # {collab/},
  abstract =	 {A popular approach to collaborative filtering is
                  matrix factorization. In this paper we develop a
                  non-linear probabilistic matrix factorization using
                  Gaussian process latent variable models. We use
                  stochastic gradient descent (SGD) to optimize the
                  model. SGD allows us to apply Gaussian processes to
                  data sets with millions of observations without
                  approximate methods. We apply our approach to
                  benchmark movie recommender data sets. The results
                  show better than previous state-of-the-art
                  performance.},
  group =	 {gplvm, collaborative filtering}
}

@InProceedings{Ek:ambiguity08,
  author =	 {Carl Henrik Ek and Jon Rihan and Philip Torr and
                  Gregory Rogez and Neil D. Lawrence},
  title =	 {Ambiguity Modeling in Latent Spaces},
  OPTcrossref =	 {},
  OPTkey =	 {},
  booktitle =	 {Machine Learning for Multimodal Interaction (MLMI
                  2008)},
  pages =	 {62--73},
  year =	 2008,
  editor =	 {Andrei {Popescu-Belis} and Rainer Stiefelhagen},
  OPTvolume =	 {},
  OPTnumber =	 {},
  series =	 {LNCS},
  OPTaddress =	 {},
  month =	 {28--30 June},
  OPTorganization ={},
  publisher =	 springer,
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =	 shefftp # {mlmi2008.pdf},
  OPTlinkpsgz =	 {},
  linksoftware = softwarehttp # {sgplvm/},
  abstract =	 {We are interested in the situation where we have two
                  or more representations of an underlying
                  phenomenon. In particular we are interested in the
                  scenario where the representation are
                  complementary. This implies that a single individual
                  representation is not sufficient to fully
                  discriminate a specific instance of the underlying
                  phenomenon, it also means that each representation
                  is an ambiguous representation of the other
                  complementary spaces. In this paper we present a
                  latent variable model capable of consolidating
                  multiple complementary representations. Our method
                  extends canonical correlation analysis by
                  introducing additional latent spaces that are
                  specific to the different representations, thereby
                  explaining the full variance of the
                  observations. These additional spaces, explaining
                  representation specific variance, separately model
                  the variance in a representation ambiguous to the
                  other. We develop a spectral algorithm for fast
                  computation of the embeddings and a probabilistic
                  model (based on Gaussian processes) for validation
                  and inference. The proposed model has several
                  potential application areas, we demonstrate its use
                  for multi-modal regression on a benchmark human pose
                  estimation data set.},
  OPTgroup =	 {}
}

@InProceedings{Urtasun:topology08,
  author =	 {Raquel Urtasun and David J. Fleet and Andreas Geiger
                  and Jovan Popovi\'c and Trevor J. Darrell and Neil D. Lawrence},
  title =	 {Topologically-Constrained Latent Variable Models},
  crossref =	 {Roweis:icml08},
  OPTkey =	 {},
  OPTbooktitle = {},
  pages =	 {1080-1087},
  year =	 2008,
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  doi =		 {10.1145/1390156.1390292},
  linkpdf =	 shefftp # {topology.pdf},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  OPTlinksoftware ={},
  abstract =	 {In dimensionality reduction approaches, the data are
                  typically embedded in a Euclidean latent
                  space. However for some data sets this is
                  inappropriate. For example, in human motion data we
                  expect latent spaces that are cylindrical or a
                  toroidal, that are poorly captured with a Euclidean
                  space. In this paper, we present a range of
                  approaches for embedding data in a non-Euclidean
                  latent space. Our focus is the Gaussian Process
                  latent variable model. In the context of human
                  motion modeling this allows us to (a) learn models
                  with interpretable latent directions enabling, for
                  example, style/content separation, and (b)
                  generalise beyond the data set enabling us to learn
                  transitions between motion styles even though such
                  transitions are not present in the data.},
  group =	 {}
}

@InProceedings{Ek:pose07,
  author =	 {Carl Henrik Ek and Philip H.S. Torr and Neil
                  D. Lawrence},
  title =	 {Gaussian Process Latent Variable Models For Human
                  Pose Estimation},
  OPTcrossref =	 {},
  OPTkey =	 {},
  booktitle =	 {Machine Learning for Multimodal Interaction (MLMI
                  2007)},
  pages =	 {132--143},
  year =	 2008,
  editor =	 {Andrei {Popescu-Belis} and Steve Renals and Herv\'e
                  Bourlard},
  volume =	 4892,
  OPTnumber =	 {},
  series =	 {LNCS},
  address =	 {Brno, Czech Republic},
  OPTmonth =	 {},
  OPTorganization ={},
  publisher =	 springer,
  OPTannote =	 {},
  OPTpmid =	 {},
  doi =		 {10.1007/978-3-540-78155-4_12},
  linkpdf =	 shefftp # {mlmi.pdf},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  linksoftware = softwarehttp # {sgplvm/},
  abstract =	 {We describe a method for recovering 3D human body
                  pose from silhouettes. Our model is based on
                  learning a latent space using the Gaussian Process
                  Latent Variable Model (GP-LVM) [1] encapsulating
                  both pose and silhouette features Our method is
                  generative, this allows us to model the ambiguities
                  of a silhouette representation in a principled
                  way. We learn a dynamical model over the latent
                  space which allows us to disambiguate between
                  ambiguous silhouettes by temporal consistency. The
                  model has only two free parameters and has several
                  advantages over both regression approaches and other
                  generative methods. In addition to the application
                  shown in this paper the suggested model is easily
                  extended to multiple observation spaces without
                  constraints on type.},
  group =	 {gplvm,pose estimation}
}

@InProceedings{Eciolaza:fault07,
  author =	 {Luka Eciolaza and M. Alkarouri and Neil D. Lawrence
                  and Visakan Kadirkamanathan and Peter J. Fleming},
  title =	 {Gaussian Process Latent Variable Models for Fault
                  Detection},
  OPTcrossref =	 {},
  OPTkey =	 {},
  booktitle =	 {Computational Intelligence and Data Mining},
  pages =	 {287--292},
  year =	 2007,
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  doi =		 {10.1109/CIDM.2007.368886},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTlinkpdf =	 {},
  OPTlinksoftware ={},
  abstract =	 {The Gaussian process latent variable model (GPLVM)
                  is a novel unsupervised approach to nonlinear low
                  dimensional embedding proposed by Lawrence
                  (2005). This paper presents the development of a
                  framework for the implementation of the GPLVM for
                  fault detection. A series of experiments have been
                  carried out comparing and combining the GPLVM to the
                  conventional and widely used linear dimension
                  reduction technique of principal component analysis
                  (PCA). The inclusion of the GPLVM for the
                  visualisation and data analysis, led to a
                  considerable improvement in the classification
                  results},
  group =	 {gplvm}
}

@InProceedings{Lawrence:hgplvm07,
  author =	 {Neil D. Lawrence and Andrew J. Moore},
  title =	 {Hierarchical {G}aussian Process Latent Variable
                  Models},
  crossref =	 {Ghahramani:icml07},
  pages =	 {481--488},
  abstract =	 {The Gaussian process latent variable model (GP-LVM)
                  is a powerful approach for probabilistic modelling
                  of high dimensional data through dimensional
                  reduction. In this paper we extend the GP-LVM
                  through hierarchies. A hierarchical model (such as a
                  tree) allows us to express conditional
                  independencies in the data as well as the manifold
                  structure. We first introduce Gaussian process
                  hierarchies through a simple dynamical model, we
                  then extend the approach to a more complex hierarchy
                  which is applied to the visualisation of human
                  motion data sets.},
  year =	 2007,
  linkpdf =	 shefftp # {hgplvm.pdf},
  linksoftware = softwarehttp # {hgplvm/},
  group =	 {manml,gp,gplvm,dimensional reduction}
}

@InProceedings{Laidler:model07,
  author =	 {Jonathan Laidler and Martin Cooke and Neil
                  D. Lawrence},
  title =	 {Model-driven detection of Clean Speech Patches in
                  Noise},
  OPTcrossref =	 {},
  OPTkey =	 {},
  booktitle =	 {Proceedings of Interspeech 2007},
  OPTpages =	 {},
  year =	 2007,
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {Antwerp, Belgium},
  month =	 {August 27th-31st},
  OPTorganization ={},
  OPTpublisher = {},
  note =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  linkpdf =	 shefftp # {LaidlerInterspeech2007.pdf},
  OPTlinksoftware ={},
  abstract =	 {Listeners may be able to recognise speech in adverse
                  conditions by glimpsing time-frequency regions where
                  the target speech is dominant. Previous
                  computational attempts to identify such regions have
                  been source-driven, using primitive cues. This paper
                  describes a model-driven approach in which the
                  likelihood of spectro-temporal patches of a noisy
                  mixture representing speech is given by a generative
                  model. The focus is on patch size and patch
                  modelling. Small patches lead to a lack of
                  discrimination, while large patches are more likely
                  to contain contributions from other sources. A
                  cleanness measure reveals that a good patch size is
                  one which extends over a quarter of the speech
                  frequency range and lasts for 40 ms. Gaussian
                  mixture models are used to represent patches. A
                  compact representation based on a 2D discrete cosine
                  transform leads to reasonable speech/background
                  discrimination.},
  group =	 {speech separation, glimpsing, model-driven,
                  spectro-temporal patches}
}

@InProceedings{Titsias:efficient08,
  author =	 {Michalis K. Titsias and Neil D. Lawrence and Magnus
                  Rattray},
  title =	 {Efficient Sampling for {G}aussian Process Inference
                  using Control Variables},
  crossref =	 {Koller:nips08},
  OPTkey =	 {},
  OPTbooktitle = {},
  pages =	 {1681--1688},
  OPTyear =	 {},
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =	 shefftp # {nipsSamGP08.pdf},
  OPTlinksoftware ={},
  abstract =	 {Sampling functions in Gaussian process (GP) models
                  is challenging because of the highly correlated
                  posterior distribution. We describe an efficient
                  Markov chain Monte Carlo algorithm for sampling from
                  the posterior process of the GP model. This
                  algorithm uses control variables which are auxiliary
                  function values that provide a low dimensional
                  representation of the function. At each iteration,
                  the algorithm proposes new values for the control
                  variables and generates the function from the
                  conditional GP prior. The control variable input
                  locations are found by continuously minimizing an
                  objective function. We demonstrate the algorithm on
                  regression and classification problems and we use it
                  to estimate the parameters of a differential
                  equation model of gene regulation.},
  OPTgroup =	 {}
}

@InProceedings{Alvarez:convolved08,
  author =	 {Maurico A. \'Alvarez and Neil D. Lawrence},
  title =	 {Sparse Convolved {G}aussian Processes for
                  Multi-output Regression},
  crossref =	 {Koller:nips08},
  linkpdf =	 shefftp # {spmulti.pdf},
  OPTkey =	 {},
  OPTbooktitle = {},
  pages =	 {57--64},
  OPTyear =	 {},
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  OPTlinkpdf =	 {},
  linksoftware = softwarehttp # {multigp/},
  abstract =	 {We present a sparse approximation approach for
                  dependent output Gaussian processes (GP). Employing
                  a latent function framework, we apply the
                  convolution process formalism to establish
                  dependencies between output variables, where each
                  latent function is represented as a GP. Based on
                  these latent functions, we establish an
                  approximation scheme using a conditional
                  independence assumption between the output
                  processes, leading to an approximation of the full
                  covariance which is determined by the locations at
                  which the latent functions are evaluated. We show
                  results of the proposed methodology for synthetic
                  data and real world applications on pollution
                  prediction and a sensor network.},
  OPTgroup =	 {}
}

@InProceedings{Calderhead:accelerating08,
  author =	 {Ben Calderhead and Mark Girolami and Neil D. Lawrence},
  title =	 {Accelerating {B}ayesian Inference over Nonlinear
                  Differential Equations with {G}aussian Processes},
  crossref =	 {Koller:nips08},
  OPTkey =	 {},
  OPTbooktitle = {},
  pages =	 {217--224},
  OPTyear =	 {},
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  OPTlinkpdf =	 {},
  OPTlinksoftware ={},
  abstract =	 {Identification and comparison of nonlinear dynamical
                  system models using noisy and sparse experimental
                  data is a vital task in many fields, however current
                  methods are computationally expensive and prone to
                  error due in part to the nonlinear nature of the
                  likelihood surfaces induced. We present an
                  accelerated sampling procedure which enables
                  Bayesian inference of parameters in nonlinear
                  ordinary and delay differential equations via the
                  novel use of Gaussian processes (GP). Our methdo
                  involves GP regression over time-series data, and
                  the resulting derivative and time delay estimates
                  make parameter inference possible \emph{without}
                  solving the dynamical system explicitly, resulting
                  in dramatic savings of computational time. We
                  demonstrate the speed and statistical accuracy of
                  our approach using examples of both ordinary and
                  elay differential equations, and provide a
                  comprehensive comparison with current state of the
                  art methods.},
  group =	 {GP, differential equations, systems biology}
}

@Article{Gao:latent08,
  author =	 {Pei Gao and Antti Honkela and Magnus Rattray and
                  Neil D. Lawrence},
  title =	 {Gaussian Process Modelling of Latent Chemical
                  Species: Applications to Inferring Transcription
                  Factor Activities},
  journal =	 bioinf,
  year =	 2008,
  OPTkey =	 {},
  volume =	 24,
  OPTnumber =	 {},
  pages =	 {i70--i75},
  OPTmonth =	 {},
  OPTnote =	 {To appear at ECCB '08},
  OPTannote =	 {},
  OPTpmid =	 {},
  doi =		 {10.1093/bioinformatics/btn278},
  linkpdf =
                  {http://bioinformatics.oxfordjournals.org/cgi/reprint/24/16/i70.pdf?ijkey=FauSn114lAUC1Ey&keytype=ref},
  linksoftware = softwarehttp # {gpsim/},
  abstract =	 {{\bf Motivation:} Inference of \emph{latent chemical
                  species} in biochemical interaction networks is a
                  key problem in estimation of the structure and
                  parameters of the genetic, metabolic and protein
                  interaction networks that underpin all biological
                  processes. We present a framework for Bayesian
                  marginalisation of these latent chemical species
                  through Gaussian process priors.\\\\ {\bf Results:}
                  We demonstrate our general approach on three
                  different biological examples of single input
                  motifs, including both activation and repression of
                  transcription. We focus in particular on the problem
                  of inferring transcription factor activity when the
                  concentration of active protein cannot easily be
                  measured. We show how the uncertainty in the
                  inferred transcription factor activity can be
                  integrated out in order to derive a likelihood
                  function that can be used for the estimation of
                  regulatory model parameters. An advantage of our
                  approach is that we avoid the use of a
                  coarse-grained discretization of continuous-time
                  functions, which would lead to a large number of
                  additional parameters to be estimated. We develop
                  efficient exact and approximate inference schemes,
                  which are much more efficient than competing
                  sampling-based schemes and therefore provide us with
                  a practical toolkit for model-based inference.\\\\
                  {\bf Availability:} The software and data for
                  recreating all the experiments in this paper is
                  available in MATLAB from
                  \url{http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/gpsim}\\\\
                  {\bf Contact:} Neil Lawrence},
  group =	 {gene networks, TFA, gp}
}

@InProceedings{Ferris:wifi07,
  author =	 {Brian D. Ferris and Dieter Fox and Neil D. Lawrence},
  title =	 {{WiFi-SLAM} Using {G}aussian Process Latent Variable
                  Models},
  OPTcrossref =	 {},
  OPTkey =	 {},
  booktitle =	 {Proceedings of the 20th International Joint
                  Conference on Artificial Intelligence (IJCAI 2007)},
  pages =	 {2480--2485},
  year =	 2007,
  editor =	 {Manuela M. Veloso},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  linkpdf =
                  {http://www.ijcai.org/papers07/Papers/IJCAI07-399.pdf},
  OPTlinksoftware ={},
  abstract =	 {WiFi localization, the task of determining the
                  physical location of a mobile device from wireless
                  signal strengths, has been shown to be an accurate
                  method of indoor and outdoor localization and a
                  powerful building block for location-aware
                  applications. However, most localization techniques
                  require a training set of signal strength readings
                  labeled against a ground truth location map, which
                  is prohibitive to collect and maintain as maps grow
                  large. In this paper we propose a novel technique
                  for solving the WiFi SLAM problem using the Gaussian
                  Process Latent Variable Model (GP-LVM) to determine
                  the latent-space locations of unlabeled signal
                  strength data. We show how GP-LVM, in combination
                  with an appropriate motion dynamics model, can be
                  used to reconstruct a topological connectivity graph
                  from a signal strength sequence which, in
                  combination with the learned Gaussian Process signal
                  strength model, can be used to perform efficient
                  localization.},
  group =	 {gplvm,dimensional reduction}
}

@Article{Sanguinetti:chipvar06,
  author =	 {Guido Sanguinetti and Neil D. Lawrence and Magnus
                  Rattray},
  title =	 {Probabilistic inference of transcription factor
                  concentrations and gene-specific regulatory
                  activities},
  year =	 2006,
  journal =	 bioinf,
  linksoftware = softwarehttp # {chipvar/},
  volume =	 22,
  number =	 22,
  doi =		 {10.1093/bioinformatics/btl473},
  pmid =	 16966362,
  pages =	 {2275--2281},
  linkpdf =
                  {http://bioinformatics.oxfordjournals.org/cgi/reprint/btl473v1},
  abstract =	 {{\bf Motivation}: Quantitative estimation of the
                  regulatory relationship between transcription
                  factors and genes is a fundamental stepping stone
                  when trying to develop models of cellular
                  processes. Recent experimental high-throughput
                  techniques such as Chromatine Immunoprecipitation
                  provide important information about the architecture
                  of the regulatory networks in the cell. However, it
                  is very difficult to measure the concentration
                  levels of transcription factor proteins and
                  determine their regulatory effect on gene
                  transcription. It is therefore an important
                  computational challenge to infer these quantities
                  using gene expression data and network architecture
                  data.\\\\ {\bf Results}: We develop a probabilistic
                  state space model that allows genome-wide inference
                  of both transcription factor protein concentrations
                  and their effect on the transcription rates of each
                  target gene from microarray data. We use variational
                  inference techniques to learn the model parameters
                  and perform posterior inference of protein
                  concentrations and regulatory strengths. The
                  probabilistic nature of the model also means that we
                  can associate credibility intervals to our
                  estimates, as well as providing a tool to detect
                  which binding events lead to significant
                  regulation. We demonstrate our model on artificial
                  data and on two yeast data sets in which the network
                  structure has previously been obtained using
                  Chromatine Immunoprecipitation data. Predictions
                  from our model are consistent with the underlying
                  biology and offer novel quantitative insights into
                  the regulatory structure of the yeast cell.\\\\ {\bf
                  Availability}: MATLAB code is available from
                  \url{http://umber.sbs.man.ac.uk/resources/puma}.},
  errata1 =	 {Equation (5) the normal density for $y_n(t)$ should
                  have existing term $+ \mu_n(t)$ for the mean.},
  errataCredit1 = {Kevin Sharp},
  errata2 =	  {Equation (7), the two terms that involve
                  $\mathbf{K}$ should be outside the sum over
                  $n$. There should be no $T$ in the term
                  $Tq\log(\alpha^2)$.},
  errataCredit2 = {Junfeng Chen},
  errata3 =	 {Equation (8), $\alpha^2$ should be $\alpha^{-2}$.},
  errataCredit3 = {Junfeng Chen},
  errata4 =	 {Equation (10), second line, there should be $T$
                  before the $\sigma^{-2}$.},
  errataCredit4 = {Junfeng Chen},
  errata5 =	 {Equation (11), sums should be normalised, first one
                  by $Nq$, second one by $NT$.},
  errataCredit5 = {Junfeng Chen},
  group =	 {shefml,puma,gene networks}
}

@TechReport{Sanguinetti:integrate06,
  author =	 {Guido Sanguinetti and Magnus Rattray and Neil D. Lawrence},
  title =	 {A Probabilistic Model to Integrate Chip and
                  Microarray Data},
  institution =	 {Department of Computer Science, University of
                  Sheffield},
  year =	 2006,
  OPTkey =	 {},
  OPTtype =	 {},
  number =	 {CS-06-02},
  linkpdf =	 shefftp # {chipTech.pdf},
  linksoftware = softwarehttp # {chipdyno/},
  address =	 {},
  OPTmonth =	 {},
  OPTnote =	 {},
  group =	 {shefml,puma,gene networks}
}

@Article{Rattray:propagating06,
  author =	 {Magnus Rattray and Xuejun Liu and Guido Sanguinetti
                  and Marta Milo and Neil D. Lawrence},
  title =	 {Propagating Uncertainty in Microarray Data Analysis},
  journal =	 {Briefings in Bioinformatics},
  year =	 2006,
  volume =	 7,
  number =	 1,
  pages =	 {37--47},
  pmid =	 16761363,
  errata1 =	 {The error bars in Figure 3 are incorrect, as
                  explained in the errata to Liu et al. Bioinformatics
                  22, 2107-2113 \cite{Liu:variances06}.},
  errataCredit1 ={Richard Pearson},
  linkpdf =	 {http://bib.oxfordjournals.org/cgi/reprint/7/1/37},
  abstract =	 {Microarray technology is associated with many
                  sources of experimental uncertainty. In this review
                  we discuss a number of approaches for dealing with
                  this uncertainty in the processing of data from
                  microarray experiments. We focus here on the
                  analysis of high-density oligonucleotide arrays,
                  such as the popular Affymetrix GeneChip array,
                  which contain multiple probes for each target. This
                  set of probes can be used to determine an estimate
                  for the target concentration and can also be used to
                  determine the experimental uncertainty associated
                  with this measurement. This measurement uncertainty
                  can then be propagated through the downstream
                  analysis using probabilistic methods. We give
                  examples showing how these credibility intervals can
                  be used to help identify differential expression, to
                  combine information from replicated experiments and
                  to improve the performance of principal component
                  analysis.},
  group =	 {puma,shefml}
}

@Article{Liu:variances06,
  author =	 {Xuejun Liu and Marta Milo and Neil D. Lawrence and
                  Magnus Rattray},
  title =	 {Probe-level Measurement Error Improves Accuracy in
                  Detecting Differential Gene Expression},
  journal =	 bioinf,
  year =	 2006,
  doi =		 {10.1093/bioinformatics/btl361},
  volume =	 22,
  number =	 17,
  pages =	 {2107--2113},
  pmid =	 16820429,
  linkpdf =
                  {http://bioinformatics.oxfordjournals.org/cgi/reprint/btl361v1.pdf},
  OPTlabel1 =	 {Advance Access},
  OPTlink1 =
                  {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/btl361v1},
  group =	 {shefml,puma,gene networks},
  errata1 =	 {The error bars in Figure 2 were incorrectly
                  calculated. The posterior variance was used instead
                  of the posterior standard deviation to compute the
                  credibility intervals. The ranking was correct in
                  both plots. Here is the revised figure:
                  \url{http://www.bioinf.manchester.ac.uk/resources/puma/intervals.pdf}. There
                  are now 7 false positives in to top 50 genes ranked
                  by differential expression in the left-hand
                  plot. None of the main quantitative results in the
                  paper (i.e. PPLR values, ROC plots, AUC scores) are
                  affected. The main point is that ranking by
                  differential expression alone leads to many false
                  positives while using the PPLR criterion will
                  greatly reduce the number of false positives. This
                  conclusion remains valid.},
  errataCredit1 ={Richard Pearson},
  abstract =	 {{\bf Motivation:} Finding differentially expressed
                  genes is a fundamental objective of a microarray
                  experiment. Numerous methods have been proposed to
                  perform this task. Existing methods are based on
                  point estimates of gene expression level obtained
                  from each microarray experiment. This approach
                  discards potentially useful information about
                  measurement error that can be obtained from an
                  appropriate probe-level analysis. Probabilistic
                  probe-level models can be used to measure gene
                  expression and also provide a level of uncertainty
                  in this measurement. This probe-level variance
                  provides useful information which can help in the
                  identification of differentially expressed
                  genes.\\\\ {\bf Results:} We propose a Bayesian
                  method to include probe-level variances into the
                  detection of differentially expressed genes from
                  replicated experiments. A variational approximation
                  is used for effcient parameter estimation. We
                  compare this approximation with MAP and MCMC
                  parameter estimation in terms of computational
                  effciency and accuracy. The method is used to
                  calculate the probability of positive log-ratio
                  (PPLR) of expression levels between
                  conditions. Using the measurements from a recently
                  developed Affymetrix probe-level model, multi-mgMOS,
                  we test PPLR on a spike-in data set and a mouse
                  time-course data set. Results show that the
                  inclusion of probelevel measurement error improves
                  accuracy in detecting differential gene
                  expression.\\\\ {\bf Availability:} The methods
                  described in this paper have been implemented in an
                  R package \emph{pplr} that is currently available
                  from
                  \url{http://umber.sbs.man.ac.uk/resources/puma}.\\\\
                  {\bf Contact:} Magnus Rattray}
}

@InProceedings{Sanguinetti:trento06,
  author =	 {Guido Sanguinetti and Magnus Rattray and Neil D. Lawrence},
  title =	 {Identifying submodules of cellular regulatory
                  networks},
  OPTcrossref =	 {},
  OPTkey =	 {},
  booktitle =	 {International Conference on Computational Methods in
                  Systems Biology},
  OPTpages =	 {155--168},
  year =	 {2006},
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  series =	 {LNCS},
  doi =		 {10.1007/11885191_11},
  OPTmonth =	 {},
  OPTorganization ={},
  publisher =	 springer,
  abstract =	 {Recent high throughput techniques in molecular
                  biology have brought about the possibility of
                  directly identifying the architecture of regulatory
                  networks on a genome-wide scale. However, the
                  computational task of estimating fine-grained models
                  on a genome-wide scale is daunting. Therefore, it is
                  of great importance to be able to reliably identify
                  submodules of the network that can be effectively
                  modelled as independent subunits. In this paper we
                  present a procedure to obtain submodules of a
                  cellular network by using information from
                  gene-expression measurements. We integrate network
                  architecture data with genome-wide gene expression
                  measurements in order to determine which regulatory
                  relations are actually confirmed by the expression
                  data. We then use this information to obtain
                  non-trivial submodules of the regulatory network
                  using two distinct algorithms, a naive exhaustive
                  algorithm and a spectral algorithm based on the
                  eigendecomposition of an affinity matrix. We test
                  our method on two yeast biological data sets, using
                  regulatory information obtained from chromatin
                  immunoprecipitation.},
  group =	 {gene networks,shefml,puma}
}

@Article{Lawrence:pnpca05,
  author =	 {Neil D. Lawrence},
  title =	 {Probabilistic Non-linear Principal Component
                  Analysis with {G}aussian Process Latent Variable
                  Models},
  journal =	 jmlr,
  year =	 2005,
  volume =	 6,
  pages =	 {1783--1816},
  month =	 11,
  label1 =	 {C++ Software},
  link1 =	 softwarehttp # {gplvmcpp/},
  label2 =	 {MATLAB Software},
  link2 =	 softwarehttp # {gplvm/},
  label3 =	 {JMLR PDF},
  link3 =
                  {http://www.jmlr.org/papers/volume6/lawrence05a/lawrence05a.pdf},
  label4 =	 {JMLR Abstract},
  link4 =	 {http://www.jmlr.org/papers/v6/lawrence05a.html},
  errata1 =	 {Page 1791: eqn (10) there is a factor of a half
                  missing on both terms of the right hand side.},
  errataCredit1 ={Andreas Geiger},
  errata2 =	 {Page 1789: second line, the exponent of $(\lambda_j
                  - \beta^{-1})$ should be 1/2, not -1/2.},
  errataCredit2 ={Mathieu Saltzman},
  errata3 =	 {Page 1812: end of Appendix A, instead of `... for
                  any symmetric matrix $\mathbf{S}$ ...' line should
                  read `... for any positive definite symmetric matrix
                  $\mathbf{S}$.'},
  OPTerrataCredit3 ={},
  errata4 =	 {Page 1812: after eqn (25) last line of paragraph,
                  instead of `... (for kernel PCA) in where ...' line
                  should read `... (for kernel PCA) in which ...'.},
  OPTerrataCredit4 ={},
  group =	 {shefml,gplvm,ppca,pca,dimensional reduction},
  abstract =	 {Summarising a high dimensional data set with a low
                  dimensional embedding is a standard approach for
                  exploring its structure. In this paper w provide an
                  overview of some existing techniques for discovering
                  such embeddings. We then introduce a novel
                  probabilistic interpretation of principal component
                  analysis (PCA) that we term dual probabilistic PCA
                  (DPPCA). The DPPCA model has the additional
                  advantage that the linear mappings from the embedded
                  space can easily be non-linearised through Gaussian
                  processes. We refer to this model as a Gaussian
                  process latent variable model (GP-LVM). Through
                  analysis of the GP-LVM objective function, we relate
                  the model to popular spectral techniques such as
                  kernel PCA and multidimensional scaling. We then
                  review a practical algorithm for GP-LVMs in the
                  context of large data sets and develop it to also
                  handle discrete valued data and missing
                  attributes. We demonstrate the model on a range of
                  real-world and artificially generated data sets.}
}

@TechReport{Fusi:accurate11,
  author = 	 {Nicol'o Fusi and Oliver Stegle and Neil D. Lawrence},
  title = 	 {Accurate modeling of confounding variation in eQTL studies leads to a great increase in power to detect trans-regulatory effects},
  institution =  {Nature Precedings},
  year = 	 2011,
  OPTkey = 	 {http://precedings.nature.com/documents/5995/version/1},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi =		 {10101/npre.2011.5995.1},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {Expression quantitative trait loci (eQTL) studies are an integral tool to investigate the genetic component of gene expression variation. A major challenge in the analysis of such studies are hidden confounding factors, such as unobserved covariates or unknown environmental influences. These factors can induce a pronounced artifactual correlation structure in the expression profiles, which may create spurious false associations or mask real genetic association signals.\\\\

Here, we report PANAMA (Probabilistic ANAlysis of genoMic dAta), a novel probabilistic model to account for confounding factors within an
eQTL analysis. In contrast to previous methods, PANAMA learns hidden factors jointly with the effect of prominent genetic regulators. As a result, PANAMA can more accurately distinguish between true genetic association signals and confounding variation.\\\\

We applied our model and compared it to existing methods on a variety of datasets and biological systems. PANAMA consistently performs better than alternative methods, and finds in particular substantially more trans regulators. Importantly, PANAMA not only identified a greater number of associations, but also yields hits that are biologically more plausible and can be better reproduced between independent studies.},
  OPTgroup = 	 {}
}

@Misc{Lawrence:mocap05,
  OPTkey =	 {},
  author =	 {Neil D. Lawrence},
  title =	 {MOCAP Toolbox for {MATLAB}},
  howpublished = {Available on-line.},
  OPTmonth =	 7,
  OPTyear =	 {2005},
  linksoftware = softwarehttp # {mocap/}
}

@Article{Sanguinetti:accounting05,
  author =	 {Guido Sanguinetti and Marta Milo and Magnus Rattray
                  and Neil D. Lawrence},
  title =	 {Accounting for Probe-level Noise in Principal
                  Component Analysis of Microarray Data},
  journal =	 {Bionformatics},
  year =	 2005,
  volume =	 21,
  number =	 19,
  doi =		 {10.1093/bioinformatics/bti617},
  pmid =	 16091409,
  pages =	 {3748--3754},
  linkpdf =
                  {http://bioinformatics.oxfordjournals.org/cgi/reprint/21/19/3748},
  label1 =	 {Advance Access},
  link1 =
                  {http://bioinformatics.oxfordjournals.org/cgi/reprint/bti617?ijkey=2Elnob2I7AyTWIM&keytype=ref},
  label2 =	 {Pre-print PDF},
  link2 =	 shefftp # {nppca.pdf},
  linksoftware = softwarehttp # {nppca/},
  label3 =	 {Bioinformatics Abstract},
  link3 =
                  {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/21/19/3748},
  abstract =	 {{\bf Motivation:} Principal Component Analysis (PCA)
                  is one of the most popular dimensionality reduction
                  techniques for the analysis of high-dimensional
                  datasets. However, in its standard form, it does not
                  take into account any error measures associated with
                  the data points beyond a standard spherical
                  noise. This indiscriminate nature provides one of
                  its main weaknesses when applied to biological data
                  with inherently large variability, such as
                  expression levels measured with microarrays. Methods
                  now exist for extracting credibility intervals from
                  the probe-level analysis of cDNA and oligonucleotide
                  microarray experiments. These credibility intervals
                  are gene and experiment specific, and can be
                  propagated through an appropriate probabilistic
                  downstream analysis.\\\\ {\bf Results:} We propose a
                  new model-based approach to PCA that takes into
                  account the variances associated with each gene in
                  each experiment. We develop an efficient
                  EM-algorithm to estimate the parameters of our new
                  model. The model provides significantly better
                  results than standard PCA, while remaining
                  computationally reasonable. We show how the model
                  can be used to 'denoise' a microarray dataset
                  leading to improved expression profiles and tighter
                  clustering across profiles. The probabilistic nature
                  of the model means that the correct number of
                  principal components is automatically obtained.\\\\
                  {\bf Availability:} The software used in the paper
                  is available from
                  \url{http://www.bioinf.manchester.ac.uk/resources/puma}. The
                  microarray data are deposited in the NCBI database.},
  group =	 {shefml,puma,pca,ppca}
}

@InProceedings{Sanguinetti:automatic05,
  author =	 {Guido Sanguinetti and Jonathan Laidler and Neil D. Lawrence},
  title =	 {Automatic Determination of the Number of Clusters
                  Using Spectral Algorithms},
  booktitle =	 {Procedings of MLSP'05},
  year =	 2005,
  linksoftware = {http://www.dcs.shef.ac.uk/\%7Eguido/software.html},
  linkpdf =	 shefftp # {clusterNumber.pdf},
  group =	 {shefml},
  OPTnumber =	 {},
  address =	 {Mystic, Connecticut, U.S.A.},
  OPTmonth =	 {},
  OPTnote =	 {},
  OPTannote =	 {}
}

@InProceedings{Lawrence:semisuper04,
  author =	 {Neil D. Lawrence and Michael I. Jordan},
  title =	 {Semi-supervised Learning via {G}aussian Processes},
  abstract =	 {We present a probabilistic approach to learning a
                  Gaussian Process classifier in the presence of
                  unlabeled data. Our approach involves a "null
                  category noise model" (NCNM) inspired by ordered
                  categorical noise models. The noise model reflects
                  an assumption that the data density is lower between
                  the class-conditional densities. We illustrate our
                  approach on a toy problem and present comparative
                  results for the semi-supervised classification of
                  handwritten digits. },
  pages =	 {753--760},
  crossref =	 {Saul:nips04},
  linkpsgz =	 shefftp # {ncnm.ps.gz},
  linksoftware = softwarehttp # {ncnm/},
  group =	 {shefml}
}

@InProceedings{Bishop:mixtures97,
  author =	 {Christoper M. Bishop and Neil D. Lawrence and Tommi
                  S. Jaakkola and Michael I. Jordan},
  title =	 {Approximating Posterior Distributions in Belief
                  Networks using Mixtures},
  pages =	 {416--422},
  crossref =	 {Jordan:nips97},
  abstract =	 {Exact inference in densely connected Bayesian
                  networks is computationally intractable, and so
                  there is considerable interest in developing
                  effective approximation schemes. One approach which
                  has been adopted is to bound the log likelihood
                  using a mean-field approximating distribution. While
                  this leads to a tractable algorithm, the mean field
                  distribution is assumed to be factorial and hence
                  unimodal. In this paper we demonstrate the
                  feasibility of using a richer class of approximating
                  distributions based on \emph{mixtures} of mean field
                  distributions. We derive an efficient algorithm for
                  updating the mixture parameters and apply it to the
                  problem of learning in sigmoid belief networks. Our
                  results demonstrate a systematic improvement over
                  simple mean field theory as the number of mixture
                  components is increased.},
  linkpsgz =	 myftp # {mixtures.ps.gz}
}

@InProceedings{Lawrence:learning04,
  author =	 {Neil D. Lawrence and John C. Platt},
  title =	 {Learning to Learn with the Informative Vector
                  Machine},
  pages =	 {512--519},
  doi =		 {10.1145/1015330.1015382},
  crossref =	 {Greiner:icml04},
  abstract =	 {This paper describes an efficient method for
                  learning the parameters of a Gaussian process
                  (GP). The parameters are learned from multiple tasks
                  which are assumed to have been drawn independently
                  from the same GP prior. An efficient algorithm is
                  obtained by extending the informative vector machine
                  (IVM) algorithm to handle the multi-task learning
                  case. The multi-task IVM (MT-IVM) saves computation
                  by greedily selecting the most informative examples
                  from the separate tasks. The MT-IVM is also shown to
                  be more efficient than sub-sampling on an artificial
                  data-set and more effective than the traditional IVM
                  in a speaker dependent phoneme recognition task.},
  year =	 2004,
  linkpsgz =	 shefftp # {mtivm.ps.gz},
  linkpdf =	 shefftp # {mtivm.pdf},
  linksoftware = softwarehttp # {mtivm/},
  group =	 {shefml,gp,spgp}
}

@InProceedings{Sanguinetti:missingkpca06,
  author =	 {Guido Sanguinetti and Neil D. Lawrence},
  title =	 {Missing Data in Kernel {PCA}},
  crossref =	 {Scheffer:ecml06},
  OPTkey =	 {},
  OPTbooktitle = {},
  pages =	 {751--758},
  OPTyear =	 {},
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {}
}

@InProceedings{King:klcorrection06,
  author =	 {Nathaniel J. King and Neil D. Lawrence},
  title =	 {Fast Variational Inference for {G}aussian {P}rocess
                  Models through {KL}-Correction},
  crossref =	 {Scheffer:ecml06},
  OPTkey =	 {},
  OPTbooktitle = {},
  pages =	 {270--281},
  OPTyear =	 {},
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =	 shefftp # {ECMLppa.pdf},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  linksoftware = softwarehttp # {ppa/},
  errata1 =	 {Page 276: first equation on page, after 'The
                  KL-corrected bound (9) can be written using (3) as
                  ...'. The left hand side of this bound should be
                  $\mathcal{L}^{\prime}(\theta)$ instead of
                  $L(\theta)$.},
  errataCredit1 ={Raquel Urtasun},
  abstract =	 {Variational inference is a exible approach to
                  solving problems of intractability in Bayesian
                  models. Unfortunately the convergence of variational
                  methods is often slow. We review a recently
                  suggested variational approach for approximate
                  inference in Gaussian process (GP) models and show
                  how convergence may be dramatically improved through
                  the use of a positive correction term to the
                  standard variational bound. We refer to the modied
                  bound as a KL-corrected bound. The KL-corrected
                  bound is a lower bound on the true likelihood, but
                  an upper bound on the original variational
                  bound. Timing comparisons between optimisation of
                  the two bounds show that optimisation of the new
                  bound consistently improves the speed of
                  convergence.},
  group =	 {gp,variational,shefml}
}

@InProceedings{Lawrence:backconstraints06,
  author =	 {Neil D. Lawrence and Joaquin {Qui\~nonero Candela}},
  title =	 {Local Distance Preservation in the {GP-LVM} through
                  Back Constraints},
  pages =	 {513--520},
  crossref =	 {Cohen:icml06},
  doi =		 {10.1145/1143844.1143909},
  year =	 2006,
  linkpdf =	 shefftp # {backConstraints.pdf},
  linksoftware = softwarehttp # {fgplvm/},
  group =	 {gplvm,dimensional reduction},
  errata1 =	 {Equation (3) is missing integration over
                  $\mathbf{f}$.},
  errataCredit1 ={Laurens van der Maaten},
  errata2 =	 {Equation after Equation (5), the $v_{ij}$ should be
                  inside the sum sign.},
  abstract =	 {The Gaussian process latent variable model (GP-LVM)
                  is a generative approach to non-linear low
                  dimensional embedding, that provides a smooth
                  probabilistic mapping from latent to data space. It
                  is also a non-linear generalization of probabilistic
                  PCA (PPCA) \cite{Tipping:probpca99}. While most
                  approaches to non-linear dimensionality methods
                  focus on preserving local distances in data space,
                  the GP-LVM focusses on exactly the opposite. Being a
                  smooth mapping from latent to data space, it
                  focusses on keeping things apart in latent space
                  that are far apart in data space. In this paper we
                  first provide an overview of dimensionality
                  reduction techniques, placing the emphasis on the
                  kind of distance relation preserved. We then show
                  how the GP-LVM can be generalized, through back
                  constraints, to additionally preserve local
                  distances. We give illustrative experiments on
                  common data sets.}
}

@Article{Pena:fbd04,
  author =	 {Tonatiuh {Pe\~na-Centeno} and Neil D. Lawrence},
  title =	 {Optimising Kernel Parameters and Regularisation
                  Coefficients for Non-linear Discriminant Analysis},
  journal =	 jmlr,
  volume =	 7,
  OPTnumber =	 {},
  pages =	 {455--491},
  month =	 2,
  abstract =	 {In this paper we consider a novel Bayesian
                  interpretation of Fisher's discriminiant
                  analysis. We relate Rayleigh's coefficient to a
                  noise model that minimizes a cost based on the most
                  probable class centres and that abandons the
                  `regression to the labels' assumption used by other
                  algorithms. This yields a direction of
                  discrimination equivalent to Fisher's
                  discriminant. We use Bayes' rule to infer the
                  posterior distribution for the direction of
                  discrimination and in this process, priors and
                  constraining distributions are incorporated to reach
                  the desired result. Going further, with the use of a
                  Gaussian process prior we show the equivalence of
                  our model to a regularised kernel Fisher's
                  discriminant. A key advantage of our approach is the
                  facility to determine kernel parameters and the
                  regularisation coefficient through the optimisation
                  of the marginal log-likelihood of the data. An added
                  bonus of the new formulation is that it enables us
                  to link the regularisation coefficient with the
                  generalisation error.},
  linksoftware = softwarehttp # {bfd/},
  linkpdf =
                  {http://www.jmlr.org/papers/volume7/centeno06a/centeno06a.pdf},
  label1 =	 {JMLR Abstract},
  link1 =	 {http://www.jmlr.org/papers/v7/centeno06a.html},
  annote =	 {An earlier version is available as technical report
                  number CS-04-13, see \cite{Pena:fbd-tech04}.},
  year =	 2006,
  group =	 {shefml}
}

@TechReport{Pena:fbd-tech04,
  author =	 {Tonatiuh {Pe\~na-Centeno} and Neil D. Lawrence},
  title =	 {Optimising Kernel Parameters and Regularisation
                  Coefficients for Non-linear Discriminant Analysis},
  abstract =	 {In this paper we consider a Bayesian interpretation
                  of Fisher's discriminant. By relating Rayleigh's
                  coefficient to a likelihood function and through the
                  choice of a suitable prior we use Bayes' rule to
                  infer a posterior distribution over
                  projections. Through the use of a Gaussian process
                  prior we show the equivalence of our model to a
                  regularised kernel Fisher's discriminant. A key
                  advantage of our approach is the facility to
                  determine kernel parameters and the regularisation
                  coefficient through optimisation of the marginalised
                  likelihood of the data.},
  linkpsgz =	 shefftp # {bfdPaper.ps.gz},
  linksoftware = softwarehttp # {bfd/},
  institution =	 sheftech,
  number =	 {CS-04-13},
  year =	 2004,
  group =	 {shefml}
}

@InProceedings{AbdelHaleem:acoustic04,
  author =	 {Yasser H. {Abdel-Haleem} and Steve Renals and Neil D. Lawrence},
  title =	 {Acoustic Space Dimensionality Selection and
                  Combination using the Maximum Entropy Principle},
  crossref =	 {icassp04},
  linkpdf =	 shefftp # {icassp04.pdf},
  abstract =	 {In this paper we propose a discriminative approach
                  to acoustic space dimensionality selection based on
                  maximum entropy modelling. We form a set of
                  constraints by composing the acoustic space with the
                  space of phone classes, and use a continuous feature
                  formulation of maximum entropy modelling to select
                  an optimal feature set. The suggested approach has
                  two steps: (1) the selection of the best acoustic
                  space that efficiently and economically represents
                  the acoustic data and its variability; (2) the
                  combination of selected acoustic features in the
                  maximum entropy framework to estimate the posterior
                  probabilities over the phonetic labels given the
                  acoustic input. Specific contributions of this paper
                  include a parameter estimation algorithm
                  (generalized improved iterative scaling) that
                  enables the use of negative features, the
                  parameterization of constraint functions using
                  Gaussian mixture models, and experimental results
                  using the TIMIT database.}
}

@TechReport{Frey:Markovian98,
  author =	 {Brendan J. Frey and Neil D. Lawrence and Christopher
                  M. Bishop},
  title =	 {Markovian inference in belief networks},
  institution =	 {The Beckman Institute},
  year =	 1998,
  address =	 {University of Illinois at Urbana-Champaign, 405
                  North Mathews Avenue, Urbana, IL 61801, USA},
  note =	 {Originally submitted to \emph{NIPS 1998}},
  abstract =	 {Bayesian belief networks can represent the
                  complicated probabilistic processes that form
                  natural sensory inputs. Once the parameters of the
                  network have been learned,nonlinear inferences about
                  the input can be made by computing the posterior
                  distribution over the hidden units (e.g., depth in
                  stereo vision) given the input. Computing the
                  posterior distribution exactly is not practical in
                  richly-connected networks, but it turns out that by
                  using a variational (a.k.a., mean field) method, it
                  is easy to find a product-form distribution that
                  approximates the true posterior distribution. This
                  approximation assumes that the hidden variables are
                  independent given the current input. In this paper,
                  we explore a more powerful variational technique
                  that models the posterior distribution using a
                  Markov chain. We compare this method with inference
                  using mean fields and mixtures of mean fields in
                  randomly generated networks.},
  linkpsgz =	 myftp # {mi.ps.gz}
}

@TechReport{Lawrence:GCA01,
  author =	 {Neil D. Lawrence and Michael E. Tipping},
  title =	 {Generalised Component Analysis},
  note =	 {},
  year =	 {2003},
  abstract =	 {Principal component analysis is a well known
                  approach for determining the principal sub-space of
                  a data-set. Independent component analysis is a
                  widely utilised technique for recovering the
                  linearly embedded independent components of a
                  data-set. In this paper we develop an algorithm
                  that, for super-Gaussian sources, extracts the
                  direction and number of independent components of a
                  data-set and determines the principal sub-space of
                  the remaining components. This is achieved through
                  the use of a latent variable model. We refer to the
                  approach as Generalised Component Analysis and
                  demonstrate its ability to both extract indpendent
                  and principal components, as well as to determine
                  the number of independent components, on toy and
                  real word data-sets.},
  institution =	 sheftech,
  number =	 {CS-03-10},
  linkpsgz =	 shefftp # {gca.ps.gz},
  linksoftware = softwarehttp # {gca/},
  group =	 {shefml}
}

@TechReport{Lawrence:ICA99,
  author =	 {Neil D. Lawrence and Christopher M. Bishop},
  title =	 {Variational {B}ayesian Independent Component
                  Analysis},
  year =	 2000,
  abstract =	 {Blind separation of signals through the info-max
                  algorithm may be viewed as maximum likelihood
                  learning in a latent variable model. In this paper
                  we present an alternative approach to maximum
                  likelihood learning in these models, namely Bayesian
                  inference. It has already been shown how Bayesian
                  inference can be applied to determine latent
                  dimensionality in principal component analysis
                  models \cite{Bishop:bayesPCA98}. In this paper we
                  derive a similar approach for removing unecessary
                  source dimensions in an independent component
                  analysis model. We present results on a toy data-set
                  and on some artificially mixed images.},
  linkpsgz =	 myftp # {bica_report.ps.gz}
}

@TechReport{Lawrence:largescale06,
  author =	 {Neil D. Lawrence},
  title =	 {Large Scale Learning with the {G}aussian Process
                  Latent Variable Model},
  institution =	 {University of Sheffield},
  year =	 {2006},
  OPTkey =	 {},
  OPTtype =	 {},
  number =	 {CS-06-05},
  linkpdf =	 shefftp # {gplvmSparse.pdf},
  linksoftware = softwarehttp # {fgplvm/},
  month =	 {},
  errata1 =	 {Page 14: Sign and inverse wrong on definition of
                  $\mathbf{C}$ just before (4). Corrected in version
                  from December 16, 2008.},
  errataCredit1 ={John Guiver},
  abstract =	 {In this paper we apply the latest techniques in
                  sparse Gaussian process regression (GPR) to the
                  Gaussian process latent variable model (GP-LVM). We
                  review three techniques and discuss how they may be
                  implemented in the context of the GP-LVM. We briefly
                  consider a GPR toy problem to highlight the
                  strenghts and weaknesses of the different approaches
                  before studying the perfomance of these techniques
                  on a benchmark visualisation data set.},
  note =	 {Document updated on December 16, 2008. Original from
                  February 17th, 2006.},
  group =	 {gplvm,motion,shefml,dimensional reduction}
}

@InProceedings{Lawrence:gplvm03,
  author =	 {Neil D. Lawrence},
  title =	 {Gaussian Process Models for Visualisation of High
                  Dimensional Data},
  pages =	 {329--336},
  crossref =	 {Thrun:nips03},
  abstract =	 {In this paper we introduce a new underlying
                  probabilistic model for principal component analysis
                  (PCA). Our formulation interprets PCA as a
                  particular Gaussian process prior on a mapping from
                  a latent space to the observed data-space. We show
                  that if the prior's covariance function constrains
                  the mappings to be linear the model is equivalent to
                  PCA, we then extend the model by considering less
                  restrictive covariance functions which allow
                  non-linear mappings. This more general Gaussian
                  process latent variable model (GPLVM) is then
                  evaluated as an approach to the visualisation of
                  high dimensional data for three different
                  data-sets. Additionally our non-linear algorithm can
                  be \emph{further} kernelised leading to `twin kernel
                  PCA' in which a \emph{mapping} \emph{between feature
                  spaces} occurs.},
  linkpsgz =	 shefftp # {gplvm.ps.gz},
  linksoftware = softwarehttp # {gplvm/},
  group =	 {shefml,gplvm,dimensional reduction}
}

@InCollection{Lawrence:extensions05,
  author =	 {Neil D. Lawrence and John C. Platt and Michael
                  I. Jordan},
  title =	 {Extensions of the Informative Vector Machine},
  crossref =	 {Winkler:smlw04},
  pages =	 {56--87},
  abstract =	 {The informative vector machine (IVM) is a practical
                  method for Gaussian process regression and
                  classification. The IVM produces a sparse
                  approximation to a Gaussian process by combining
                  assumed density filtering with a heuristic for
                  choosing points based on minimizing posterior
                  entropy. This paper extends IVM in several
                  ways. First, we propose a novel noise model that
                  allows the IVM to be applied to a mixture of labeled
                  and unlabeled data. Second, we use IVM on a
                  block-diagonal covariance matrix, for ``learning to
                  learn'' from related tasks. Third, we modify the IVM
                  to incorporate prior knowledge from known
                  invariances. All of these extensions are tested on
                  artificial and real data.},
  linkpsgz =	 shefftp # {ivmdev.ps.gz},
  linksoftware = softwarehttp # {ivm/},
  group =	 {shefml,gp,spgp}
}

@Unpublished{Milo:proboligo04,
  author =	 {Marta Milo and Mahesan Niranjan and Matthew
                  C. Holley and Magnus Rattray and Neil D. Lawrence},
  title =	 {A Probabilistic Approach for Summarising
                  Oligonucleotide Gene Expression Data},
  year =	 2005
}

@Article{Liu:tractable04,
  author =	 {Xuejun Liu and Marta Milo and Neil D. Lawrence and
                  Magnus Rattray},
  title =	 {A Tractable Probabilistic Model for {A}ffymetrix
                  Probe-level Analysis across Multiple Chips},
  journal =	 bioinf,
  year =	 2005,
  volume =	 21,
  number =	 18,
  pages =	 {3637--3644},
  doi =		 {10.1093/bioinformatics/bti583},
  pmid =	 16020470,
  linkpdf =
                  {http://bioinformatics.oxfordjournals.org/cgi/reprint/21/18/3637},
  label1 =	 {Advance Access},
  link1 =
                  {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/bti583?ijkey=NwpQ2i5ZVAlgBwj&keytype=ref},
  linksoftware =
                  {http://www.bioconductor.org/packages/2.0/bioc/html/puma.html},
  label2 =	 {Pre-print PDF},
  link2 =	 shefftp # {multigmos.pdf},
  group =	 {shefml,puma},
  abstract =	 {{\bf Motivation:} Affymetrix GeneChip arrays are
                  currently the most widely used microarray
                  technology. Many summarisation methods have been
                  developed to provide gene expression levels from
                  Affymetrix probe-level data. Most of the currently
                  popular methods do not provide a measure of
                  uncertainty for the expression level of each
                  gene. The use of probabilistic models can overcome
                  this limitation. A full hierarchical Bayesian
                  approach requires the use of computationally
                  intensive MCMC methods that are impractical for
                  large data sets. An alternative computationally
                  efficient probabilistic model, mgMOS, uses Gamma
                  distributions to model specific and non-specific
                  binding with a latent variable to capture variations
                  in probe affinity. Although promising, the main
                  limitations of this model are that it does not use
                  information from multiple chips and that it does not
                  account for specific binding to the mismatch (MM)
                  probes.\\\\ {\bf Results:} We extend mgMOS to model
                  the binding affinity of probe-pairs across multiple
                  chips and to capture the effect of specific binding
                  to MM probes. The new model, multi-mgMOS, provides
                  improved accuracy, as demonstrated on some
                  bench-mark data sets and a real time-course data
                  set, and is much more computationally efficient than
                  a competing hierarchical Bayesian approach that
                  requires MCMC sampling. We demonstrate how the
                  probabilistic model can be used to estimate
                  credibility intervals for expression levels and
                  their log-ratios between conditions.\\\\ {\bf
                  Availability:} Both mgMOS and the new model
                  multi-mgMOS have been implemented in an R package
                  that is currently available from
                  \url{http://umber.sbs.man.ac.uk/resources/puma}.}
}

@Misc{Lawrence:informative01,
  author =	 {Neil D. Lawrence and Ralf Herbrich},
  title =	 {A Sparse {B}ayesian Compression Scheme --- The
                  Informative Vector Machine},
  howpublished = {Presentation during the Kernel Workshop at NIPS
                  2001},
  year =	 2001,
  abstract =	 {Kernel based learning algorithms allow the mapping
                  of data-set into an infinite dimensional feature
                  space in which a classification may be performed. As
                  such kernel methods represent a powerful approach to
                  the solution of many non-linear problems. However
                  kernel methods do suffer from one unfortunate
                  drawback, the Gram matrix contains m rows and
                  columns where m is the number of data-points. Many
                  operations are therefore precluded (e.g.~matrix
                  inverse $O(m^3)$) when data-sets containing more
                  than about 10^4 points are encountered. One approach
                  to resolving these issues is to look for sparse
                  representations of the data-set A sparse
                  representation contains a reduced number of
                  examples. Loosely speaking we are interested in
                  extracting the maximum amount of information from
                  the minimum number of data-points. To achieve this
                  in a principled manner we are interested in
                  estimating the amount of information each data-point
                  contains. In the framework presented here we make
                  use of the Bayesian methodology to determine how
                  much information is gained from each data-point.},
  linkpsgz =	 myftp # {kips01.ps.gz},
  group =	 {shefml}
}

@TechReport{Lawrence:ivmTech04,
  author =	 {Neil D. Lawrence and Mattias Seeger and Ralf
                  Herbrich},
  title =	 {The Informative Vector Machine: A Practical
                  Probabilistic Alternative to the Support Vector
                  Machine},
  institution =	 {Department of Computer Science, University of
                  Sheffield},
  year =	 2004,
  linkpsgz =	 shefftp # {ivmTechreport.ps.gz},
  label1 =	 {Matlab Software},
  link1 =	 softwarehttp # {ivm/},
  label2 =	 {C++ Software},
  link2 =	 softwarehttp # {ivmcpp/},
  note =	 {Last updated December 2005},
  OPTtype =	 {},
  number =	 {CS-04-07},
  OPTaddress =	 {},
  abstract =	 {We present a practical probabilistic alternative to
                  the popular support vector machine (SVM). The
                  algorithm is an approximation to a Gaussian process,
                  and is probabilistic in the sense that it maintains
                  the process variance that is implied by the use of a
                  kernel function, which the SVM discards. We show
                  that these variances may be tracked and made use of
                  selection of an active set which gives a sparse
                  representation for the model. For an active set size
                  of $d$ our algorithm exhibits $O(d^{2}N)$
                  computational complexity and $O(dN)$ storage
                  requirements. It has already been shown that the
                  approach is comptetive with the SVM in terms of
                  performance and running time, here we give more
                  details of the approach and demonstrate that kernel
                  parameters may also be learned in a practical and
                  effective manner.},
  group =	 {shefml}
}

@TechReport{Lawrence:gplvmTech04,
  author =	 {Neil D. Lawrence},
  title =	 {Probabilistic Non-linear Principal Component
                  Analysis with {G}aussian Process Latent Variable
                  Models},
  institution =	 {Department of Computer Science, University of
                  Sheffield},
  year =	 2004,
  number =	 {CS-04-08},
  abstract =	 {Summarising a high dimensional data-set with a low
                  dimensional embedding is a standard approach for
                  exploring its structure. In this paper we provide an
                  overview of some existing techniques for discovering
                  such embeddings. We then introduce a novel
                  probabilistic interpretation of principal component
                  analysis (PCA) that we term dual probabilistic PCA
                  (DPPCA). The DPPCA model has the additional
                  advantage that the linear mappings from the embedded
                  space can easily be non-linearised through Gaussian
                  processes. We refer to this model as a Gaussian
                  process latent variable model (GPLVM). We develop a
                  practical algorithm for GPLVMs which allow for
                  non-linear mappings from the embedded space giving a
                  non-linear probabilistic version of PCA. We develop
                  the new algorithm to provide a principled approach
                  to handling discrete valued data and missing
                  attributes. We demonstrate the algorithm on a range
                  of real-world and artificially generated data-sets
                  and finally, through analysis of the GPLVM objective
                  function, we relate the algorithm to popular
                  spectral techniques such as kernel PCA and
                  multidimensional scaling.},
  errata1 =	 {Page 9: just before Section 5. The inequality should
                  be $N>>d$ not $N<<d$.},
  errataCredit1 ={Shaobo Hou},
  linkpsgz =	 shefftp # {nlpca.ps.gz},
  linksoftware = softwarehttp # {gplvm/},
  group =	 {shefml,dimensional reduction}
}

@InProceedings{Lawrence:ivm02,
  author =	 {Neil D. Lawrence and Matthias Seeger and Ralf
                  Herbrich},
  title =	 {Fast Sparse {G}aussian Process Methods: The
                  Informative Vector Machine},
  crossref =	 {Becker:nips02},
  pages =	 {625--632},
  abstract =	 {We present a framework for sparse Gaussian process
                  (GP) methods which uses forward selection with
                  criteria based on information-theoretical
                  principles, previously suggested for active
                  learning. In contrast to most previous work on
                  sparse GPs, our goal is not only to learn sparse
                  predictors (which can be evaluated in $O(d)$ rather
                  than $O(n)$, $d<<n$, $n$ the number of training
                  points), but also to perform training under strong
                  restrictions on time and memory requirements. The
                  scaling of our method is at most $O(nd^2)$, and in
                  large real-world classification experiments we show
                  that it can match prediction performance of the
                  popular support vector machine (SVM), yet it
                  requires only a fraction of the training time. In
                  contrast to the SVM, our approximation produces
                  estimates of predictive probabilities (`error
                  bars'), allows for Bayesian model selection and is
                  less complex in implementation.},
  linkpsgz =	 shefftp # {ivm.ps.gz},
  linksoftware = softwarehttp # {ivm},
  group =	 {shefml,gp,spgp}
}

@InProceedings{Lawrence:larger07,
  author =	 {Neil D. Lawrence},
  title =	 {Learning for Larger Datasets with the {G}aussian
                  Process Latent Variable Model},
  crossref =	 {Meila:aistats07},
  OPTkey =	 {},
  OPTbooktitle = {},
  pages =	 {243--250},
  OPTyear =	 {},
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  linkpdf =	 shefftp # {gplvmLarger.pdf},
  linksoftware = softwarehttp # {fgplvm/},
  abstract =	 {In this paper we apply the latest techniques in
                  sparse Gaussian process regression (GPR) to the
                  Gaussian process latent variable model (GP-LVM). We
                  review three techniques and discuss how they may be
                  implemented in the context of the GP-LVM. Each
                  approach is then implemented on a well known
                  benchmark data set and compared with earlier
                  attempts to sparsify the model.},
  group =	 {shefml,gp,spgp,gplvm,dimensional reduction}
}

@InProceedings{Lawrence:ltu01,
  author =	 {Neil D. Lawrence},
  title =	 {Variational Learning for Multi-layer networks of
                  Linear Threshold Units},
  crossref =	 {Jaakkola:aistats01},
  pages =	 {245--252},
  abstract =	 {Linear threshold units were originally proposed as
                  models of biological neurons. They were widely
                  studied in the context of the perceptron
                  \cite{Rosenblatt:book62}. Due to the difficulties of
                  finding a general algorithm for networks with hidden
                  nodes, they never passed into general use. We derive
                  an algorithm in the context of graphical models and
                  show how it may be applied in multi-layer networks
                  of linear threshold units. We demonstrate the
                  algorithm through three well known datasets.},
  linkpsgz =	 myftp # {ltus.ps.gz},
  linkpdf =	 myftp # {ltus.pdf}
}

@TechReport{Lawrence:ltu_report00,
  author =	 {Neil D. Lawrence},
  title =	 {Variational Learning for Multi-layer networks of
                  Linear Threshold Units},
  year =	 2000,
  note =	 {Draft report, slightly extended version of
                  \cite{Lawrence:ltu01}.},
  abstract =	 {Linear threshold units were originally proposed as
                  models of biological neurons. They were widely
                  studied in the context of the perceptron
                  \cite{Rosenblatt:book62}. Due to the difficulties of
                  finding a general algorithm for networks with hidden
                  nodes, they never passed into general use. We derive
                  an algorithm in the context of graphical models and
                  show how it may be applied in multi-layer networks
                  of linear threshold units. We demonstrate the
                  algorithm through three well known datasets.},
  linkpsgz =	 myftp # {ltupaper.ps.gz}
}

@InProceedings{Lawrence:microarray03,
  author =	 {Neil D. Lawrence and Marta Milo and Mahesan Niranjan
                  and Penny Rashbass and Stephan Soullier},
  title =	 {{B}ayesian Processing of Microarray Images},
  pages =	 {71--80},
  crossref =	 {Molina:nnsp03},
  abstract =	 {Gene expression measurements quantify the level of
                  mRNA produced from each gene. Two principal methods
                  exist for producing slides for extracting these
                  levels: photolithography and spotted arrays. One
                  difficulty with the spotted array format is
                  determining the size and location of the spots on
                  the array. In this paper we present a Bayesian
                  approach to processing images produced by these
                  arrays that seeks posterior distributions over the
                  size and positions of the spots. This enables us to
                  estimate expression ratios and their
                  variances. Exact inference for the model we specify
                  is intractable; we develop an approximate inference
                  technique which combines importance sampling with
                  variational inference. Our technique has already
                  been shown to be more consistent than both manual
                  processing and another automated technique
                  \cite{Lawrence:variability03}. Here we present
                  large-scale results for twenty-four microarray
                  slides each representing 5760 genes and show the
                  dramatic effects of incorporating variance in our
                  downsteam analysis. Software based on this algorithm
                  is available for academic use.},
  linkpsgz =	 shefftp # {visMicroarray.ps.gz},
  linkpdf =	 shefftp # {visMicroarray.pdf},
  linksoftware = softwarehttp # {vis/},
  errata1 =	 {Page 7: on the bottom half of the page in Algorithm
                  1. Line number 6 (the first line in the repeat
                  loop). This statement should be before the repeat
                  loop starts.},
  errataCredit1 ={Nilanjan Dasgupta},
  group =	 {shefml,mig}
}

@InProceedings{Lawrence:mixtures98,
  author =	 {Neil D. Lawrence and Christoper M. Bishop and
                  Michael I. Jordan},
  title =	 {Mixture Representations for Inference and Learning
                  in {B}oltzmann Machines},
  pages =	 {320--327},
  crossref =	 {Cooper:uai98},
  year =	 1998,
  abstract =	 {Boltzmann machines are undirected graphical models
                  with two-state stochastic variables, in which the
                  logarithms of the clique potentials are quadratic
                  functions of the node states. They have been widely
                  studied in the neural computing literature, although
                  their practical applicability has been limited by
                  the difficulty of finding an effective learning
                  algorithm. One well-established approach, known as
                  mean field theory, represents the stochastic
                  distribution using a factorized
                  approximation. However, the corresponding learning
                  algorithm often fails to find a good solution. We
                  conjecture that this is due to the implicit
                  uni-modality of the mean field approximation which
                  is therefore unable to capture multi-modality in the
                  true distribution. In this paper we use variational
                  methods to approximate the stochastic distribution
                  using multi-modal \emph{mixtures} of factorized
                  distributions. We present results for both inference
                  and learning to demonstrate the effectiveness of
                  this approach.},
  linkpsgz =	 myftp # {boltzmann.ps.gz}
}

@TechReport{Lawrence:nnmixtures99,
  title =	 {A Variational {B}ayesian Committee of Neural
                  Networks},
  author =	 {Neil D. Lawrence and Mehdi Azzouzi},
  year =	 1999,
  abstract =	 {Exact inference in Bayesian neural networks is non
                  analytic to compute and as a result approximate
                  approaches such as the evidence procedure,
                  Monte-Carlo sampling and variational inference have
                  been proposed. In this paper we present a general
                  overview of the Bayesian approach with a particular
                  emphasis on the variational procedure. We then
                  present a new approximating distribution based on
                  \emph{mixtures} of Gaussian distributions and show
                  how it may be implemented. We present results on a
                  simple toy problem and on two real world data-sets.},
  linkpsgz =	 myftp # {nnmixture.ps.gz}
}

@InProceedings{Lawrence:noisy01,
  author =	 {Neil D. Lawrence and Bernhard Sch\"olkopf},
  title =	 {Estimating a Kernel {F}isher Discriminant in the
                  Presence of Label Noise},
  crossref =	 {Brodley:icml01},
  OPTpages =	 {},
  abstract =	 {Data noise is present in many machine learning
                  problems domains, some of these are well studied but
                  others have received less attention. In this paper
                  we propose an algorithm for constructing a kernel
                  Fisher discriminant (KFD) from training examples
                  with \emph{noisy labels}. The approach allows to
                  associate with each example a probability of the
                  label being flipped. We utilise an expectation
                  maximization (EM) algorithm for updating the
                  probabilities. The E-step uses class conditional
                  probabilities estimated as a by-product of the KFD
                  algorithm. The M-step updates the flip probabilities
                  and determines the parameters of the
                  discriminant. We have applied the approach to two
                  real-world data-sets. The results show the
                  feasibility of the approach.},
  linksoftware = softwarehttp # {nkfd/},
  linkpsgz =	 shefftp # {noisyfisher.ps.gz},
  group =	 {shefml}
}

@InProceedings{Lawrence:nrd01,
  author =	 {Neil D. Lawrence},
  title =	 {Node Relevance Determination},
  crossref =	 {Marinaro:wirn01},
  OPTpages =	 {},
  abstract =	 {Hierarchical Bayesian inference in parameterised
                  models offers an approach for controlling
                  complexity. In this paper we utilise a novel prior
                  for the leaning of a model's structure. We call the
                  prior \emph{node relevance determination}. It is
                  applicable in a range of models including sigmoid
                  belief networks and Boltzmann machines. We
                  demonstrate how the approach may be applied to
                  determine structure in a multi-layer perceptron.},
  linkpdf =	 myftp # {structure.pdf},
  linkpsgz =	 myftp # {structure.ps.gz}
}

@Patent{Lawrence:patent01,
  author =	 {Neil D. Lawrence and Anthony I. T. Rowstron and 
                 Christopher M. Bishop and Michael J. Taylor},
  title =	 {System and Method for Replicating Data in a Distributed System},
  patentnumber = {6889333},
  country =      {U.S.A.},
  link1 =        {http://www.google.com/patents?vid=USPAT6889333},
  label1 =       {Google Patents},
  abstract =     {It is common in distributed systems to replicate data. 
                 In many cases, this data evolves in a consistent
                 fashion, and this evolution can be modeled. A
                 probabilistic model of the evolution allows us to
                 estimate the divergence of the replicas and can be
                 used by the application to alter its behavior, for
                 example, to control synchronization times, to
                 determine the propagation of writes, and to convey to
                 the user information about how much the data may have
                 evolved. In this paper, we describe how the evolution
                 of the data may be modeled and outline how the
                 probabilistic model may be utilized in various
                 applications, concentrating on a news database
                 example.},
  note =         {Filing date: Nov 1st 2001, issue date May 3rd, 2005},
  year =	 2005
}
@TechReport{Lawrence:structure01,
  author =	 {Neil D. Lawrence and Mehdi Azzouzi},
  title =	 {The Structure of Neural Network Posteriors},
  note =	 {},
  OPTkey =	 {},
  OPTmonth =	 {},
  year =	 2001,
  OPTannote =	 {},
  abstract =	 {Exact inference in Bayesian neural networks is non
                  analytic to compute and as a result approximate
                  approaches such as the evidence procedure,
                  Monte-Carlo sampling and variational inference have
                  been proposed. In this paper we explore the
                  structure of the posterior distributions in such a
                  model through a new approximating distribution based
                  on \emph{mixtures} of Gaussian distributions and
                  show how it may be implemented.},
  linkpdf =	 myftp # {mixture.pdf},
  linkpsgz =	 myftp # {mixture.ps.gz}
}

@InProceedings{Lawrence:sync01,
  author =	 {Neil D. Lawrence and Anthony I. T. Rowstron and
                  Christopher M. Bishop and Michael J. Taylor},
  title =	 {Optimising Synchronisation Times for Mobile Devices},
  pages =	 {1401--1408},
  crossref =	 {Dietterich:nips01},
  abstract =	 {With the increasing number of users of mobile
                  computing devices (e.g. personal digital assistants)
                  and the advent of third generation mobile phones,
                  wireless communications are becoming increasingly
                  important. Many applications rely on the device
                  maintaining a \emph{replica} of a data-structure
                  which is stored on a server, for example news
                  databases, calendars and e-mail. In this paper we
                  explore the question of the optimal strategy for
                  synchronising such replicas. We utilise
                  probabilistic models to represent how the
                  data-structures evolve and to model user
                  behaviour. We then formulate objective functions
                  which can be minimised with respect to the
                  synchronisation timings. We demonstrate, using two
                  real world data-sets, that a user can obtain more
                  up-to-date information using our approach.},
  linkpsgz =	 myftp # {jitcache.ps.gz},
  group =	 {shefml}
}

@PhdThesis{Lawrence:thesis00,
  author =	 {Neil D. Lawrence},
  title =	 {Variational Inference in Probabilistic Models},
  school =	 {Computer Laboratory, University of Cambridge},
  address =	 {New Museums Site, Pembroke Street, Cambridge, CB2
                  3QG, U.K.},
  year =	 2000,
  note =	 {Available from
                  \url{http://www.thelawrences.net/neil}},
  abstract =	 {One of the key objectives of modern artificial
                  intelligence is the handling of
                  uncertainty. Probability theory provides a framework
                  for handling of uncertainty in a principled
                  manner. Probabilistic inference is the process of
                  reasoning under uncertainty and as such is vital for
                  both learning and decision making in probabilistic
                  models. For many models of interest this inference
                  proves to be intractable and to make progress
                  approximate methods need to be considered. This
                  thesis concerns itself with a particular
                  approximating formalism known as variational
                  inference \cite{Jordan:variational98}. Variational
                  inference has the advantage that it often provides
                  bounds on quantities of interest, such as
                  marginalised likelihoods. In this thesis we describe
                  a general framework for the implementation of the
                  variational approach. We then explore the approach
                  in several different probabilistic models including
                  undirected and directed graphs, Bayesian neural
                  networks and independent component analysis. In
                  those models which have been handled using
                  variational inference previously we show how we may
                  improve the quality of our inference engine by
                  considering more complex variational
                  distributions. },
  linkpsgz =	 myftp # {thesis.ps.gz},
  linkpdf =	 myftp # {thesis.pdf}
}

@Article{Lawrence:variability03,
  author =	 {Neil D. Lawrence and Marta Milo and Mahesan Niranjan
                  and Penny Rashbass and Stephan Soullier},
  title =	 {Reducing the Variability in {cDNA} Microarray Image
                  Processing by {B}ayesian Inference},
  journal =	 bioinf,
  year =	 {2004},
  volume =	 20,
  number =	 4,
  doi =		 {10.1093/bioinformatics/btg438},
  pages =	 {518--526},
  pmid =	 14990447,
  abstract =	 {{\bf Motivation:} Gene expression levels are
                  obtained from microarray experiments through the
                  extraction of pixel intensities from a scanned image
                  of the slide. It is widely acknowledged that
                  variabilities can occur in expression levels
                  extracted from the same images by different users
                  with the same software packages. These
                  inconsistencies arise due to differences in the
                  refinement of the placement of the microarray
                  `grids'. We introduce a novel automated approach to
                  the refinement of grid placements that is based upon
                  the use of Bayesian inference for determining the
                  size, shape and positioning of the microarray
                  `spots', capturing uncertainty that can be passed to
                  downstream analysis.\\\\ {\bf Results:} Our
                  experiments demonstrate that variability between
                  users can be significantly reduced using the
                  approach. The automated nature of the approach also
                  saves hours of researchers' time normally spent in
                  refining the grid placement.\\\\ {\bf Availability:}
                  A MATLAB implementation of the algorithm and an
                  image of the slide used in our experiments, as well
                  as the code necessary to recreate them are available
                  for non-commercial use from
                  \url{http://www.dcs.shef.ac.uk/\~{ }neil/vis}.},
  linkpsgz =	 shefftp # {microarrayImage.ps.gz},
  link1 =	 shefftp # {microarrayImage.pdf},
  label1 =	 {Pre-print PDF},
  linksoftware = softwarehttp # {vis/},
  group =	 {shefml,mig,puma}
}

@Article{Lerner:comparison01,
  author =	 {Boaz Lerner and Neil D. Lawrence},
  title =	 {A Comparison of State-of-the-Art Classification
                  Techniques with Application to Cytogenetics},
  journal =	 {Neural Computing and Applications},
  year =	 2001,
  volume =	 10,
  number =	 1,
  pages =	 {39--47},
  abstract =	 {Several state-of-the-art techniques: a neural
                  network, Bayesian neural network, support vector
                  machine and naive Bayesian classifier are
                  experimentally evaluated in discriminating
                  fluorescence in-situ hybridization (FISH)
                  signals. Highly accurate classification of signals
                  from real data and artefacts of two cytogenetic
                  probes (colours) is required for detecting
                  abnormalities in the data. More than 3,100 FISH
                  signals are classified by the techniques into colour
                  and as real or artefact with accuracies of around
                  98\% and 88\%, respectively. The results of the
                  comparison also show a trade-off between simplicity
                  represented by the naive Bayesian classifier and
                  high classification performance represented by the
                  other techniques. },
  linkpsgz =	 myftp # {comparison.ps.gz}
}

@InProceedings{Rowstron:sync01,
  author =	 {Anthony I. T. Rowstron and Neil D. Lawrence and
                  Christopher M. Bishop},
  title =	 {Probabilistic Modelling of Replica Divergence},
  booktitle =	 {Proceedings of the 8th Workshop on Hot Topics in
                  Operating Systems HOTOS (VIII)},
  OPTpages =	 {},
  year =	 2001,
  OPTeditor =	 {},
  OPTaddress =	 {},
  OPTpublisher = {},
  abstract =	 {It is common in distributed systems to replicate
                  data. In many cases this data evolves in a
                  consistent fashion and this evolution can be
                  modelled. A \emph{probabilistic model} of the
                  evolution allows us to estimate the divergence of
                  the replicas and can be used by the application to
                  alter its behaviour, for example to control
                  synchronisation times, to determine the propagation
                  of writes, and to convey to the user information
                  about how much the data may have evolved. In this
                  paper, we describe how the evolution of the data may
                  be modelled and outline how the probabilistic model
                  may be utilised in various applications,
                  concentrating on a news database example.},
  linkpdf =	 myftp # {hotos_sync.pdf},
  linkpsgz =	 myftp # {hotos_sync.ps.gz}
}

@InProceedings{Seeger:fast03,
  author =	 {Matthias Seeger and Christopher K. I. Williams and
                  Neil D. Lawrence},
  title =	 {Fast Forward Selection to Speed Up Sparse {G}aussian
                  Process Regression},
  crossref =	 {Bishop:aistats03},
  abstract =	 {We present a method for the sparse greedy
                  approximation of Bayesian Gaussian process
                  regression, featuring a novel heuristic for very
                  fast forward selection. Our method is essentially as
                  fast as an equivalent one which selects the
                  ``support'' patterns at random, yet it can
                  outperform random selection on hard curve fitting
                  tasks. More importantly, it leads to a sufficiently
                  stable approximation of the log marginal likelihood
                  of the training data, which can be optimised to
                  adjust a large number of hyperparameters
                  automatically. We demonstrate the model selection
                  capabilities of the algorithm in a range of
                  experiments. In line with the development of our
                  method, we present a simple view on sparse
                  approximations for GP models and their underlying
                  assumptions and show relations to other methods.},
  linkpsgz =	 myftp # {fastForward.ps.gz},
  group =	 {shefml,spgp}
}

@InProceedings{Tipping:variational03,
  author =	 {Michael E. Tipping and Neil D. Lawrence},
  title =	 {A Variational Approach to Robust {B}ayesian
                  Interpolation},
  pages =	 {229--238},
  crossref =	 {Molina:nnsp03},
  abstract =	 {This paper details a robust Bayesian interpolation
                  procedure for linear-in-the-parameter
                  models. Robustness is achieved via a Student-$t$
                  noise model, defined hierarchically in terms of an
                  inverse-Gamma prior distribution over individual
                  Gaussian observation variances. Variational
                  techniques are exploited to update this prior in
                  light of the data, while also inferring all other
                  model variables. The key to this approach is
                  flexibility; it can infer Gaussian noise where
                  appropriate but can adapt to accommodate
                  heavier-tailed distributions in the presence of
                  outliers.},
  linkpdf =	 shefftp # {robustBayesian.pdf},
  group =	 {shefml}
}

@InProceedings{Vermaak:variational03,
  author =	 {Jaco Vermaak and Neil D. Lawrence and Patrick
                  P\'erez},
  title =	 {Variational Inference for Visual Tracking},
  booktitle =	 pCVPR,
  publisher =	 ieeecomp,
  year =	 2003,
  pages =	 {773--780},
  volume =	 {I},
  abstract =	 {The likelihood models used in probabilistic visual
                  tracking applications are often complex non-linear
                  and/or non-Gaussian functions, leading to
                  analytically intractable inference. Solutions then
                  require numerical approximation techniques, of which
                  the particle filter is a popular choice. Particle
                  filters, however, degrade in performance as the
                  dimensionality of the state space increases and the
                  support of the likelihood decreases. As an
                  alternative to particle filters this paper
                  introduces a variational approximation to the
                  tracking recursion. The variational inference is
                  intractable in itself, and is combined with an
                  efficient importance sampling procedure to obtain
                  the required estimates. The algorithm is shown to
                  compare favourably with particle filtering
                  techniques on a synthetic example and two real
                  tracking problems. The first involves the tracking
                  of a designated object in a video sequence based on
                  its colour properties, whereas the second involves
                  contour extraction in a single image.},
  linkpdf =	 shefftp # {variationalTracking.pdf},
  group =	 {shefml}
}

@TechReport{lawrence:sparse02,
  title =	 {Sparse {B}ayesian Learning: The Informative Vector
                  Machine},
  author =	 {Neil D. Lawrence and Matthias Seeger and Ralf
                  Herbrich},
  institution =	 {Department of Computer Science, Sheffield, UK},
  note =	 {This document has now evolved into
                  \cite{Lawrence:ivmTech04}.},
  year =	 {2002}
}

@TechReport{lawrence:variationalguide02,
  author =	 {Neil D. Lawrence},
  title =	 {Variational Inference Guide},
  institution =	 {University of Sheffield},
  year =	 {2002},
  abstract =	 {This report is a brief introduction to variational
                  inference for Bayesian models from the perspective
                  of the Expectation Maximisation (EM) algorithm
                  \cite{Dempster:EM77}. We start with an overview of
                  the EM algorithm from the perspective of variational
                  inference and then we show how approximate inference
                  may also be performed. We discuss briefly when
                  variational inference may be used and finally we
                  mention the variational importance sampler as an
                  alternative approach.},
  linkpsgz =	 shefftp # {variationalInference.ps.gz},
  linkpdf =	 shefftp # {variationalInference.pdf},
  group =	 {shefml}
}

@FailedPatent{Lawrence:patent03,
  author =	 {Neil D. Lawrence and Marta Milo},
  title =	 {Variational Importance Sampling},
  patentnumber = {0217827.5},
  country =      {U.K.},
  year =	 2003
}
@TechReport{Lawrence:matching04,
  author =	 {Neil D. Lawrence and Guido Sanguinetti},
  title =	 {Matching Kernels through {K}ullback-{L}eibler
                  Divergence Minimisation},
  year =	 2004,
  OPTkey =	 {},
  OPTtype =	 {},
  institution =	 sheftech,
  number =	 {CS-04-12},
  linkpsgz =	 shefftp # KLobjTech.ps.gz,
  OPTmonth =	 {},
  OPTnote =	 {},
  abstract =	 {In this paper we study the general constrained
                  minimisation of Kullback-Leibler (KL) divergences
                  between two zero mean Gaussian distributions. We
                  reduce the problem to an equivalent minimisation
                  involving the eigenvectors of the two kernel
                  matrices, and provide explicit solutions in some
                  cases. We then focus, as an example, on the
                  important case of constraining the approximating
                  matrix to be block diagonal. We prove a stability
                  result on the approximating matrix, and speculate on
                  how these results may be used to give further
                  theoretical foundation to widely used techniques
                  such as spectral clustering.}
}

@Article{Sanguinetti:chipdyno06,
  author =	 {Guido Sanguinetti and Magnus Rattray and Neil D. Lawrence},
  title =	 {A probabilistic dynamical model for quantitative
                  inference of the regulatory mechanism of
                  transcription},
  journal =	 bioinf,
  year =	 2006,
  volume =	 22,
  number =	 14,
  pages =	 {1753--1759},
  pmid =	 16632490,
  doi =		 {10.1093/bioinformatics/btl154},
  linkpdf =
                  {http://bioinformatics.oxfordjournals.org/cgi/reprint/22/14/1753},
  label2 =	 {Supplementary Material},
  link2 =	 softwarehttp # {chipdyno/supplementary.pdf},
  linksoftware = softwarehttp # {chipdyno/},
  abstract =	 {{\bf Motivation:} Quantitative estimation of the
                  regulatory relationship between transcription
                  factors and genes is a fundamental stepping stone
                  when trying to develop models of cellular
                  processes. This task, however, is difficult for a
                  number of reasons: transcription factors' expression
                  levels are often low and noisy, and many
                  transcription factors are post-transcriptionally
                  regulated. It is therefore useful to infer the
                  activity of the transcription factors from the
                  expression levels of their target genes.\\\\ {\bf
                  Results:} We introduce a novel probabilistic model
                  to infer transcription factor activities from
                  microarray data when the structure of the regulatory
                  network is known. The model is based on regression,
                  retaining the computational efficiency to allow
                  genome-wide investigation, but is rendered more
                  flexible by sampling regression coefficients
                  independently for each gene. This allows us to
                  determine the strength with which a transcription
                  factor regulates each of its target genes, therefore
                  providing a quantitative description of the
                  transcriptional regulatory network. The
                  probabilistic nature of the model also means that we
                  can associate credibility intervals to our estimates
                  of the activities. We demonstrate our model on two
                  yeast data sets. In both cases the network structure
                  was obtained using Chromatine Immunoprecipitation
                  data. We show how predictions from our model are
                  consistent with the underlying biology and offer
                  novel quantitative insights into the regulatory
                  structure of the yeast cell.\\\\ {\bf Availability:}
                  MATLAB code is available from
                  \url{http://umber.sbs.man.ac.uk/resources/puma}.},
  group =	 {gene networks,shefml,puma},
}

@InProceedings{Lawrence:transcriptionalGP06,
  author =	 {Neil D. Lawrence and Guido Sanguinetti and Magnus
                  Rattray},
  title =	 {Modelling transcriptional regulation using
                  {G}aussian Processes},
  crossref =	 {Schoelkopf:nips06},
  pages =	 {785--792},
  linksoftware = softwarehttp # {gpsim/},
  linkpdf =	 shefftp # {gpsim.pdf},
  linkpsgz =	 shefftp # {gpsim.ps.gz},
  abstract =	 {Modelling the dynamics of transcriptional processes
                  in the cell requires the knowledge of a number of
                  key biological quantities. While some of them are
                  relatively easy to measure, such as mRNA decay rates
                  and mRNA abundance levels, it is still very hard to
                  measure the active concentration levels of the
                  transcription factor proteins that drive the process
                  and the sensitivity of target genes to these
                  concentrations. In this paper we show how these
                  quantities for a given transcription factor can be
                  inferred from gene expression levels of a set of
                  known target genes. We treat the protein
                  concentration as a latent function with a Gaussian
                  Process prior, and include the sensitivities, mRNA
                  decay rates and baseline expression levels as
                  hyperparameters. We apply this procedure to a human
                  leukemia dataset, focusing on the tumour repressor
                  p53 and obtaining results in good accordance with
                  recent biological studies.},
  group =	 {gene networks,shefml,puma},
  errata1 =	 {Equation after equation (5): the square outside the
                  $\exp(\gamma_k)$ term should be inside the
                  bracket. Same applies to equation after equation
                  (6)},
  errataCredit1 ={Pei Gao and David Luengo},
  errata2 =	 {Equation after equation (5): there is a missing $t$
                  on the second line of the equation after $D_j$.},
  errataCredit2 ={Pei Gao},
  errata3 =	 {Equation (10): There is a missing log on the left
                  hand side of the equation.},
  errataCredit3 ={Pei Gao},
  errata4 =	 {Equation (10): The sign before log(\sigma_{ji}^2)
                  should be positive, not negative.},
  errataCredit4 ={Pei Gao},
  errata5 =	 {Equation (7): We missed the mean function which
                  should be subtracted from the genes observations,
                  $\mathbf{x}$, to get the posterior mean
                  prediction. It was also misimplemented in the
                  original code, but since everything needs to be
                  offset to fit the earlier results we didn't
                  notice. It is correct in the later journal paper
                  \cite{Gao:latent08}},
  errataCredit5 ={Pei Gao and James Anderson}
}

@Article{Milo:probabilistic03,
  author =	 {Marta Milo and Alireza Fazeli and Mahesan Niranjan
                  and Neil D. Lawrence},
  title =	 {A Probabilistic Model for the Extraction of
                  Expression Levels from Oligonucleotide Arrays},
  journal =	 {Biochemical Transations},
  year =	 2003,
  volume =	 31,
  number =	 6,
  pages =	 {1510--1512},
  abstract =	 {In this work we present a probabilistic model to
                  estimate summaries of Affymetrix GeneChip probe
                  level data. Comparisons with two different models
                  were made both on a publicly available dataset and
                  on a study performed in our laboratory, showing that
                  our model performs better for consistency of fold
                  change.},
  group =	 {shefml,mig,puma},
  linkpdf =	 shefftp # {probabilisticOligo.pdf}
}

@TechReport{Lawrence:gplvmtut06,
  author =	 {Neil D. Lawrence},
  title =	 {The {G}aussian Process Latent Variable Model},
  institution =	 sheftech,
  year =	 2006,
  number =	 {CS-06-03},
  abstract =	 {The Gaussian process latent variable model (GP-LVM)
                  is a recently proposed probabilistic approach to
                  obtaining a reduced dimension representation of a
                  data set. In this tutorial we motivate and describe
                  the GP-LVM, giving reviews of the model itself and
                  some of the concepts behind it.},
  linkpdf =	 shefftp # {gplvmTutorial.pdf},
  label1 =	 {Demos Software},
  link1 =	 softwarehttp # {oxford/},
  label2 =	 {Main Software},
  link2 =	 softwarehttp # {fgplvm/},
  group =	 {shefml,gplvm,motion,dimensional reduction}
}

@Poster{Lawrence:varoptexit07,
  author =  	 {Neil D. Lawrence},
  title =  	 {Variational Optimisation by Marginal Matching},
  venue =  	 {NIPS 07 Workshop on Approximate Bayesian Inference in Continuous/Hybrid Models, Whistler, Canada},
  linkpdf =	 shefftp # {epvarPoster.pdf},
  label1 =	 {Spotlight},
  link1 =	 shefftp # {epvarSpotlight.pdf},
  label2 =	 {Spotlight Video},
  link2 =	 {http://videolectures.net/abi07_lawrence_vap/snippet/},
  year =  	 2007, 
  month =  	 12,
  day =  	 7,
  group =  	 {ep}
}

@Talk{Lawrence:rank11,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models: Combining the Mechanistic and Data Driven
 Modelling Paradigms},
  venue =        {Rank Prize Workshop, Grasmere, Lake District},
  OPTlinkpdf =	 shefftp # {},
  year =  	 2012, 
  month =  	 03,
  day =  	 {26--29},
  OPTabstract =   	 {}
}

@Talk{Lawrence:cambridge11,
  author =  	 {Neil D. Lawrence},
  title =  	 {A Maximum Entropy Perspective on Spectral Dimensionality Reduction},
  abstract = 	 {Spectral approaches to dimensionality reduction typically reduce the dimensionality of a data set through taking the eigenvectors of a Laplacian or a similarity matrix. Classical multidimensional scaling also makes use of the eigenvectors of a similarity matrix. In this talk we introduce a maximum entropy approach to designing this similarity matrix. The approach is closely related to maximum variance unfolding. Other spectral approaches, e.g. locally linear embeddings, turn out to also be closely related. These  methods can be seen as a sparse Gaussian graphical model where correlations between data points (rather than across data features) are specified in the graph. The hope is that this unifying perspective will allow the relationships between these methods to be better understood and will also provide the groundwork for further research.},
  venue =  	 {Machine Learning @ CUED, University of Cambridge, U.K.},
  linkpdf =	 shefftp # {spectral_cambridge11.pdf},
  mp3 =          shefftp # {111116_ode_cambridge11.mp3},
  youtube =      {2XM2tS6TKhA},
  year =  	 2011, 
  month =  	 11,
  day =  	 16
}
@Talk{Lawrence:liverpool11,
  author =  	 {Neil D. Lawrence},
  title =  	 {Model Based Target Identification from Expression Data},
    venue =  	 {Gene Expression Profiling Workshop, Liverpool, UK},
  linkpdf =	 shefftp # {ode_liverpool11.pdf},
  mp3 =          shefftp # {111012_ode_liverpool11.mp3},
  youtube =      {pGVMRyfulB0},
  year =  	 2011, 
  month =  	 10,
  day =  	 12,
  abstract =   	 {A simple approach to target identification through
                  gene expression studies has been to cluster the
                  expression profiles and look for coregulated genes
                  within clusters. Within systems biology mechanistic
                  models of gene expression are typically constructed
                  through differential equations. mRNA's production is
                  taken to be proportional to transcription factor
                  activity (with the proportionality given by the
                  sensitivity) and the mRNA is assumed to decay at a
                  particular rate. The assumption that coregulated
                  genes have similar profiles is equivalent to
                  assuming both the decay and the sensitivity are
                  high. \\\\ 

                  Typically researchers either use a data driven
                  approach (such as clustering) or a model based
                  approach (such as differential equations). In this
                  talk we advocate hybrid techniques which have
                  aspects of the mechanistic and data driven
                  models. We combine simple differential equation
                  models with Gaussian process priors to make
                  probabilistic models with mechanistic
                  underpinnings. We show applications in target
                  identification from mRNA measurements.}  
}


@Talk{Lawrence:abcd11,
  author =  	 {Neil D. Lawrence},
  title =  	 {Between Systems and Data-driven Modeling for Computational Biology: Target Identification with {Gaussian} Processes},
    venue =  	 {ABCD2011, Ravenna, Italy},
  linkpdf =	 shefftp # {ode_abcd11.pdf},
  mp3 =          shefftp # {110910_ode_abcd11.mp3},
  year =  	 2011, 
  month =  	 09,
  day =  	 10,
  abstract =   	 {A simple approach to target identification through
                  gene expression studies has been to cluster the
                  expression profiles and look for coregulated genes
                  within clusters. Within systems biology mechanistic
                  models of gene expression are typically constructed
                  through differential equations. mRNA's production is
                  taken to be proportional to transcription factor
                  activity (with the proportionality given by the
                  sensitivity) and the mRNA is assumed to decay at a
                  particular rate. The assumption that coregulated
                  genes have similar profiles is equivalent to
                  assuming both the decay and the sensitivity are
                  high. \\\\ 

                  Typically researchers either use a data driven
                  approach (such as clustering) or a model based
                  approach (such as differential equations). In this
                  talk we advocate hybrid techniques which have
                  aspects of the mechanistic and data driven
                  models. We combine simple differential equation
                  models with Gaussian process priors to make
                  probabilistic models with mechanistic
                  underpinnings. We show applications in target
                  identification from mRNA measurements.}  
}

@Talk{Lawrence:bayes11,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models},
    abstract =   {Physics based approaches to data modeling involve 
                 constructing an accurate mechanistic model of data,
                 often based on differential equations. Statistical
                 and machine learning approaches are typically data
                 driven---perhaps through regularized function
                 approximation.\\\\

                 These two approaches to data modeling are often seen
                 as polar opposites, but in reality they are two
                 different ends to a spectrum of approaches we might
                 take. Physics based approaches can be seen as
                 \emph{strongly mechanistic}, the mechanistic
                 assumptions are hard encoded into the
                 model. Data-driven approaches do incorporate
                 assumptions that might be seen as being derived from
                 some underlying mechanism, such as smoothness. In
                 this sense they are \emph{weakly mechanistic}.\\\\

                 In this talk we introduce latent force models. Latent
                 force models are a new approach to data
                 representation that model data through unknown
                 forcing functions that drive differential equation
                 models.  By treating the unknown forcing functions
                 with Gaussian process priors we can create
                 probabilistic models that exhibit particular physical
                 characteristics of interest, for example, in
                 dynamical systems resonance and inertia. This allows
                 us to perform a synthesis of the data driven and
                 physical modeling paradigms. A \emph{moderately
                 mechanistic} approach. We show an application in
                 modelling of human motion capture data.},

  venue =  	 {Bayes 250 Workshop, University of Edinburgh, U.K.},
  linkpdf =	 shefftp # {lfm_bayes250.pdf},
  mp3 =          shefftp # {110906_lfm_bayes250.mp3},
  year =  	 2011, 
  month =  	 9,
  day =  	 6,
  group =  	 {}
}


@Talk{Lawrence:dagstuhl11,
  author =  	 {Neil D. Lawrence},
  title =  	 {Gaussian Processes and Probabilistic Models for Dimensionality Reduction},
  venue =  	 {Schloss Dagstuhl, Germany},
  year =  	 2011, 
  month =  	 8,
  day =  	 25,
  mp3 =          shefftp # {110825_probDimRed_dagstuhl11.mp3},
  linkpdf =	 shefftp # {probDimRed_dagstuhl11.pdf},
  abstract =     {In this talk we present an overview of probabilistic 
                 approaches to dimensionality reduction and
                 probabilistic interpretations of dimensionality
                 reduction. We start by reviewing spectral methods and
                 then turn to probabilistic PCA and the Gaussian
                 process latent variable model.},
  group =  	 {gplvm,lfm}
}


@Talk{Lawrence:krebs11,
  author =  	 {Neil D. Lawrence},
  title =  	 {Model Based Target Identification from Expression Data},
  venue =  	 {Krebs Institute Symposium},
  year =  	 2011, 
  month =  	 6,
  day =  	 7,
  abstract =     {A simple approach to target identification through gene 
                 expression studies has been to cluster the expression
                 profiles and look for coregulated genes within
                 clusters. Within systems biology mechanistic models
                 of gene expression are typically constructed through
                 differential equations. mRNA's production is taken to
                 be proportional to transcription factor activity
                 (with the proportionality given by the sensitivity)
                 and the mRNA is assumed to decay at a particular
                 rate. The assumption that coregulated genes have
                 similar profiles is equivalent to assuming both the
                 decay and the sensitivity are high.\\\\

                 Typically researchers either use a data driven
                 approach (such as clustering) or a model based
                 approach (such as differential equations). In this
                 talk we advocate hybrid techniques which have aspects
                 of the mechanistic and data driven models. We combine
                 simple differential equation models with Gaussian
                 process priors to make probabilistic models with
                 mechanistic underpinnings. We show applications in
                 target identification from mRNA measurements.},
  group =  	 {gplvm,lfm}
}

@Talk{Lawrence:bonn11,
  author =  	 {Neil D. Lawrence},
  title =  	 {A unifying probabilistic perspective on spectral approaches to dimensionality reduction},
  venue =  	 {Hausdorff Research Institute for Mathematics, Workshop on Manifold Learning, University of Bonn},
  year =  	 2011, 
  month =  	 5,
  day =  	 31,
  linkpdf =	 shefftp # {spectral_bonn11.pdf},
  abstract =     {Spectral approaches to dimensionality reduction typically 
                 reduce the dimensionality of a data set through
                 taking the eigenvectors of a Laplacian or a
                 similarity matrix. Classical multidimensional scaling
                 also makes use of the eigenvectors of a similarity
                 matrix. In this talk we introduce a maximum entropy
                 approach to designing this similarity matrix. The
                 approach is closely related to maximum variance
                 unfolding. Other spectral approaches such as locally
                 linear embeddings and Laplacian eigenmaps also turn
                 out to be closely related. Each method can be seen as
                 a sparse Gaussian graphical model where correlations
                 between data points (rather than across data
                 features) are specified in the graph. This also
                 suggests optimization via sparse inverse covariance
                 techniques such as the graphical LASSO. The hope is
                 that this unifying perspective will allow the
                 relationships between these methods to be better
                 understood and will also provide the groundwork for
                 further research.}
}

@Talk{Lawrence:siena11b,
  author =  	 {Neil D. Lawrence},
  title =  	 {Advanced Use of {G}aussian Processes},
  venue =  	 {University of Siena, Italy},
  linkpdf =	 shefftp # {gpAdvanced.pdf},
  year =  	 2011, 
  month =  	 4,
  day =  	 7,
  group =  	 {gplvm,lfm}
}

@Talk{Lawrence:siena11a,
  author =  	 {Neil D. Lawrence},
  title =  	 {Introduction to {G}aussian Processes},
  venue =  	 {Mathematics and Computer Science, University of Siena, Italy},
  linkpdf =	 shefftp # {gpReview.pdf},
  year =  	 2011, 
  month =  	 4,
  day =  	 6,
  group =  	 {gp}
}

@Talk{Lawrence:exeter11,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models},
    abstract =   {Physics based approaches to data modeling involve 
                 constructing an accurate mechanistic model of data,
                 often based on differential equations. Machine
                 learning approaches are typically data
                 driven---perhaps through regularized function
                 approximation.

                 These two approaches to data modeling are often seen
                 as polar opposites, but in reality they are two
                 different ends to a spectrum of approaches we might
                 take.

                 In this talk we introduce latent force models. Latent
                 force models are a new approach to data
                 representation that model data through unknown
                 forcing functions that drive differential equation
                 models.  By treating the unknown forcing functions
                 with Gaussian process priors we can create
                 probabilistic models that exhibit particular physical
                 characteristics of interest, for example, in
                 dynamical systems resonance and inertia. This allows
                 us to perform a synthesis of the data driven and
                 physical modeling paradigms. We will show
                 applications of these models in systems biology and
                 (given time) modelling of human motion capture data.},
  venue =  	 {Mathematics and Computer Science, University of Exeter, U.K.},
  linkpdf =	 shefftp # {lfm_exeter.pdf},
  year =  	 2011, 
  month =  	 3,
  day =  	 16,
  group =  	 {gplvm}
}

@Talk{Lawrence:loughborough11,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle5,
  abstract = 	 gplvmAbstract5,
  venue =  	 {Department of Computer Science, University of Loughgborough, U.K.},
  linkpdf =	 shefftp # {loughborough_gplvm.pdf},
  label1 =	 {Bayesian GPLVM Software},
  link1 =	 softwarehttp # {vargplvm/},
  label2 =	 {Main Software},
  link2 =	 softwarehttp # {fgplvm/},
  year =  	 2011, 
  month =  	 3,
  day =  	 09,
  group =  	 {gplvm}
}
@Talk{Lawrence:edinburgh11,
  author =  	 {Neil D. Lawrence},
  title =  	 {A Unifying Probabilistic Perspective on Spectral Approaches to Dimensionality Reduction},
  abstract = 	 {Spectral approaches to dimensionality reduction
                  typically reduce the dimensionality of a data set
                  through taking the eigenvectors of a Laplacian or a
                  similarity matrix. Classical multidimensional
                  scaling also makes use of the eigenvectors of a
                  similarity matrix. In this talk we introduce a
                  maximum entropy approach to designing this
                  similarity matrix. The approach is closely related
                  to maximum variance unfolding. Other spectral
                  approaches such as locally linear embeddings and
                  Laplacian eigenmaps also turn out to be closely
                  related. Each method can be seen as a sparse
                  Gaussian graphical model where correlations between
                  data points (rather than across data features) are
                  specified in the graph. This also suggests
                  optimization via sparse inverse covariance
                  techniques such as the graphical LASSO. The hope is
                  that this unifying perspective will allow the
                  relationships between these methods to be better
                  understood and will also provide the groundwork for
                  further research.},
  venue =  	 {ANC/DTC Seminar, School of Informatics, University of Edinburgh, U.K.},
  linkpdf =	 shefftp # {spectral_edinburgh11.pdf},
  year =  	 2011, 
  month =  	 03,
  day =  	 01
}

@Talk{Lawrence:smpgd11,
  author =  	 {Neil D. Lawrence},
  title =  	 {Between Systems and Data-driven Modeling for Computational Biology: Target Identification with {Gaussian} Processes},
    venue =  	 {SMPGD2010, Paris, France},
  linkpdf =	 shefftp # {ode_smpgd11.pdf},
  year =  	 2011, 
  month =  	 01,
  day =  	 27,
  abstract =   	 {A simple approach to target identification through
                  gene expression studies has been to cluster the
                  expression profiles and look for coregulated genes
                  within clusters. Within systems biology mechanistic
                  models of gene expression are typically constructed
                  through differential equations. mRNA's production is
                  taken to be proportional to transcription factor
                  activity (with the proportionality given by the
                  sensitivity) and the mRNA is assumed to decay at a
                  particular rate. The assumption that coregulated
                  genes have similar profiles is equivalent to
                  assuming both the decay and the sensitivity are
                  high.  \\\\ Typically researchers either use a data
                  driven approach (such as clustering) or a model
                  based approach (such as differential equations). In
                  this talk we advocate hybrid techniques which have
                  aspects of the mechanistic and data driven
                  models. We combine simple differential equation
                  models with Gaussian process priors to make
                  probabilistic models with mechanistic
                  underpinnings. We show applications in target
                  identification from mRNA measurements.}
}

@Talk{Lawrence:nipsw10,
  author =  	 {Neil D. Lawrence},
  title =  	 {A Probabilistic Perspective on Spectral Dimensionality Reduction},
    venue =  	 {Challenges of Data Visualization Workshop, NIPS 2010, Whistler},
  linkpdf =	 shefftp # {spectral_nipsw10.pdf},
  year =  	 2010, 
  month =  	 12,
  day =  	 11,
  abstract =   	 {Spectral approaches to dimensionality reduction
                  typically reduce the dimensionality of a data set
                  through taking the eigenvectors of a Laplacian or a
                  similarity matrix. Classical multidimensional
                  scaling also makes use of the eigenvectors of a
                  similarity matrix. In this talk we introduce a
                  maximum entropy approach to designing this
                  similarity matrix. The approach is closely related
                  to maximum variance unfolding and other spectral
                  approaches such as locally linear embeddings and
                  Laplacian eigenmaps also turn out to be closely
                  related. Each method can be seen as a sparse
                  Gaussian graphical model where correlations between
                  data points (rather than across data features) are
                  specified in the graph. This also suggests
                  optimization via sparse inverse covariance
                  techniques such as the graphical LASSO. The hope is
                  that this unifying perspective will allow the
                  relationships between these methods to be better
                  understood and will also provide the groundwork for
                  further research.}
}
@Talk{Lawrence:lfmSheffield10,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models},
    venue =  	 {Dynamics Research Group Seminar, Mechanical Engineering, University of Sheffield, U.K.},
  linkpdf =	 shefftp # {lfm_sheffield10.pdf},
  year =  	 2010, 
  month =  	 11,
  day =  	 04,
  abstract =   	 {Physics based approaches to data modeling involve 
                 constructing an accurate mechanistic model of data,
                 often based on differential equations. Machine
                 learning approaches are typically data
                 driven---perhaps through regularized function
                 approximation.

                 These two approaches to data modeling are often seen
                 as polar opposites, but in reality they are two
                 different ends to a spectrum of approaches we might
                 take.

                 In this talk we introduce latent force models. Latent
                 force models are a new approach to data
                 representation that model data through unknown
                 forcing functions that drive differential equation
                 models.  By treating the unknown forcing functions
                 with Gaussian process priors we can create
                 probabilistic models that exhibit particular physical
                 characteristics of interest, for example, in
                 dynamical systems resonance and inertia. This allows
                 us to perform a synthesis of the data driven and
                 physical modeling paradigms. We will show
                 applications of these models in systems biology and
                 (given time) modelling of human motion capture data.}
}

@Talk{Lawrence:aalto10,
  author =  	 {Neil D. Lawrence},
  title =  	 {A Probabilistic Perspective on Spectral Dimensionality Reduction},
    venue =  	 {Department of Statistics, Aalto University, Finland},
  linkpdf =	 shefftp # {spectral_aalto10.pdf},
  year =  	 2010, 
  month =  	 10,
  day =  	 20,
  abstract =   	 {Spectral approaches to dimensionality reduction typically reduce the dimensionality of a data set through taking the eigenvectors of a Laplacian or a similarity matrix. Classical multidimensional scaling also makes use of the eigenvectors of a similarity matrix. In this talk we introduce a maximum entropy approach to designing this similarity matrix. The approach is closely related to maximum variance unfolding and other spectral approaches such as locally linear embeddings and Laplacian eigenmaps also turn out to be closely related. Each method can be seen as a sparse Gaussian graphical model where correlations between data points (rather than across data features) are specified in the graph. This also suggests optimization via sparse inverse covariance techniques such as the graphical LASSO. The hope is that this unifying perspective will allow the relationships between these methods to be better understood and will also provide the groundwork for further research.}
}

@Talk{Lawrence:aaai10,
  author =  	 {Neil D. Lawrence},
  title =  	 {A Probabilistic Perspective on Spectral Dimensionality Reduction},
    venue =  	 {AAAI Fall Symposium on Manifold Methods, U.S.A.},
  linkpdf =	 shefftp # {spectral_aaai10.pdf},
  year =  	 2010, 
  month =  	 11,
  day =  	 11,
  abstract =   	 {Spectral approaches to dimensionality reduction
                  typically reduce the dimensionality of a data set
                  through taking the eigenvectors of a Laplacian or a
                  similarity matrix. Classical multidimensional
                  scaling also makes use of the eigenvectors of a
                  similarity matrix. In this talk we introduce a
                  maximum entropy approach to designing this
                  similarity matrix. The approach is closely related
                  to maximum variance unfolding and other spectral
                  approaches such as locally linear embeddings and
                  Laplacian eigenmaps also turn out to be closely
                  related. Each method can be seen as a sparse
                  Gaussian graphical model where correlations between
                  data points (rather than across data features) are
                  specified in the graph. This also suggests
                  optimization via sparse inverse covariance
                  techniques such as the graphical LASSO. The hope is
                  that this unifying perspective will allow the
                  relationships between these methods to be better
                  understood and will also provide the groundwork for
                  further research.}
}
@Talk{Lawrence:eurogene10,
  author =  	 {Neil D. Lawrence and Antti Honkela},
  title =  	 {Bayesian approaches to Transcription Factor Target Identification},
    venue =  	 {Course in Practical Systems Biology: Visualisation and Reverse engineering gene regulatory networks, EuroMediterranean University Centre of Ronzano, Bologna, Italy},
  linkpdf =	 shefftp # {gp_bologna10.pdf},
  year =  	 2010, 
  month =  	 10,
  day =  	 10,
  abstract =   	 {A simple approach to target identification through
                  gene expression studies has been to cluster the
                  expression profiles and look for coregulated genes
                  within clusters. Within systems biology mechanistic
                  models of gene expression are typically constructed
                  through differential equations. mRNA's production is
                  taken to be proportional to transcription factor
                  activity (with the proportionality given by the
                  sensitivity) and the mRNA is assumed to decay at a
                  particular rate. The assumption that coregulated
                  genes have similar profiles is equivalent to
                  assuming both the decay and the sensitivity are
                  high.  

                  In this lecture we introduce Bayesian
                  approaches to target identification which make use
                  of sampling approaches to rank candidate lists of
                  targets. We will begin with an introduction to the
                  target identification problem and an overview of the
                  power of Bayesian approaches in solving it. We will
                  then consider how probabilistic models such as
                  Gaussian processes can be used for ranking potential
                  targets of a transcription factor. These models are
                  simple enough to allow genome wide target
                  identification, but rich enough to encode dynamical
                  behavior that, allowing us to identify putative
                  targets even when decay rates are low.}
}
@Talk{Lawrence:validation10,
  author =  	 {Neil D. Lawrence},
  title =  	 {Making Implementations Available for the Research Community},
    venue =  	 {Validation in Statistics and Machine Learning, Weierstrass Institute, Berlin, Germany},
  linkpdf =	 shefftp # {reproducible_berlin10.pdf},
  year =  	 2010, 
  month =  	 10,
  day =  	 6,
  abstract =   	 {Machine learning research is either inspired by a particular application, or by a general desire to make technology more ``inteligent''. In modern machine learning most methodological development is mathematically inspired and results in an algorithm for optimization or fitting of a model to data. 

Design choices in implementation of an algorithm can have a significant effect on the quality of results. Decisions such as model initializaiton and data pre-processing are all part of the implementation. Necessarily, space constraints sometimes mean that such details are not included in the associated paper. It seems clear that the paper only tells part of the story. Implementations need to be made available at the time of submission of the paper, so that the full story may be followed. In our research group we have done this since 2001. In this talk I will make the arguments in favour of doing this universally and give personal experiences of the results.}
}
@Talk{Lawrence:phylogenetics10,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models},
    venue =  	 {Functional Phylogenies: Evolutionary Inference for Functional Data, University of Oxford, U.K.},
  linkpdf =	 shefftp # {lfm_oxford10.pdf},
  year =  	 2010, 
  month =  	 09,
  day =  	 27,
  abstract =   	 {Physics based approaches to data modeling involve constructing an
accurate mechanistic model of data, often based on differential
equations. Machine learning approaches are typically data
driven---perhaps through regularized function approximation.

These two approaches to data modeling are often seen as polar
opposites, but in reality they are two different ends to a spectrum of
approaches we might take.

In this talk we introduce latent force models. Latent force models are
a new approach to data representation that model data through unknown
forcing functions that drive differential equation models.  By
treating the unknown forcing functions with Gaussian process priors we
can create probabilistic models that exhibit particular physical
characteristics of interest, for example, in dynamical systems
resonance and inertia. This allows us to perform a synthesis of the
data driven and physical modeling paradigms. We will show applications
of these models in systems biology and (given time) modelling of human
motion capture data.}
}
@Talk{Lawrence:tutorialPRIB10,
  author =  	 {Neil D. Lawrence},
  title =  	 {{PRIB} Tutorial: {G}aussian Processes and Gene Regulation},
    venue =  	 {PRIB2010, Radboud University, Nijmegen, Netherlands},
  linkpdf =	 shefftp # {gp_prib10.pdf},
  year =  	 2010, 
  month =  	 09,
  day =  	 22,
  abstract =   	 {Computational biology models are often missing information, such as the concentration of biochemical species of interest. One approach to dealing with this missing information is to place a probabilistic prior over the missing data. One possible choice for such a prior is a Gaussian process.

In this tutorial we will give an introduction to Gaussian processes. We will give simple examples of Gaussian processes in regression and interpolation. We will then show how Gaussian processes can be incorporated with differential equation models to give probabilistic models for transcription. Such models can then be used to rank potential targets of given transcription factors. }

}


@Talk{Lawrence:ibsb10,
  author =  	 {Neil D. Lawrence},
  title =  	 {Between Systems and Data-driven Modeling for Computational Biology: Target Identification with {Gaussian} Processes},
    venue =  	 {IBSB2010, Kyoto University, Japan},
  linkpdf =	 shefftp # {ode_kyoto10.pdf},
  year =  	 2010, 
  month =  	 07,
  day =  	 27,
  abstract =   	 {A simple approach to target identification through gene expression studies has been to cluster the expression profiles and look for coregulated genes within clusters. Within systems biology mechanistic models of gene expression are typically constructed through differential equations. mRNA's production is taken to be proportional to transcription factor activity (with the proportionality given by the sensitivity) and the mRNA is assumed to decay at a particular rate. The assumption that coregulated genes have similar profiles is equivalent to assuming both the decay and the sensitivity are high.  

Typically researchers either use a data driven approach (such as clustering) or a model based approach (such as differential equations). In this talk we advocate hybrid techniques which have aspects of the mechanistic and data driven models. We combine simple differential equation models with Gaussian process priors to make probabilistic models with mechanistic underpinnings. We show applications in target identification from mRNA measurements.}
}

@Talk{Lawrence:inference10,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models},
  venue =  	 {Inference Group, Cavendish Laboratory, University of Cambridge},
  linkpdf =	 shefftp # {lfm_inference10.pdf},
  year =  	 2010, 
  month =  	 03,
  day =  	 01,
  abstract =   	 {Physics based approaches to data modeling involve 
                 constructing an accurate mechanistic model of data,
                 often based on differential equations. Machine
                 learning approaches are typically data driven---
                 perhaps through regularized function approximation.\\\\

                 These two approaches to data modeling are often seen
                 as polar opposites, but in reality they are two
                 different ends to a spectrum of approaches we might
                 take.\\\\

                 In this talk we introduce latent force models. Latent
                 force models are a new approach to data
                 representation that model data through unknown
                 forcing functions that drive differential equation
                 models.  By treating the unknown forcing functions
                 with Gaussian process priors we can create
                 probabilistic models that exhibit particular physical
                 characteristics of interest, for example, in
                 dynamical systems resonance and inertia. This allows
                 us to perform a synthesis of the data driven and
                 physical modeling paradigms. We will show
                 applications of these models in systems biology and
                 modelling of human motion capture data.}  
}


@Talk{Lawrence:tlsd09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Transfer Learning and Multiple Output Kernel Functions},
  venue =  	 {NIPS 09 Workshop on Transfer Learning for Structured Data},
  linkpdf =	 shefftp # {tlsd09.pdf},
  year =  	 2009, 
  month =  	 12,
  day =  	 12,
  abstract =   	 {A standard Bayesian approach to transfer learning is to 
                 construct hierarchical probabilistic models. Learning
                 tasks are typically related in the model through
                 conditional independencies of the
                 variables/parameters. Many of the variables are
                 unobserved. Marginalization of the unobserved
                 variables and Bayesian treatment of parameters
                 induces structure and correlations between the tasks.
                 Gaussian processes are prior distributions over
                 functions: kernel functions are the covariances
                 associated with these priors. A Gaussian process can
                 be set up to have multiple outputs. However, for
                 these outputs to have correlation between them a
                 covariance function that models correlations between
                 outputs is required. Equivalently we need to develop
                 multiple output kernel functions (also known as
                 multitask kernel functions, or structured output
                 kernels).  In this talk we will briefly review work
                 in creating multiple output kernels before focusing
                 on models represented by a convolution processes. We
                 will arrive at convolutional processes through
                 physical interpretations of our models. We will try
                 to illustrate these models with a range of real world
                 examples of both transfer learning and other
                 applications.}  
}



@Talk{Lawrence:kcl09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models},
  venue =  	 {Disordered Systems Group, Department of Mathematics, King's College London},
  linkpdf =	 shefftp # {lfm_kcl09.pdf},
  year =  	 2009, 
  month =  	 11,
  day =  	 25,
  abstract =   	 {Physics based approaches to data modeling involve 
                 constructing an accurate mechanistic model of data,
                 often based on differential equations. Machine
                 learning approaches are typically data driven---
                 perhaps through regularized function approximation.\\\\

                 These two approaches to data modeling are often seen
                 as polar opposites, but in reality they are two
                 different ends to a spectrum of approaches we might
                 take.\\\\

                 In this talk we introduce latent force models. Latent
                 force models are a new approach to data
                 representation that model data through unknown
                 forcing functions that drive differential equation
                 models.  By treating the unknown forcing functions
                 with Gaussian process priors we can create
                 probabilistic models that exhibit particular physical
                 characteristics of interest, for example, in
                 dynamical systems resonance and inertia. This allows
                 us to perform a synthesis of the data driven and
                 physical modeling paradigms. We will show
                 applications of these models in systems biology and
                 modelling of human motion capture data.}  
}

@Talk{Lawrence:tigem09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Nonlinear Response in {G}aussian Process Models of Transcription},
  venue =  	 {Telethon Institute of Genetics and Medicine, Naples, Italy},
  linkpdf =	 shefftp # {tigem09.pdf},
  year =  	 2009, 
  month =  	 10,
  day =  	 29,
  OPTabstract =   	 {}
}



@Talk{Lawrence:napoli09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Model Based Target Identification from Gene Expression with {G}aussian Processes},
  venue =  	 {BioDN@work '09, Computational Biology \& Bioinformatics, University of Naples ``Federico II''},
  linkpdf =	 shefftp # {ode_napoli09.pdf},
  year =  	 2009, 
  month =  	 10,
  day =  	 28,
  abstract =   	 {A simple approach to target identification through gene 

                 expression studies has been to cluster the expression
                 profiles and look for coregulated genes within
                 clusters. Within systems biology mechanistic models
                 of gene expression are typically constructed through
                 differential equations. mRNA's production is taken to
                 be proportional to transcription factor activity
                 (with the proportionality given by the sensitivity)
                 and the mRNA is assumed to decay at a particular
                 rate. The assumption that coregulated genes have
                 similar profiles is equivalent to assuming both the
                 decay and the sensitivity are high.

                In this talk we advocate model based target
                identification. We develop a simple probabilistic
                models of transcription (and translation) which encode
                mRNA (or Transcription Factor) production and
                decay. Our models are simple enough to allow genome
                wide target identification, but rich enough to encode
                dynamical behavior that, allowing us to identify
                putative targets even when decay rates are low.}
}


@Talk{Lawrence:nyu09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models},
  venue =  	 {Computer Science Colloquium, NYU, U.S.A.},
  linkpdf =	 shefftp # {lfm_nyu09.pdf},
  year =  	 2009, 
  month =  	 10,
  day =  	 23,
  abstract =   	 {Physics based approaches to data modeling involve 
                 constructing an accurate mechanistic model of data,
                 often based on differential equations. Machine
                 learning approaches are typically data driven---
                 perhaps through regularized function approximation.\\\\

                 These two approaches to data modeling are often seen
                 as polar opposites, but in reality they are two
                 different ends to a spectrum of approaches we might
                 take.\\\\

                 In this talk we introduce latent force models. Latent
                 force models are a new approach to data
                 representation that model data through unknown
                 forcing functions that drive differential equation
                 models.  By treating the unknown forcing functions
                 with Gaussian process priors we can create
                 probabilistic models that exhibit particular physical
                 characteristics of interest, for example, in
                 dynamical systems resonance and inertia. This allows
                 us to perform a synthesis of the data driven and
                 physical modeling paradigms. We will show
                 applications of these models in systems biology and
                 modelling of human motion capture data.}  
}

@Talk{Lawrence:google09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models},
  venue =  	 {Google Research, New York, U.S.A.},
  linkpdf =	 shefftp # {lfm_google09.pdf},
  year =  	 2009, 
  month =  	 10,
  day =  	 21,
  abstract =   	 {Physics based approaches to data modeling involve 
                 constructing an accurate mechanistic model of data,
                 often based on differential equations. Machine
                 learning approaches are typically data driven---
                 perhaps through regularized function approximation.\\\\

                 These two approaches to data modeling are often seen
                 as polar opposites, but in reality they are two
                 different ends to a spectrum of approaches we might
                 take.\\\\

                 In this talk we introduce latent force models. Latent
                 force models are a new approach to data
                 representation that model data through unknown
                 forcing functions that drive differential equation
                 models.  By treating the unknown forcing functions
                 with Gaussian process priors we can create
                 probabilistic models that exhibit particular physical
                 characteristics of interest, for example, in
                 dynamical systems resonance and inertia. This allows
                 us to perform a synthesis of the data driven and
                 physical modeling paradigms. We will show
                 applications of these models in systems biology and
                 modelling of human motion capture data.}  
}

@Talk{Lawrence:jhu09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Model Based Target Identification from Gene Expression with {G}aussian Processes},
  venue =  	 {School of Public Health, Johns Hopkins University, U.S.A.},
  linkpdf =	 shefftp # {ode_jhu09.pdf},
  year =  	 2009, 
  month =  	 10,
  day =  	 19,
  abstract =   	 {A simple approach to target identification through gene 

                 expression studies has been to cluster the expression
                 profiles and look for coregulated genes within
                 clusters. Within systems biology mechanistic models
                 of gene expression are typically constructed through
                 differential equations. mRNA's production is taken to
                 be proportional to transcription factor activity
                 (with the proportionality given by the sensitivity)
                 and the mRNA is assumed to decay at a particular
                 rate. The assumption that coregulated genes have
                 similar profiles is equivalent to assuming both the
                 decay and the sensitivity are high.

                In this talk we advocate model based target
                identification. We develop a simple probabilistic
                models of transcription (and translation) which encode
                mRNA (or Transcription Factor) production and
                decay. Our models are simple enough to allow genome
                wide target identification, but rich enough to encode
                dynamical behavior that, allowing us to identify
                putative targets even when decay rates are low.}
}

@Talk{Lawrence:newcastle09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Modelling with {G}aussian Processes},
  venue =  	 {School of Mathematics and Statistics, University of Newcastle, U.K.},
  linkpdf =	 shefftp # {lfm_newcastle09.pdf},
  year =  	 2009, 
  month =  	 10,
  day =  	 9,
  abstract =   	 {Physics based approaches to data modeling involve 
                 constructing an accurate mechanistic model of data,
                 often based on differential equations. Machine
                 learning typically focuses on data driven
                 approaches---perhaps through regularized function
                 approximations.\\\\

                 These two approaches to data modeling are often seen
                 as polar opposites, but in reality they are two
                 different ends to a spectrum of approaches we might
                 take.\\\\

                 In this talk we introduce latent force models. Latent
                 force models are a new approach to data
                 representation that model data through unknown
                 forcing functions that drive differential equation
                 models.  By treating the unknown forcing functions
                 with Gaussian process priors we can create
                 probabilistic models that exhibit particular physical
                 characteristics of interest, for example, in
                 dynamical systems resonance and inertia. This allows
                 us to perform a synthesis of the data driven and
                 physical modeling paradigms. We will show
                 applications of these models in systems biology and
                 modelling of human motion capture data.}  
}




@Talk{Lawrence:inspire09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models with {G}aussian Processes},
  venue =  	 {Inspire Workshop, Imperial College, U.K.},
  linkpdf =	 shefftp # {lfm_inspire09.pdf},
  year =  	 2009, 
  month =  	 9,
  day =  	 24,
  abstract =   	 {}
}

@Talk{Lawrence:warwick09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Efficient Multiple Output Convolution Processes for Multiple Task Learning},
  venue =  	 {Inf Workshop, University of Warwick, U.K.},
  linkpdf =	 shefftp # {lfm_warwick09.pdf},
  year =  	 2009, 
  month =  	 9,
  day =  	 16,
  abstract =   	 {}
}


@Talk{Lawrence:interspeech09,
  author =  	 {Neil D. Lawrence and Jon Barker},
  title =  	 {Dealing with High Dimensional Data with Dimensionality Reduction},
  venue =  	 {Interspeech Tutorial, Brighton, U.K.},
  linkpdf =	 shefftp # {interspeech09.pdf},
  year =  	 2009, 
  month =  	 9,
  day =  	 6,
  abstract =   	 {}
}


@Talk{Lawrence:lfm_slim09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models and Multiple Output {G}aussian Processes},
  venue =  	 {Statistics and Learning Interface Meeting, University of Manchester, U.K.},
  linkpdf =	 shefftp # {lfm_slim09.pdf},
  year =  	 2009, 
  month =  	 7,
  day =  	 23,
  abstract =   	 {We are used to dealing with the situation where we have a 
                 latent variable. Often we assume this latent variable
                 to be independently drawn from a distribution,
                 e.g. probabilistic PCA or factor analysis. This
                 simplification is often extended for temporal data
                 where tractable Markovian independence assumptions
                 are used (e.g. Kalman filters or hidden Markov
                 models). In this talk we will consider the more
                 general case where the latent variable is a forcing
                 function in a differential equation model. We will
                 show how for some simple ordinary differential
                 equations the latent variable can be dealt with
                 analytically for particular Gaussian process priors
                 over the latent force. In this talk we will introduce
                 the general framework and present results in systems
                 biology and motion capture.}
}

@Talk{Lawrence:lfm_cagliary09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models with {G}aussian Processes},
  venue =  	 {Pattern Recognition Applications Group, Department of Electrical and Electronic Engineering, University of Cagliari, Italy},
  linkpdf =	 shefftp # {lfm_cagliari09.pdf},
  year =  	 2009, 
  month =  	 7,
  day =  	 13,
  abstract =   	 {We are used to dealing with the situation where we have a
                 latent variable. Often we assume this latent variable
                 to be independently drawn from a distribution,
                 e.g. probabilistic PCA or factor analysis. This
                 simplification is often extended for temporal data
                 where tractable Markovian independence assumptions
                 are used (e.g. Kalman filters or hidden Markov
                 models). In this talk we will consider the more
                 general case where the latent variable is a forcing
                 function in a differential equation model. We will
                 firstly give a brief introduction to Gaussian
                 processes, then we will show how for some simple
                 ordinary differential equations the latent variable
                 can be dealt with analytically for particular
                 Gaussian process priors over the latent force. In
                 this talk we will introduce the general framework,
                 present results in systems biology and motion
                 capture.}
}




@Talk{Lawrence:tut09,
  author =  	 {Neil D. Lawrence},
  title =  	 {An Introduction to Systems Biology from a Machine Learning Perspective},
  venue =  	 {TISE Summer School, Tampere, Finland},
  linkpdf =	 shefftp # {tut.pdf},
  year =  	 2009, 
  month =  	 6,
  day =  	 22,
  abstract =   	 {}
}
@Talk{Lawrence:tutII09,
  author =  	 {Neil D. Lawrence},
  title =  	 {An Introduction to Systems Biology from a Machine Learning Perspective {II}},
  venue =  	 {TISE Summer School, Tampere, Finland},
  linkpdf =	 shefftp # {tut2.pdf},
  year =  	 2009, 
  month =  	 6,
  day =  	 23,
  abstract =   	 {}
}

@Talk{Lawrence:learning09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Non-linear Matrix Factorization with {G}aussian Processes},
  venue =  	 {Learning Workshop, Clearwater, Florida},
  linkpdf =	 shefftp # {snowbird09.pdf},
  year =  	 2009, 
  month =  	 4,
  day =  	 14,
  abstract =   	 {}
}

@Talk{Lawrence:emmds09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Non-linear Matrix Facorization with {G}aussian Proceses},
  venue =  	 {European Modern Massive Datasets Workshop, Denmark Techinical University, Copenhagen},
  linkpdf =	 shefftp # {emmds09.pdf},
  year =  	 2009, 
  month =  	 7,
  day =  	 3,
  abstract =   	 {A popular approach to collaborative filtering is matrix
                 factorization. In this talk we consider the
                 ``probabilistic matrix factorization'' and by taking
                 a latent variable model perspective we show its
                 equivalence to Bayesian PCA. This inspires us to
                 consider probabilistic PCA and its non-linear
                 extension, the Gaussian process latent variable model
                 (GP-LVM) as an approach for probabilistic non-linear
                 matrix factorization. We apply out approach to
                 benchmark movie recommender data sets. The results
                 show better than previous state-of-the-art
                 performance.}  
}

@Talk{Lawrence:bristol08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models with {G}aussian Processes},
  venue =  	 {Intelligent Systems Seminars, University of Bristol, U.K.},
  linkpdf =	 shefftp # {lfm_bristol08.pdf},
  year =  	 2008, 
  month =  	 10,
  day =  	 16,
  abstract =   	 {We are used to dealing with the situation where we 
                 have a latent variable. Often we assume this latent
                 variable to be independently drawn from a
                 distribution, e.g. probabilistic PCA or factor
                 analysis. This simplification is often extended for
                 temporal data where tractable Markovian independence
                 assumptions are used (e.g. Kalman filters or hidden
                 Markov models). In this talk we will consider the
                 more general case where the latent variable is a
                 forcing function in a differential equation model. We
                 will firstly give a brief introduction to Gaussian
                 processes, then we will show how for some simple
                 ordinary differential equations the latent variable
                 can be dealt with analytically for particular
                 Gaussian process priors over the latent force. In
                 this talk we will introduce the general framework,
                 present results in systems biology.\\\\

                 Joint work with Magnus Rattray, Mauricio \'Alvarez, Pei
                 Gao, Antti Honkela, David Luengo, Guido Sanguinetti
                 and Michalis K. Titsias. },

  group =  	 {ode, gp}
}
@Talk{Lawrence:rss08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Inference in Ordinary Differential Equations with Latent Functions through {G}aussian Processes},
  venue =  	 {RSS Manchester Local Group},
  linkpdf =	 shefftp # {manchesterRss.pdf},
  year =  	 2008, 
  month =  	 10,
  day =  	 8,
  abstract =   	 {In biochemical interaction networks is a key problem 
                 in estimation of the structure and parameters of the
                 genetic, metabolic and protein interaction networks
                 that underpin all biological processes. We present a
                 framework for Bayesian marginalisation of these
                 latent chemical species through Gaussian process
                 priors. We demonstrate our general approach on three
                 different biological examples of single input motifs,
                 including both activation and repression of
                 transcription.  We focus in particular on the problem
                 of inferring transcription factor activity when the
                 concentration of active protein cannot easily be
                 measured. The uncertainty in the inferred
                 transcription factor activity can be integrated out
                 in order to derive a likelihood function that can be
                 used for the estimation of regulatory model
                 parameters.},
  group =  	 {ode, gp}
}

@Talk{Lawrence:ncaf08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Dynamics with {G}aussian Processes},
  venue =  	 {Natural Computing Applications Forum, University of Sheffield, U.K.},
  linkpdf =	 shefftp # {dynamics_ncaf08.pdf},
  year =  	 2008, 
  month =  	 9,
  day =  	 10,
  abstract =   	 {We are used to dealing with the situation where we have 
                 a latent variable. Often we assume this latent
                 variable to be independently drawn from a
                 distribution, \emph{e.g.} probabilistic PCA or factor
                 analysis. This simplification is often extended for
                 temporal data where tractable Markovian independence
                 assumptions are used (\emph{e.g.} Kalman filters or
                 hidden Markov models). In this talk we will consider
                 the more general case where the latent variable is a
                 forcing function in a differential equation model. We
                 will firstly give a brief introduction to Gaussian
                 processes, then we will show how for some simple
                 ordinary differential equations the latent variable
                 can be dealt with analytically for particular
                 Gaussian process priors over the latent force. In
                 this talk we will introduce the general framework,
                 present results in systems biology.\\\\

                 Joint work with Magnus Rattray, Mauricio \'Alvarez, Pei
                 Gao, Antti Honkela, David Luengo, Guido Sanguinetti
                 and Michalis K. Titsias.},
  group =  	 {}
}

@Talk{Lawrence:mlmi08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Ambiguity Modelling in Latent Spaces},
  venue =  	 {Machine Learning for Multimodal Interaction, Utrecht, The Netherlands},
  linkpdf =	 shefftp # {ncca_mlmi08.pdf},
  linkvideo =	 {http://mmm.idiap.ch/pres-mlmi2008/results?query=//kleweldata/mlmi2008/2008-09-08_09h14},
  year =  	 2008, 
  month =  	 9,
  day =  	 8,
  abstract =   	 {We are interested in the situation where we have two
                  or more representations of an underlying
                  phenomenon. In particular we are interested in the
                  scenario where the representation are
                  complementary. This implies that a single individual
                  representation is not sufficient to fully
                  discriminate a specific instance of the underlying
                  phenomenon, it also means that each representation
                  is an ambiguous representation of the other
                  complementary spaces. In this paper we present a
                  latent variable model capable of consolidating
                  multiple complementary representations. Our method
                  extends canonical correlation analysis by
                  introducing additional latent spaces that are
                  specific to the different representations, thereby
                  explaining the full variance of the
                  observations. These additional spaces, explaining
                  representation specific variance, separately model
                  the variance in a representation ambiguous to the
                  other. We develop a spectral algorithm for fast
                  computation of the embeddings and a probabilistic
                  model (based on Gaussian processes) for validation
                  and inference. The proposed model has several
                  potential application areas, we demonstrate its use
                  for multi-modal regression on a benchmark human pose
                  estimation data set.},
  group =  	 {}
}

@Talk{Lawrence:bark08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models with {G}aussian Processes},
  venue =  	 {Bayesian Research Kitchen, Grasmere, Lake District, U.K.},
  linkpdf =	 shefftp # {lfm_bark08.pdf},
  linkvideo =	 {http://videolectures.net/bark08_lawrence_lfmwgp/},
  year =  	 2008, 
  month =  	 9,
  day =  	 6,
  abstract =   	 {We are used to dealing with the situation where we have 
                 a latent variable. Often we assume this latent
                 variable to be independently drawn from a
                 distribution, \emph{e.g.} probabilistic PCA or factor
                 analysis. This simplification is often extended for
                 temporal data where tractable Markovian independence
                 assumptions are used (\emph{e.g.} Kalman filters or
                 hidden Markov models).\\\\

                 In this talk we will consider the more general case
                 where the latent variable is a forcing function in a
                 differential equation model. We will show how for
                 some simple ordinary differential equations the
                 latent variable can be dealt with analytically for
                 particular Gaussian process priors over the latent
                 force. In this talk we will introduce the general
                 framework, present results in systems biology and
                 preview extensions.\\\\

                 Joint work with Magnus Rattray, Mauricio \'Alvarez, Pei
                 Gao, Antti Honkela, David Luengo, Guido Sanguinetti
                 and Michalis K. Titsias.},
  group =  	 {}
}


@Talk{Lawrence:warwick08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Statistical inference in systems biology through {G}aussian processes and ordinary differential equations},
  venue =  	 {LICSB Workshop, University of Warwick, U.K.},
  linkpdf =	 shefftp # {warwick.pdf},
  year =  	 2008, 
  month =  	 06,
  day =  	 17,
  abstract =   	 {In this talk we will summarise recent work from our group 
                 in Manchester on inferring `latent biochemical
                 species' in biological systems using Gaussian
                 processes and differential equations. A key problem
                 in biological data is when particular biochemical
                 species of interest are not directly measurable. We
                 will show how the framework of Gaussian processes can
                 be brought to bear on the problem and values of
                 latent chemical species can be inferred given data
                 and a differential equation model.},
  group =  	 {}
}

@Talk{Lawrence:gpbayes08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Learning and Inference with {G}aussian Processes: An Overview of {B}ayesian Inference and {G}aussian Processes},
  venue =  	 {Data Modelling Series, University of Sheffield, U.K.},
  linkpdf =	 shefftp # {gpAndBayes.pdf},
  year =  	 2008, 
  month =  	 04,
  day =  	 01,
  group =  	 {}
}

@Talk{Lawrence:human08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Human Motion Modelling through Dimensional Reduction with {G}aussian Processes},
  venue =  	 {Hotel Golf, Bled, Slovenia},
  linkpdf =	 shefftp # {human.pdf},
  year =  	 2008, 
  month =  	 01,
  day =  	 29,
  group =  	 {pascal}
}

@Talk{Lawrence:newton08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Human Motion Modelling with {G}aussian Processes},
  venue =  	 {Netwon Institute, Cambridge, U.K.},
  linkpdf =	 shefftp # {gplvm_newton.pdf},
  year =  	 2008, 
  month =  	 02,
  day =  	 07,
  group =  	 {pascal}
}

@Talk{Lawrence:thematic08,
  author =  	 {Neil D. Lawrence},
  title =  	 {{TP1}: Leveraging Complex Prior Knowledge in Learning},
  venue =  	 {Hotel Golf, Bled, Slovenia},
  linkpdf =	 shefftp # {thematic.pdf},
  year =  	 2008, 
  month =  	 01,
  day =  	 28,
  group =  	 {pascal}
}

@Talk{Lawrence:data08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Dimensionality Reduction},
  abstract = 	 {We approach dimensionality reduction from the 
                 perspective of multidimensional scaling. Starting
                 from the basics, we draw the relationship between
                 multidimensional scaling and principal component
                 analysis. From this background we briefly review
                 kernel PCA and Isomap. Finally, we consider the
                 problem of model selection using Gaussian
                 processes.},
  venue =  	 {EPSRC Winter School, University of Sheffield, Sheffield, U.K.},
  linkpdf =	 shefftp # {dataModellingWinter.pdf},
  linksoftware =	 softwarehttp # {dimred/},
  linkvideo =	 {http://videolectures.net/epsrcws08_lawrence_dr/},
  year =  	 2008, 
  month =  	 01,
  day =  	 24,
  group =  	 {spectral}
}

@Talk{Lawrence:msr07,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Variables, Differential Equations and {G}aussian Processes},
  abstract = 	 {We are used to dealing with the situation where we have a
                 latent variable. Often we assume this latent variable
                 to be independently drawn from a distribution,
                 e.g. probabilistic PCA or factor analysis. This
                 simplification is often extended for temporal data
                 where tractable Markovian independence assumptions
                 are used (e.g. Kalman filters or hidden Markov
                 models). In this talk we will consider such models in
                 the context of a biological problem: inferring
                 transcription factor activities in simple
                 transcription networks. We will extend the simpler
                 formalisms described above to consider the case where
                 the latent variable is a 'latent function' and the
                 relationship with the observed data is described by a
                 linear differential equation. Through the use of a
                 Gaussian process prior over the latent function we
                 can perform inference tractably and learn parameters
                 of interest in the system.},
  venue =  	 {Microsoft Research, Cambridge, U.K.},
  linkpdf =	 shefftp # {lvDeGp.pdf},
  label1 =	 {PUMA Project Page},
  link1 =	 {http://bioinf.man.ac.uk/resources/puma/},
  label2 =	 {TIGRE Project Page},
  link2 =	 {http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/projects/tigre/},
  label3 =	 {GPSIM Software},
  link3 =	 softwarehttp # {gpsim/},
  label4 =	 {Demos Software},
  link4 =	 softwarehttp # {oxford/},
  year =  	 2007, 
  month =  	 11,
  day =  	 12,
  group =  	 {gp,puma,gpsim}
}

@Talk{Lawrence:param07,
  author =  	 {Neil D. Lawrence},
  title =  	 {Modelling Transcriptional Regulation with {G}aussian Processes},
  abstract = 	 {},
  venue =  	 {Parameter Estimation Workshop, Manchester Interdisciplinary Biocentre, University of Manchester, U.K.},
  linkpdf =	 shefftp # {parameterEstimationTalk.pdf},
  label1 =	 {PUMA Project Page},
  link1 =	 {http://bioinf.man.ac.uk/resources/puma/},
  label2 =	 {TIGRE Project Page},
  link2 =	 {http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/projects/tigre/},
  label3 =	 {GPSIM Software},
  link3 =	 softwarehttp # {gpsim/},
  label4 =	 {Demos Software},
  link4 =	 softwarehttp # {oxford/},
  year =  	 2007, 
  month =  	 11,
  day =  	 7,
  group =  	 {puma,gpsim}
}
@Talk{Lawrence:mbb07,
  author =  	 {Neil D. Lawrence},
  title =  	 {Towards Computational Systems Biology with a Statistical Analysis Pipeline for Microarray Data},
  abstract = 	 {Since the human genome project began mathematical models 
                 have become an integral part of biological data analysis. 
                 The growth in data availability has necessitated their use 
                 in summarization of the data (e.g. \emph{statistical} 
                 approaches such as hierarchical clustering). Simultaneously, 
                 as more has become understood about the mechanisms 
                 underpinning particular pathways \emph{mechanistic} 
                 models of interactions have become more widespread.\\\\


                 The data-driven statistical approach and the mechanistic 
                 model approach each have their advantages. Data-driven models 
                 can be used in genome wide analyses to 'fish' for genes that 
                 were not known to be relevant but provide a critical role in 
                 a pathway. Mechanistic models make real predictions about how 
                 systems will respond given particular interventions. The two 
                 approaches have interacted only loosely, often not through 
                 interaction between the `mathematicians' but through indirect 
                 interaction via the biologists. \\\\

                 In this talk we will follow describe a statistical analysis 
                 `pipeline' for microarray data which handles the noise in 
                 the data. As we proceed down the pipeline we will come closer 
                 to mechanistic models of systems. We will finish with some 
                 general thoughts about the contribution that a combined 
                 statistical/mechanistic modelling approach can make.},
  venue =  	 {Department of Molecular Biology and Biotechnology, University of Sheffield, U.K.},
  linkpdf =	 shefftp # {puma_mbb.pdf},
  label1 =	 {PUMA Project Page},
  link1 =	 {http://bioinf.man.ac.uk/resources/puma/},
  label2 =	 {TIGRE Project Page},
  link2 =	 {http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/projects/tigre/},
  label3 = 	 {PUMA Software},
  link3 = 	 {http://bioconductor.org/packages/2.2/bioc/html/puma.html},
  year =  	 2007, 
  month =  	 10,
  day =  	 31,
  group =  	 {puma,gpsim}
}
@Talk{Lawrence:inverse07,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Variable Modelling with {G}aussian Processes},
  abstract = 	 {In this talk we will briefly describe the Gaussian process 
                 latent variable model, an approach to probabilistic
                 modelling of data through non-linear dimensional
                 reduction. The model takes a dual approach to
                 statistical inference and can be shown to generalise
                 PCA. We will briefly introduce the model and quickly
                 show some example applications.},
  venue =  	 {Workshop on Probabilistic formulation of the inverse problem and application to image reconstruction, Neuroscience Research Institute, University of Manchester, U.K.},
  linkpdf =	 shefftp # {gplvm_inverse_09_07.pdf},
  label1 =	 {Demos Software},
  link1 =	 softwarehttp # {oxford/},
  label2 =	 {Main Software},
  link2 =	 softwarehttp # {fgplvm/},
  year =  	 2007, 
  month =  	 9,
  day =  	 13,
  group =  	 {gplvm}
}

@Talk{Lawrence:uc3mivm07,
  author =  	 {Neil D. Lawrence},
  title =  	 {Fast Sparse {G}aussian Process Methods: The Informative Vector Machine},
  abstract = 	 {Gaussian processes are a non parametric approach to learning 
                 regression models. In this talk we will given a brief
                 review of the use of Gaussian processes for
                 regression. We will then introduce the informative
                 vector machine approach to learning Gaussian
                 processes for Classification on large scale data
                 sets. We will show extensions of the method including
                 multi-task learning, semi-supervised learning and
                 learning invariances.},
  venue =  	 {Dept of Signal Theory and Communications, Universidad Carlos III de Madrid, Spain},
  label1 =  	 {IVM Software},
  link1 =	 softwarehttp # {ivm/},
  label2 =  	 {IVM C++ Software},
  link2 =	 softwarehttp # {ivmcpp/},
  linkpdf =	 shefftp # {gpivm_07_07.pdf},
  year =  	 2007, 
  month =  	 7,
  day = 3
}
@Talk{Lawrence:uc3tfa07,
  author =  	 {Neil D. Lawrence},
  title =  	 {Probabilistic Inference for Modelling of Transcription Factor Activity},
  abstract = 	 {Accurate modelling of transcriptional processes in the 
                 cell requires the knowledge of a number of key
                 biological quantities. In practice many of them are
                 difficult to measure in vivo. For example, it is very
                 hard to measure the active concentration levels of
                 the transcription factor proteins that drive the
                 process.\\\\

                 In this talk we will show how, by making use of
                 structural information about the interaction network
                 (e.g. arising form ChIP-chip data), transcription
                 factor activities can estimated using probabilistic
                 inference. We propose two different probabilistic
                 models: a simple linear model with Kalman filter
                 based dynamics for genome/transcriptome wide studies
                 and a differential equation based Gaussian process
                 model with a more physically realistic
                 parameterisation for smaller interaction networks.
                 },
  venue =  	 {Dept of Signal Theory and Communications, Universidad Carlos III de Madrid, Spain},
  label1 =	 {GPSIM Software},
  link1 =	 softwarehttp # {gpsim/},
  label2 = 	 {TFA Software},
  link2 =	 softwarehttp # {chipdyno/},
  label3 = 	 {TFA Software II},
  link3 =	 softwarehttp # {chipvar/},
  label4 = 	 {PUMA Software},
  link4 = 	 {http://www.bioconductor.org/packages/2.0/bioc/html/puma.html},
  linkpdf =	 shefftp # {puma_07_07.pdf},
  year =  	 2007, 
  month =  	 7,
  day = 5
}


@Talk{Lawrence:uc3mgplvm07,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle4,
  abstract = 	 gplvmAbstract4,
  venue =  	 {Dept of Signal Theory and Communications, Universidad Carlos III de Madrid, Spain},
  linkpdf =	 shefftp # {gplvm_07_07.pdf},
  label1 =	 {Demos Software},
  link1 =	 softwarehttp # {oxford/},
  label2 =	 {Main Software},
  link2 =	 softwarehttp # {fgplvm/},
  year =  	 2007, 
  month =  	 7,
  day = 4,
  group =  	 {gplvm}
}

@Talk{Lawrence:mathbio07,
  author =  	 {Neil D. Lawrence},
  title =  	 {{G}aussian Processes for Inference in Biological Interaction Networks},
  abstract = 	 {In many biological applications key functions of interest, 
                 such as chemical species concentrations, are
                 unobserved.  In this talk we will briefly introduce
                 Gaussian processes, which are probabilistic models of
                 functions. We will show how they can be used, in
                 combination with a simple differential equation
                 model, to estimate the concentration of a
                 transcription factor in a simple single input module
                 network motif.},
  venue =  	 {Exploring the Interface Between Mathematics and Bioscience, Manchester Interdisciplinary Biocentre, University of Manchester, U.K.},
  linksoftware =	 softwarehttp # {gpsim/},
  linkpdf =	 shefftp # {gpsim_07_04.pdf},
  year =  	 2007, 
  month =  	 4,
  day = 4
}
@Talk{Lawrence:pesb07,
  author =  	 {Neil D. Lawrence},
  title =  	 {Modelling Transcriptional Regulation with {G}aussian Processes},
  abstract = 	 {Modelling the dynamics of transcriptional processes in the 
                 cell requires the knowledge of a number of key
                 biological quantities. While some of them are
                 relatively easy to measure, such as mRNA decay rates
                 and mRNA abundance levels, it is still very hard to
                 measure the active concentration levels of the
                 transcription factor proteins that drive the process
                 and the sensitivity of target genes to these
                 concentrations. In this paper we show how these
                 quantities for a given transcription factor can be
                 inferred from gene expression levels of a set of
                 known target genes. We treat the protein
                 concentration as a latent function with a Gaussian
                 process prior, and include the sensitivities, mRNA
                 decay rates and baseline expression levels as
                 hyperparameters. We apply this procedure to a human
                 leukemia dataset, focusing on the tumour repressor
                 p53 and obtaining results in good accordance with
                 recent biological studies.},
  venue =  	 {Parameter Estimation in Systems Biology, School of Computer Science, University of Manchester, U.K.},
  linksoftware =	 softwarehttp # {gpsim/},
  linkpdf =	 shefftp # {gpsim_07_03.pdf},
  linkvideo =	 {mms://velblod2.ijs.si/pascal/2007/pesb07_manchester/lawrence_neil/lawrence_neil_00.wmv},
  year =  	 2007, 
  month =  	 3,
  day =   	 28
}

@Talk{Lawrence:icml07,
  author =  	 {Neil D. Lawrence},
  title =  	 {Hierarchical {G}aussian Process Latent Variable Models},
  abstract = 	 {The Gaussian process latent variable model (GP-LVM) is a 
                 powerful approach for probabilistic modelling of high
                 dimensional data through dimensional reduction. In
                 this paper we extend the GP-LVM through
                 hierarchies. A hierarchical model (such as a tree)
                 allows us to express conditional independencies in
                 the data as well as the manifold structure. We first
                 introduce Gaussian process hierarchies through a
                 simple dynamical model, we then extend the approach
                 to a more complex hierarchy which is applied to the
                 visualisation of human motion data sets.},
  venue =  	 {ICML, Corvallis, Oregon},
  linkpdf =	 shefftp # {hgplvm_07_06.pdf},
  label1 =	 {Demos Software},
  link1 =	 softwarehttp # {oxford/},
  label2 =	 {Main Software},
  link2 =	 softwarehttp # {hgplvm/},
  year =  	 2007, 
  month =  	 6,
  day = 22,
  group =  	 {gplvm}
}
@Talk{Lawrence:ncrg07,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle4,
  abstract = 	 gplvmAbstract4,
  venue =  	 {Neural Computing Research Group, Aston University, U.K.},
  linkpdf =	 shefftp # {gplvm_07_02.pdf},
  label1 =	 {Demos Software},
  link1 =	 softwarehttp # {oxford/},
  label2 =	 {Main Software},
  link2 =	 softwarehttp # {fgplvm/},
  year =  	 2007, 
  month =  	 3, 
  day =          9,
  group =  	 {gplvm}
}
@Talk{Lawrence:google07,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle4,
  abstract = 	 gplvmAbstract4,
  venue =  	 {Google Research, New York, N.Y., U.S.A.},
  linkvideo =  	 {http://video.google.com/videoplay?docid=-5127068978792458641},
  linkpdf =	 shefftp # {gplvm_07_02.pdf},
  label1 =	 {Demos Software},
  link1 =	 softwarehttp # {oxford/},
  label2 =	 {Main Software},
  link2 =	 softwarehttp # {fgplvm/},
  year =  	 2007, 
  month =  	 2, 
  day = 12, 
  group =  	 {gplvm}
}
@Talk{Lawrence:csail07,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle4,
  abstract = 	 gplvmAbstract4,
  venue =  	 {Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Boston, MA, U.S.A.},
  linkpdf =	 shefftp # {gplvm_07_02.pdf},
  label1 =	 {Demos Software},
  link1 =	 softwarehttp # {oxford/},
  label2 =	 {Main Software},
  link2 =	 softwarehttp # {fgplvm/},
  label3 =	 {Seminar Page},
  link3 =	 {http://www.eecs.mit.edu/cgi-bin/calendar.cgi?page=AY06-07/data/106.dat},
  year =  	 2007, 
  month =  	 2, 
  day = 9,
  group =  	 {gplvm}
}
@Talk{Lawrence:manchesterGuest06,
  author =  	 {Neil D. Lawrence},
  title =  	 {Learning and Inference with {G}aussian Processes: An Overview of {G}aussian Processes and the {GP-LVM}},
  venue =  	 {University of Manchester, Machine Learning Course Guest Lecture},
  label1 =	 {Demos Software},
  link1 =        softwarehttp # {oxford/},
  label2 =	 {Main Software},
  link2 =        softwarehttp # {fgplvm/},
  linkpdf =	 shefftp # {gpGuestLecture.pdf},
  year =  	 2006, 
  month =  	 11, 
  day = 3,
  group =  	 {gplvm}
}
@Talk{Lawrence:latentFunc08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Inferring Latent Functions with {G}aussian Processes in Differential Equations},
  venue =  	 {Department of Electronics and Computer Science, University of Southampton, U.K.},
  linkpdf =	 shefftp # {gpSysBio.pdf},
  label1 = 	 {Seminar Page},
  link1 = 	 {http://www.isis.ecs.soton.ac.uk/seminars/?date=20080430},
  year =  	 2008, 
  month =  	 4,
  day =   	 30,
  abstract =   	 {In this talk we will present recent work from Manchester in
                 inference of latent functions in differential
                 equations.  Simple computational models for systems
                 biology make use of ordinary differential equations
                 that are driven from an often unobserved input
                 function. We will describe how probabilistic
                 inference over these latent functions may be
                 performed through Gaussian process prior
                 distributions. We will describe the algorithms and
                 show results on toy problems and real biological
                 systems.},
  group =  	 {gp}
}
@Talk{Lawrence:sysbioIntroA08,
  author =  	 {Neil D. Lawrence},
  title =  	 {An Introduction to Systems Biology from a Machine Learning Perspective},
  venue =  	 {Max Planck Society, Ringberg Castle, Germany},
  linkpdf =	 shefftp # {mpi.pdf},
  OPTlabel1 = 	 {},
  OPTlink1 = 	 {},
  year =  	 2008, 
  month =  	 5,
  day =   	 5,
  abstract =   	 {In this talk we will introduce some of the challenges in 
                 systems biology and discuss the efforts being made to
                 address them using statistical inference. General
                 biological background will be interlaced with case
                 studies that illustrate the salient issues in systems
                 biology from the perspective of a machine learning
                 researcher.},
  group =  	 {sysbio}
}
@Talk{Lawrence:sysbioIntroB08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Statistical Inference in Systems Biology through {G}aussian Processes and Ordinary Differential Equations},
  venue =  	 {Max Planck Society, Ringberg Castle, Germany},
  linkpdf =	 shefftp # {mpi2.pdf},
  OPTlabel1 = 	 {},
  OPTlink1 = 	 {},
  year =  	 2008, 
  month =  	 5,
  day =   	 7,
  abstract =   	 {In this talk we will summarise recent work from our group 
                 in Manchester on inferring `latent biochemical
                 species' in biological systems using Gaussian
                 processes and differential equations. A key problem
                 in biological data is when particular biochemical
                 species of interest are not directly measurable. We
                 will show how the framework of Gaussian processes can
                 be brought to bear on the problem and values of
                 latent chemical species can be inferred given data
                 and a differential equation model.},
  group =  	 {gp,sysbio}
}

@Talk{Lawrence:tuebingen06,
  author =  	 {Neil D. Lawrence},
  title =  	 pumaTitle1,
  abstract = 	 pumaAbstract1,
  venue =  	 {Max Planck Institute, T\"ubingen, Germany},
  label1 =	 {PUMA Project Page},
  link1 =	 {http://bioinf.man.ac.uk/resources/puma/},
  label2 =	 {NPPCA Software},
  link2 =	 softwarehttp # {nppca/},
  label3 =	 {ChipDyno Software},
  link3 =	 softwarehttp # {chipdyno/},
  label4 =	 {ChipVar Software},
  link4 =	 softwarehttp # {chipvar/},
  label5 =	 {GP p53 Software},
  link5 =	 softwarehttp # {gpsim/},
  label6 = 	 {PUMA Software},
  link6 = 	 {http://www.bioconductor.org/packages/2.0/bioc/html/puma.html},
  linkpdf =	 shefftp # {puma_06_08.pdf},
  year =  	 2006, 
  month =  	 8,
  day = 2
}
@Talk{Lawrence:intel06,
  author =  	 {Neil D. Lawrence},
  title =  	 gpTitle1,
  abstract = 	 gpAbstract1,
  venue =  	 {Intel Research, Seattle, U.S.A.},
  label1 =	 {p53 Software},
  link1 =	 softwarehttp # {gpsim/},
  linkpdf =	 shefftp # {gpintroTalk_06_08.pdf},
  linksoftware =	 softwarehttp # {oxford/},
  year =  	 2006, 
  month =  	 8, 
  day = 21,
}

@Talk{Lawrence:erice06,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle4,
  abstract = 	 gplvmAbstract4,
  venue =  	 {Erice Workshop on Mathematics and Medical Diagnosis, Sicily, Italy},
  linkpdf =	 shefftp # {gplvm_06_07.pdf},
  label1 =	 {Demos Software},
  link1 =	 softwarehttp # {oxford/},
  label2 =	 {Main Software},
  link2 =	 softwarehttp # {fgplvm/},
  year =  	 2006, 
  month =  	 7,
  day = 11,
  group =  	 {gplvm}
}

  
@Talk{Lawrence:manchester06,
  author =  	 {Neil D. Lawrence},
  title =  	 gpTitle1,
  abstract = 	 gpAbstract1,

  venue =  	 {School of Computer Science, University of Manchester, U.K.},
  label1 =	 {p53 Software},
  link1 =	 softwarehttp # {gpsim/},
  linkpdf =	 shefftp # {gpintroTalk_06_06.pdf},
  linksoftware =	 softwarehttp # {oxford/},
  year =  	 2006, 
  month =  	 6,
  day = 22
}
@Talk{Lawrence:icml06,
  author =  	 {Neil D. Lawrence},
  title =  	 {Local Distance Preservation in the GP-LVM through Back Constraints},
  venue =  	 {International Conference on Machine Learning, Pittsburgh, U.S.A.},
  linkpdf =	 shefftp # {backConstraintsBeamer.pdf},
  linksoftware =	 softwarehttp # {fgplvm/},
  year =  	 2006, 
  month =  	 6,
  day = 27,
  group =  	 {gplvm}
}
@Talk{Sanguinetti:masamb06,
  author =  	 {Guido Sanguinetti},
  title =  	 {A probabilistic dynamical model for quantitative inference of the regulatory mechanism of transcription},
  venue =  	 {Mathematical and Statistical Aspects of Molecular Biology},
  linkpptgz =	 shefftp # {MASAMB06.ppt.gz},
  label1 =	 {Model One Software},
  link1 =	 softwarehttp # {chipdyno/},
  label2 =	 {Model Two  Software},
  link2 =	 softwarehttp # {chipchip/},
  year =  	 2006,
  group =	 {shefml,puma,gene networks}, 
  month =  	 4,
  day = 10
}
@Talk{Lawrence:cued06,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle4,
  abstract = 	 gplvmAbstract4,
  venue =  	 {Information Engineering, University of Cambridge, U.K.},
  linkpptgz =	 shefftp # {gplvm_06_03.ppt.gz},
  label1 =	 {Demos Software},
  link1 =	 softwarehttp # {oxford/},
  label2 =	 {Main Software},
  link2 =	 softwarehttp # {fgplvm/},
  year =  	 2006, 
  month =  	 3,
  day = 7,
  group =  	 {gplvm}
}
@Talk{Lawrence:oxford06,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle3,
  abstract = 	 gplvmAbstract3,
  venue =  	 {Computer Vision Reading Group, Visual Geometry Group, Department of Engineering Science, University of Oxford, U.K.},
  label1 =	 {PDF Slides},
  link1 =	 shefftp # {gplvmTutorialSlides.pdf},
  label2 =	 {PDF Notes},
  link2 =	 shefftp # {gplvmTutorial.pdf},
  label3 =	 {Demos Software},
  link3 =        softwarehttp # {oxford/},
  label4 =	 {Main Software},
  link4 =        softwarehttp # {fgplvm/},
  year =  	 2006, 
  month =  	 1,
  day = 27,
  group =  	 {gplvm}
}
@Talk{Lawrence::uw05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle2,
  abstract = 	 gplvmAbstract2,
  venue =  	 {University of Washington, Seattle, U.S.A.},
  linkpptgz =	 shefftp # {gplvm_05_11.ppt.gz},
  year =  	 2005, 
  month = 12,
  day = 14,
  group =  	 {gplvm}
}
@Talk{Lawrence::msrred05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle2,
  abstract = 	 gplvmAbstract2,
  venue =  	 {Microsoft Research, Redmond, U.S.A.},
  linkpptgz =	 shefftp # {gplvm_05_11.ppt.gz},
  year =  	 2005, 
  month =  	 12,
  day = 12,
  group =  	 {gplvm}
}
@Talk{Lawrence::ubc05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle2,
  abstract = 	 gplvmAbstract2,
  venue =  	 {Electronic Arts Speaker Series, University of British Columbia, Canada},
  linkpptgz =	 shefftp # {gplvm_05_11.ppt.gz},
  year =  	 2005, 
  month =  	 12,
  day = 2,
  group =  	 {gplvm}
}
@Talk{Lawrence::columbia05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle2,
  abstract = 	 gplvmAbstract2,
  venue =  	 {Columbia University, New York, U.S.A.},
  linkpptgz =	 shefftp # {gplvm_05_11.ppt.gz},
  year =  	 2005, 
  month =  	 11,
  day = 29,
  group =  	 {gplvm}
}
@Talk{Lawrence::ibm05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle2,
  abstract = 	 gplvmAbstract2,
  venue =  	 {IBM Thomas J Watson Research Center, New York, U.S.A.},
  linkpptgz =	 shefftp # {gplvm_05_11.ppt.gz},
  year =  	 2005, 
  month =  	 11,
  day = 28,
  group =  	 {gplvm}
}

@Talk{Lawrence:gatsby05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle2,
  abstract = 	 gplvmAbstract2,
  venue =  	 {Gatsby Computational Neuroscience Unit, University College London, U.K.},
  linkpptgz =	 shefftp # {gplvm_gatsby.ppt.gz},
  year =  	 2005, 
  month =  	 11,
  day = 16,
  group =  	 {gplvm}
}


@Talk{Lawrence:idiap05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle2,
  abstract = 	 gplvmAbstract2,
  venue =  	 {IDIAP Research Institute, Martigny, Switzerland},
  linkpptgz =	 shefftp # {gplvm_epfl.ppt.gz},
  year =  	 2005, 
  month =  	 11,
  day = 2,
  group =  	 {gplvm}
}
@Talk{Lawrence:epfl05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle2,
  abstract = 	 gplvmAbstract2,
  venue =  	 {School of Computer and Communication Sciences, Swiss Federal Institute of 
                 Technology (EPFL), Lausanne, Switzerland},
  linkpptgz =	 shefftp # {gplvm_epfl.ppt.gz},
  year =  	 2005, 
  month =  	 10,
  day = 31,
  group =  	 {gplvm}
}
@Talk{Lawrence:manchester05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle1,
  abstract = 	 gplvmAbstract1,

  venue =  	 {School of Computer Science, University of Manchester, U.K.},
  linkpptgz =	 shefftp # {long_gplvm.ppt.gz},
  year =  	 2005, 
  month =  	 3,
  day = 9,
  group =  	 {gplvm}
}
@Talk{Lawrence:soton05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle1,
  abstract = 	 gplvmAbstract1,
  venue =  	 {Department of Electronics and Computer Science, University of Southampton, U.K.},
  linkpptgz =	 shefftp # {long_gplvm.ppt.gz},
  label1 = 	 {Seminar Page},
  link1 = 	 {http://www.isis.ecs.soton.ac.uk/seminars/?date=20050511},
  year =  	 2005, 
  month =  	 5,
  day = 11,
  group =  	 {gplvm}
}
@Talk{Lawrence:msr05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle1,
  abstract = 	 gplvmAbstract1,
  venue =  	 {Microsoft Research, Cambridge, U.K.},
  linkpptgz =	 shefftp # {long_gplvm.ppt.gz},
  year =  	 2005, 
  month =  	 3,
  day = 15,
  group =  	 {gplvm}

}
@Talk{Lawrence:edinburgh05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle1,
  abstract = 	 gplvmAbstract1,
  venue =  	 {Institute for Adaptive and Neural Computation, University of Ediburgh, U.K.},
  linkpptgz =	 shefftp # {long_gplvm.ppt.gz},
  year =  	 2005, 
  month =  	 3,
  day = 1,
  group =  	 {gplvm}

}
@Talk{Lawrence:oxford05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle1,
  abstract = 	 gplvmAbstract1,
  venue =  	 {Robotics Research Group, Department of Engineering Science, University of Oxford, U.K.},
  linkpptgz = 	 shefftp # {long_gplvm.ppt.gz},
  year =  	 2005, 
  month =  	 2,
  day = 21,
  group =  	 {gplvm}
}
@Talk{Lawrence:tuebingen05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle1,
  venue =  	 {Max Planck Institute, T\"ubingen, Germany},
  linkpptgz = 	 shefftp # {gplvm.ppt.gz},
  year =  	 2005, 
  month =  	 8,
  day = 15,
  group =  	 {gplvm}
}

@Talk{Lawrence:msrb03,
  author =  	 {Neil D. Lawrence},
  title =  	 {Bayesian Processing of {cDNA} Microarray Images through 
                 the Variational Importance Sampler},
  abstract =  	 {Each cell in the human body contains the same basic code 
                 in the form of the genome, however cells have differentiated 
                 roles which come about through different cells `expressing' 
                 different genes. 
                 
                 Key insights into gene interactions can be studied through 
                 measuring the level of expression of each gene at different 
                 times. Gene expression levels can be obtained from cDNA 
                 microarray experiments through the extraction of pixel
                 intensities from a scanned image of a slide. 

                 In this talk we will start by briefly reviewing cDNA
                 microarray technology. We will then focus on one
                 problem that arises when processing these images:
                 human error in locating the position of the spots can
                 lead to variabilities in the extracted expression
                 levels. We will present a Bayesian approach to the image
                 processing which alleviates this problem. Our
                 approach makes use of a novel combination of
                 importance sampling and variational approximations.
 
                 Finally if there is time we will briefly show some examples 
                 of the variational importance sampler applied to visual 
                 tracking problems.},
  venue =  	 {Microsoft Research, Redmond, U.S.A.},
  year =  	 {2003}, 
  month =  	 12,
  day = 4
}

@Talk{Lawrence:ucbgplvm03,
  author =  	 {Neil D. Lawrence},
  title =  	 {Probabilistic Non-linear Component Analysis through {G}aussian
                 Process Latent Variable Models},
  abstract =  	 {It is known that Principal Component Analysis has an
                 underlying probabilistic representation based on a
                 latent variable model. PCA is recovered when the
                 latent variables are integrated out and the
                 parameters of the model are optimised by maximum
                 likelihood. It is less well known that the dual
                 approach of integrating out the parameters and
                 optimising with respect to the latent variables also
                 leads to PCA. The marginalised likelihood in this
                 case takes the form of Gaussian process mappings,
                 with linear Covariance functions, from a latent space
                 to an observed space, which we refer to as a Gaussian
                 Process Latent Variable Model (GPLVM) \cite{Lawrence:gplvm03} . It is
                 straightforward to `non-linearise' this model by
                 substituting the linear covariance function for a
                 non-linear one. The result is a non-linear
                 probabilistic PCA model.

                 In this talk we will present a practical algorithm
                 for optimising the latent variables in a non-linear
                 GPLVM and discuss some relations with other
                 models. Finally we will present results from a
                 SIGGRAPH paper which uses the GPLVM to learn styles
                 in an inverse kinematics problem \cite{Grochow:styleik04}.},
  venue =  	 {University of California, Berkeley, U.S.A.},
  year =  	 {2004}, 
  month =  	 5,
  day = 6
}

@Talk{Lawrence:smlwgplvm03,
  author =  	 {Neil D. Lawrence},
  title =  	 {Probabilistic Non-linear Component Analysis through {G}aussian
                 Process Latent Variable Models},
  abstract =  	 {It is known that Principal Component Analysis has an
                 underlying probabilistic representation based on a
                 latent variable model. Principal component analysis
                 (PCA) is recovered when the latent variables are
                 integrated out and the parameters of the model are
                 optimised by maximum likelihood. It is less well
                 known that the dual approach of integrating out the
                 parameters and optimising with respect to the latent
                 variables also leads to PCA. The marginalised
                 likelihood in this case takes the form of Gaussian
                 process mappings, with linear Covariance functions,
                 from a latent space to an observed space, which we
                 refer to as a Gaussian Process Latent Variable Model
                 (GPLVM). This dual probabilistic PCA is still a
                 linear latent variable model, but by looking beyond
                 the inner product kernel as a for a covariance
                 function we can develop a non-linear probabilistic
                 PCA.},
  venue =  	 {Sheffield Machine Learning Workshop, Sheffield, U.K.},
  linkvideo =  	 {mms://velblod2.ijs.si/pascal/2004/sheffield_04/lawrence_neil/lawrence_neil_00.wmv},
  year =  	 {2004}, 
  month =  	 9, 
  day = 9,
  linkpptgz =   	 shefftp # {gplvm_smlw.ppt.gz}
}
@Talk{Lawrence:sussex03,
  author =  	 {Neil D. Lawrence},
  title =  	 {Bayesian Processing of {cDNA} Microarray Images},
  abstract =  	 {Gene expression levels are obtained from microarray 
                 experiments through the extraction of pixel
                 intensities from a scanned image of the slide. It is
                 widely acknowledged that variabilities can occur in
                 expression levels extracted from the same images by
                 different users with the same software
                 packages. These inconsistencies arise due to
                 differences in the refinement of the placement of the
                 microarray 'grids'. We introduce a novel automated
                 approach to the refinement of grid placements that is
                 based upon the use of Bayesian inference for
                 determining the size, shape and positioning of the
                 microarray 'spots', capturing uncertainty that can be
                 passed to downstream analysis.

                 Our experiments demonstrate that variability between
                 users can be significantly reduced using the
                 approach. The automated nature of the approach also
                 saves hours of researchers' time normally spent in
                 refining the grid placement.},
  venue =  	 {The University of Sussex, Department of Cognitive Science, 
                 Bioinformatics and Vision Seminars},
  year =  	 {2003}, 
  month =  	 6,
  day = 20
}
@Talk{Lawrence:manchester03,
  author =  	 {Neil D. Lawrence},
  title =  	 {Bayesian Processing of {cDNA} Microarray Images},
  abstract =  	 {Gene expression levels are obtained from microarray 
                 
                 experiments through the extraction of pixel
                 intensities from a scanned image of the slide. It is
                 widely acknowledged that variabilities can occur in
                 expression levels extracted from the same images by
                 different users with the same software
                 packages. These inconsistencies arise due to
                 differences in the refinement of the placement of the
                 microarray `grids'. We introduce a novel automated
                 approach to the refinement of grid placements that is
                 based upon the use of Bayesian inference for
                 determining the size, shape and positioning of the
                 microarray `spots', capturing uncertainty that can be
                 passed to downstream analysis.

                 Our experiments demonstrate that variability between
                 users can be significantly reduced using the
                 approach. The automated nature of the approach also
                 saves hours of researchers' time normally spent in
                 refining the grid placement.

                 A MATLAB implementation of the algorithm is available
                 for non-commercial use from
                 \url{http://www.dcs.shef.ac.uk/\~{ }neil/vis}.},
  venue =  	 {The University of Manchester, Department of Computer Science,
                 Bio-health sciences Seminars},
  year =  	 {2003}, 
  month =  	 5,
  day = 21
}
@Talk{Lawrence:gatsby07,
  author =  	 {Neil D. Lawrence},
  title =  	 {Probabilistic Inference for Modelling of Transcription Factor Activity},
  link1 =   	 {http://bioinf.manchester.ac.uk/resources/puma/},
  label1 =   	 {Project Page},
  label2 =	 {NPPCA Software},
  link2 =	 softwarehttp # {nppca/},
  label3 =	 {ChipDyno Software},
  link3 =	 softwarehttp # {chipdyno/},
  label4 =	 {ChipVar Software},
  link4 =	 softwarehttp # {chipvar/},
  label5 =	 {GP p53 Software},
  link5 =	 softwarehttp # {gpsim/},
  label6 = 	 {PUMA Software},
  link6 = 	 {http://www.bioconductor.org/packages/2.0/bioc/html/puma.html},
  linkpdf =	 shefftp # {puma_07_06.pdf},
  abstract =  	 {Accurate modelling of transcriptional processes in the 
                 cell requires the knowledge of a number of key
                 biological quantities. In practice many of them are
                 difficult to measure in vivo. For example, it is very
                 hard to measure the active concentration levels of
                 the transcription factor proteins that drive the
                 process.\\\\

                 In this talk we will show how, by making use of
                 structural information about the interaction network
                 (e.g. arising form ChIP-chip data), transcription
                 factor activities can estimated using probabilistic
                 inference. We propose two different probabilistic
                 models: a simple linear model with Kalman filter
                 based dynamics for genome/transcriptome wide studies
                 and a differential equation based Gaussian process
                 model with a more physically realistic
                 parameterisation for smaller interaction networks.},
  venue =  	 {Gatsby Computational Neuroscience Unit, University College London, U.K.},
  year =  	 {2007}, 
  month =  	 6,
  day = 13
}
@InCollection{Lawrence:gpncnm05,
  author =	 {Neil D. Lawrence and Michael I. Jordan},
  title =	 {Gaussian Processes and the Null-Category Noise
                  Model},
  crossref =	 {Chapelle:semisuper06},
  abstract =	 {With Gaussian process classifiers (GPC) we aim to
                  predict the posterior probability of the class
                  labels given an input data point, $p(y_i|x_i)$. In
                  general we find that this posterior distribution is
                  unaffected by unlabeled data points during
                  learning. Support vector machines are strongly
                  related to GPCs, but one notable difference is that
                  the decision boundary in an SVM can be influenced by
                  unlabeled data. The source of this discrepancy is
                  the SVM's margin: a characteristic which is not
                  shared with the GPC. The presence of the marchin
                  allows the support vector machine to seek low data
                  density regions for the decision boundary,
                  effectively allowing it to incorporate the cluster
                  assumption (see Chapter 6). In this chapter we
                  present the \emph{null category noise model}. A
                  probabilistic equivalent of the margin. By combining
                  this noise model with a GPC we are able to
                  incorporated the cluster assumption without
                  explicitly modeling the input data density
                  distributions and without a special choice of
                  kernel.},
  pages =	 {152--165},
  linkpsgz =	 shefftp # {gpncnm.ps},
  label1 =	 {MATLAB Software},
  link1 =	 softwarehttp # {ncnm/},
  label2 =	 {C++ Software},
  link2 =	 softwarehttp # {ivmcpp/}
}

@InProceedings{Hifny:maxent05,
  author =	 {Yasser Hifny and Steve Renals and Neil D. Lawrence},
  title =	 {A Hybrid {MaxEnt/HMM} Based {ASR} System},
  booktitle =	 {Proceedings of Interspeech 2005 --- 9th European
                  Conference on Speech Communication and Technology},
  OPTpages =	 {},
  year =	 2005,
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  month =	 9,
  organization = {Lisbon, Portugal},
  abstract =	 {The aim of this work is to develop a practical
                  framework, which extends the classical Hidden Markov
                  Model (HMM) for continuous speech recognition based
                  on the Maximum Entropy (MaxEnt) principle. The
                  MaxEnt models can estimate the posterior
                  probabilities directly as with Hybrid NN/HMM
                  connectionist speech recogniton systems. In
                  particular, a new acoustic modelling based on
                  discriminative MaxEnt models is formulated and is
                  being developed to replace the generative Gaussian
                  Mixture Models (GMM) commonly used to model acoustic
                  variability. Initial experimental results using the
                  TIMIT phone task are reported.},
  linkpdf =	 shefftp # {hifny-eurospeech05.pdf}
}

@Article{Tipping:variational05,
  author =	 {Michael E. Tipping and Neil D. Lawrence},
  title =	 {Variational inference for {S}tudent-$t$ models:
                  Robust {B}ayesian interpolation and generalised
                  component analysis},
  journal =	 {Neurocomputing},
  year =	 2005,
  OPTkey =	 {},
  volume =	 69,
  OPTnumber =	 {},
  pages =	 {123--141},
  abstract =	 {We demonstrate how a variational approximation
                  scheme enables effective inference of key parameters
                  in probabilisitic signal models which employ the
                  Student-t distribution. Using the two scenarios of
                  previous termrobustnext term interpolation and
                  independent component analysis (ICA) as examples, we
                  illustrate the key feature of the approach: that the
                  form of the noise distribution in the interpolation
                  case, and the source distributions in the ICA case,
                  can be inferred from the data concurrent with all
                  other model parameters.},
  link1 =
                  {http://authors.elsevier.com/sd/article/S0925231205001694},
  label1 =	 {Access via Science Direct},
  OPTlink2 =	 {},
  OPTlabel2 =	 {}
}

@Talk{Lawrence:msr03,
  author =  	 {Neil D. Lawrence},
  title =  	 {Particle Filters, Variational methods and Importance Sampling},
  abstract =  	 {Particle filters allow tracking of systems with highly 
                 non-linear, multi-modal posterior distributions,
                 however they are prone to failure when model
                 likelihoods are sharply peaked or state spaces are
                 high dimensional. This failure is caused by a
                 mismatch between the proposal distribution and the
                 true posterior. The number of particles of samples
                 then required to accurately represent the posterior
                 increases dramatically and with it the computational
                 demands of the algorithm.

                 By formulating the problem within the framework of
                 variational inference we derive an algorithm in which
                 the proposal naturally adapts to more accurately
                 reflect the true posterior. This is achieved by
                 replacing intractable moment evaluations, arising
                 from the highly non-linear nature of the likelihood
                 functions, with sample based approximations.

                 In this talk we shall first introduce the approach in
                 a static setting: Bayesian processing of cDNA
                 microarray images.  We will then add dynamics to the
                 model and demonstrate a marked improvement over
                 standard approaches on both synthetic and real-world
                 tracking examples.},
  venue =  	 {Machine Learning and Perception Group, Microsoft Research, 
                 Cambridge, U.K.},
  year =  	 {2003}, 
  month =  	 3,
  day = 24
}
@TechReport{King:ppa05,
  author =	 {Nathaniel J. King and Neil D. Lawrence},
  title =	 {Variational Inference in {G}aussian Processes via
                  Probabilistic Point Assimilation},
  institution =	 sheftech,
  year =	 2005,
  number =	 {CS-05-06},
  linkpsgz =	 shefftp # {ppatech.ps.gz},
  link2 =	 softwarehttp # {kpca/},
  label2 =	 {Software},
  group =	 {shefml},
  abstract =	 {We introduce a novel variational approach
                  for approximate inference in Gaussian process (GP)
                  models. The key advantages of our approach are the
                  ease with which different noise models can be
                  incorporated and improved speed of convergence. We
                  refer to the algorithm as probabilistic point
                  assimilation (PPA). We introduce the algorithm
                  firstly using the `weight space' view and then
                  through its Gaussian process formulation. We
                  illustrate the approach on several benchmark data
                  sets.}
}
@PhdProject{Animation:10,
  author = {Neil D. Lawrence},
  title = {Animation by Machine Learning with Motion Capture Data},
  abstract = {This project is about using Machine Learning to model human motion for animation, with a particular focus on the demands of computer games. The idea is to learn what natural motion looks like, and then combine it with constraints to develop an animation sequence. The constraints could be animator imposed, or imposed by the computer game. A typical scenario might be that the player's character is required to interact with a character in the game, for example a player might be given an object in the game. The constraint could be that the hands of the player touch the hands of the character giving the object. With the current approach to animation (looking up a library of motion capture data) such a constraint is very difficult to fulfill as the required motion won't exactly match a sequence in the library. By modelling natural motion through machine learning, we should be able to generate a new sequence to satisfy the constraint.\\\\ 

The models of motion will be developed using Gaussian processes. In particular the project will make use of the "Gaussian Process Latent Variable Model" (Lawrence, 2003) which has already shown a lot of promise in this domain, and "Latent Force Models" (Alvarez et al, 2009) a recently developed approach to learning based on physics and probabilistic models. \\\\

The project will involve a large amount of mathematics, in particular advanced linear algebra and calculus.}
}

@PhdProject{Mechanistic:10,
  author = {Neil D. Lawrence},
  title = {Data Driven Mechanistic Models for Systems Biology},
  abstract = {Models of biological systems, such as gene networks, are useful in 
extracting meaning from quantitative data obtained from specific biological systems. In this PhD proposal we are interested in how things actually work in real observable biological systems. To do this we will make use of machine learning to infer mechanistic models biological systems. \\\\

Biological systems are immensely complex, and extracting all the data necessary to characterize the system is often impossible. It is therefore important to exploit other sources of information when modelling biological systems. Once such source of information is "mechanistic models". These are models of the underlying physical properties of the system. In this project we will ensure that such physical models can be easily combined with data driven machine learning approaches, aiming to obtain the best of both worlds: mechanistic modelling and data driven machine learning models. \\\\

The project will involve a large amount of mathematics, in particular advanced linear algebra and calculus.}
}

@PhdProject{Disease:10,
  author = {Neil D. Lawrence},
  year = 2010,
  title = {Inferring Complex Hidden Causes of Disease through Probabilistic Models},
  abstract = {The increased ability to measure individuals' genetic profiles ( through single nucleotide polymorphisms), combined with the ability to characterize a disease activity through gene expression, and other biomarkers, should reveal more realistically complex webs of causal factors for disease. Understanding these causal factors would enable personalised interventions targeted to an individual's genetic, environmental (including concurrent treatments) and treatment preference profile. \\\\

This vision is challenging on two fronts: first, integration of different sources of (genomic) biological data is not straightforward; second, environments are not artificially controlled. In practice, disease occurrence and progression is often triggered by a combination of genetic and environmental factors. Environmental factors need to be assimilated with the genetic background (through the genomic data) and placed in a unified modelling framework to characterize the disease. \\\\

The objective of this project is to address these issues. Our aim is to perform statistical inference from these models to enable us to resolve the determinants of a given disease and its responses to treatments. The research will involve amalgamation of several different research areas, covering health, biology, computational and statistical inference. \\\\

The project will involve a large amount of mathematics, in particular probability theory and advanced linear algebra.}
}

@BscProject{Rpackage:11,
  author = {Neil D. Lawrence},
  projectNumber = {NDL-1},  
  year = 2011,
  title = {Transcription Factor Inference in Bioconductor},
  abstract = {Gene transcription networks underpin the operation of all living cells. The genome is the same in each cell, but which genes are switched on (or expressed) is determined by transcription factors. Unfortunately these transcription factors (which are in the form of proteins) are difficult to measure directly. However, microarray technology (and recently 3rd generation sequencing technology) allow us to make measurements of the gene expression across the genome. We can make indirect inference about the transcription factor activities from this information. In this project software for making these inferences from real data will be developed within the Bioconductor framework using the R programming language, allowing biologists to infer transcription factor activities across the genome.

\\\\This project will suit students with strong analytical skills, there will be a focus on linear algebra and probabilistic inference in the software.}
}

@BScProject{CyclingData:11,
  author = {Neil D. Lawrence},
  projectNumber = {NDL-2},  
  year = 2011,
  title = {Machine Learning for Fitness Monitoring in Cycling},
  abstract = {Technologies that were previously only available to elite athletes are becoming widespread. Now casual athletes can buy systems that monitor pace, heart rate and other information for under 300 pounds. This project will build a software tool for analysis of data of this type. After first constructing a simple tool for loading in data, and corresponding ploting facilities for comparing traces, we will attempt to assess the fitness of the athlete using machine learning to enhance existing sports science approaches. The project is a collaboration with Dr Tim Chico, Consultant Cadiologist in the Cardiovascular Biomedical Research Unit.

\\\\This project will suit students with strong analytical skills, there will be a focus on linear algebra and probabilistic inference in the software.}
}

@BScProject{PythonMocap:11,
  author = {Neil D. Lawrence},
  projectNumber = {NDL-3},  
  year = 2011,
  title = {Motion Capture Data Modelling in Python},
  abstract = {The Python programming language is becoming a de facto standard for implementation of machine learning algorithms. This project will develop tools for modelling of motion capture data in the Python programming language. Based on existing tools in MATLAB, the aim of the project will be to port the underlying machine learning techniques to the more powerful Python programming language. The end aim is to provide a simple tool for animators to model motion capture data and create new animations for computer games or the film industry.

\\\\This project will suit students with strong analytical skills, there will be a focus on linear algebra and probabilistic inference in the software.}
}